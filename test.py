from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
from peft import PeftModel
import torch


def test_model(
    base_model_path, lora_ckpt_path, instruction, input_text="", max_new_tokens=128
):
    device = "cpu"

    # Load tokenizer v√† m√¥ h√¨nh g·ªëc
    tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token

    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path, torch_dtype=torch.float32
    )
    base_model = base_model.to(device)

    # N·∫°p LoRA ƒë√£ fine-tune
    model = PeftModel.from_pretrained(base_model, lora_ckpt_path)
    model = model.to(device)
    model.eval()

    # T·∫°o prompt
    prompt = f"<|user|>\n{instruction}\n{input_text}\n<|assistant|>\n"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    # Sinh output
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            top_p=0.95,
            eos_token_id=tokenizer.eos_token_id,
        )

    result = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # C·∫Øt ph·∫ßn prompt kh·ªèi k·∫øt qu·∫£ tr·∫£ l·ªùi
    return result.replace(prompt, "").strip()


output = test_model(
    base_model_path="deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
    lora_ckpt_path="./checkpoints/deepseek-lora-cpu",  # ƒë∆∞·ªùng d·∫´n b·∫°n ƒë√£ hu·∫•n luy·ªán
    instruction="T·∫°o b√°o c√°o t·ªïng h·ª£p c√¥ng vi·ªác",
    input_text="H√¥m nay c√≥ 3 nh√¢n vi√™n ngh·ªâ vi·ªác v√† t·ªìn kho gi·∫£m",
)

print("üí° Output:", output)
