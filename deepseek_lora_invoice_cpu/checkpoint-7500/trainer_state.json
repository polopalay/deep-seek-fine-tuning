{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 7500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004,
      "grad_norm": 2.759556531906128,
      "learning_rate": 4.993333333333334e-05,
      "loss": 6.9448,
      "step": 10
    },
    {
      "epoch": 0.008,
      "grad_norm": 2.6815710067749023,
      "learning_rate": 4.986666666666667e-05,
      "loss": 6.5367,
      "step": 20
    },
    {
      "epoch": 0.012,
      "grad_norm": 2.6790547370910645,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 6.1002,
      "step": 30
    },
    {
      "epoch": 0.016,
      "grad_norm": 2.5447604656219482,
      "learning_rate": 4.973333333333334e-05,
      "loss": 5.988,
      "step": 40
    },
    {
      "epoch": 0.02,
      "grad_norm": 2.284958600997925,
      "learning_rate": 4.966666666666667e-05,
      "loss": 5.5643,
      "step": 50
    },
    {
      "epoch": 0.024,
      "grad_norm": 2.623711109161377,
      "learning_rate": 4.96e-05,
      "loss": 5.436,
      "step": 60
    },
    {
      "epoch": 0.028,
      "grad_norm": 2.5011746883392334,
      "learning_rate": 4.9533333333333336e-05,
      "loss": 5.1097,
      "step": 70
    },
    {
      "epoch": 0.032,
      "grad_norm": 2.7262942790985107,
      "learning_rate": 4.9466666666666665e-05,
      "loss": 4.9532,
      "step": 80
    },
    {
      "epoch": 0.036,
      "grad_norm": 3.627720832824707,
      "learning_rate": 4.94e-05,
      "loss": 4.8502,
      "step": 90
    },
    {
      "epoch": 0.04,
      "grad_norm": 2.2221462726593018,
      "learning_rate": 4.933333333333334e-05,
      "loss": 4.5088,
      "step": 100
    },
    {
      "epoch": 0.044,
      "grad_norm": 2.4715688228607178,
      "learning_rate": 4.926666666666667e-05,
      "loss": 4.3146,
      "step": 110
    },
    {
      "epoch": 0.048,
      "grad_norm": 3.2634997367858887,
      "learning_rate": 4.92e-05,
      "loss": 4.1567,
      "step": 120
    },
    {
      "epoch": 0.052,
      "grad_norm": 3.7843117713928223,
      "learning_rate": 4.913333333333334e-05,
      "loss": 3.9467,
      "step": 130
    },
    {
      "epoch": 0.056,
      "grad_norm": 3.697192907333374,
      "learning_rate": 4.906666666666667e-05,
      "loss": 3.4938,
      "step": 140
    },
    {
      "epoch": 0.06,
      "grad_norm": 3.7621970176696777,
      "learning_rate": 4.9e-05,
      "loss": 3.4514,
      "step": 150
    },
    {
      "epoch": 0.064,
      "grad_norm": 4.317609786987305,
      "learning_rate": 4.8933333333333335e-05,
      "loss": 3.1587,
      "step": 160
    },
    {
      "epoch": 0.068,
      "grad_norm": 5.358492374420166,
      "learning_rate": 4.886666666666667e-05,
      "loss": 2.8246,
      "step": 170
    },
    {
      "epoch": 0.072,
      "grad_norm": 4.7422566413879395,
      "learning_rate": 4.88e-05,
      "loss": 2.597,
      "step": 180
    },
    {
      "epoch": 0.076,
      "grad_norm": 4.18316650390625,
      "learning_rate": 4.8733333333333337e-05,
      "loss": 2.58,
      "step": 190
    },
    {
      "epoch": 0.08,
      "grad_norm": 5.756173610687256,
      "learning_rate": 4.866666666666667e-05,
      "loss": 2.6342,
      "step": 200
    },
    {
      "epoch": 0.084,
      "grad_norm": 5.036791801452637,
      "learning_rate": 4.86e-05,
      "loss": 2.2276,
      "step": 210
    },
    {
      "epoch": 0.088,
      "grad_norm": 3.5329713821411133,
      "learning_rate": 4.853333333333334e-05,
      "loss": 1.9682,
      "step": 220
    },
    {
      "epoch": 0.092,
      "grad_norm": 5.244832515716553,
      "learning_rate": 4.8466666666666675e-05,
      "loss": 2.1785,
      "step": 230
    },
    {
      "epoch": 0.096,
      "grad_norm": 4.147612571716309,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 1.9097,
      "step": 240
    },
    {
      "epoch": 0.1,
      "grad_norm": 4.916409492492676,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 1.741,
      "step": 250
    },
    {
      "epoch": 0.104,
      "grad_norm": 4.962234973907471,
      "learning_rate": 4.826666666666667e-05,
      "loss": 1.6556,
      "step": 260
    },
    {
      "epoch": 0.108,
      "grad_norm": 4.5096049308776855,
      "learning_rate": 4.82e-05,
      "loss": 1.9169,
      "step": 270
    },
    {
      "epoch": 0.112,
      "grad_norm": 3.5282247066497803,
      "learning_rate": 4.8133333333333336e-05,
      "loss": 1.4767,
      "step": 280
    },
    {
      "epoch": 0.116,
      "grad_norm": 5.556105613708496,
      "learning_rate": 4.806666666666667e-05,
      "loss": 1.3737,
      "step": 290
    },
    {
      "epoch": 0.12,
      "grad_norm": 4.778581619262695,
      "learning_rate": 4.8e-05,
      "loss": 1.6706,
      "step": 300
    },
    {
      "epoch": 0.124,
      "grad_norm": 4.684844970703125,
      "learning_rate": 4.793333333333334e-05,
      "loss": 1.3503,
      "step": 310
    },
    {
      "epoch": 0.128,
      "grad_norm": 5.1298346519470215,
      "learning_rate": 4.7866666666666674e-05,
      "loss": 1.5066,
      "step": 320
    },
    {
      "epoch": 0.132,
      "grad_norm": 5.994271278381348,
      "learning_rate": 4.78e-05,
      "loss": 1.2799,
      "step": 330
    },
    {
      "epoch": 0.136,
      "grad_norm": 3.960545778274536,
      "learning_rate": 4.773333333333333e-05,
      "loss": 1.2529,
      "step": 340
    },
    {
      "epoch": 0.14,
      "grad_norm": 4.558290004730225,
      "learning_rate": 4.766666666666667e-05,
      "loss": 1.2773,
      "step": 350
    },
    {
      "epoch": 0.144,
      "grad_norm": 3.381321430206299,
      "learning_rate": 4.76e-05,
      "loss": 1.1133,
      "step": 360
    },
    {
      "epoch": 0.148,
      "grad_norm": 4.675635814666748,
      "learning_rate": 4.7533333333333334e-05,
      "loss": 1.1821,
      "step": 370
    },
    {
      "epoch": 0.152,
      "grad_norm": 4.232751369476318,
      "learning_rate": 4.746666666666667e-05,
      "loss": 1.115,
      "step": 380
    },
    {
      "epoch": 0.156,
      "grad_norm": 4.222426414489746,
      "learning_rate": 4.74e-05,
      "loss": 1.1435,
      "step": 390
    },
    {
      "epoch": 0.16,
      "grad_norm": 4.465365886688232,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 1.1767,
      "step": 400
    },
    {
      "epoch": 0.164,
      "grad_norm": 3.332958459854126,
      "learning_rate": 4.726666666666667e-05,
      "loss": 1.1384,
      "step": 410
    },
    {
      "epoch": 0.168,
      "grad_norm": 4.981296539306641,
      "learning_rate": 4.72e-05,
      "loss": 1.1218,
      "step": 420
    },
    {
      "epoch": 0.172,
      "grad_norm": 3.843855381011963,
      "learning_rate": 4.713333333333333e-05,
      "loss": 1.054,
      "step": 430
    },
    {
      "epoch": 0.176,
      "grad_norm": 2.8759288787841797,
      "learning_rate": 4.706666666666667e-05,
      "loss": 1.1171,
      "step": 440
    },
    {
      "epoch": 0.18,
      "grad_norm": 3.349506378173828,
      "learning_rate": 4.7e-05,
      "loss": 1.0589,
      "step": 450
    },
    {
      "epoch": 0.184,
      "grad_norm": 3.022496223449707,
      "learning_rate": 4.6933333333333333e-05,
      "loss": 1.0216,
      "step": 460
    },
    {
      "epoch": 0.188,
      "grad_norm": 2.823489189147949,
      "learning_rate": 4.686666666666667e-05,
      "loss": 1.0716,
      "step": 470
    },
    {
      "epoch": 0.192,
      "grad_norm": 4.40871000289917,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 1.0866,
      "step": 480
    },
    {
      "epoch": 0.196,
      "grad_norm": 4.188097953796387,
      "learning_rate": 4.6733333333333335e-05,
      "loss": 1.1785,
      "step": 490
    },
    {
      "epoch": 0.2,
      "grad_norm": 5.198055744171143,
      "learning_rate": 4.666666666666667e-05,
      "loss": 1.0437,
      "step": 500
    },
    {
      "epoch": 0.204,
      "grad_norm": 3.5218608379364014,
      "learning_rate": 4.660000000000001e-05,
      "loss": 1.0634,
      "step": 510
    },
    {
      "epoch": 0.208,
      "grad_norm": 2.7180092334747314,
      "learning_rate": 4.653333333333334e-05,
      "loss": 1.0608,
      "step": 520
    },
    {
      "epoch": 0.212,
      "grad_norm": 3.551999807357788,
      "learning_rate": 4.646666666666667e-05,
      "loss": 1.0044,
      "step": 530
    },
    {
      "epoch": 0.216,
      "grad_norm": 3.723654270172119,
      "learning_rate": 4.64e-05,
      "loss": 0.9652,
      "step": 540
    },
    {
      "epoch": 0.22,
      "grad_norm": 4.270537853240967,
      "learning_rate": 4.633333333333333e-05,
      "loss": 0.9777,
      "step": 550
    },
    {
      "epoch": 0.224,
      "grad_norm": 4.861824989318848,
      "learning_rate": 4.626666666666667e-05,
      "loss": 1.0043,
      "step": 560
    },
    {
      "epoch": 0.228,
      "grad_norm": 5.739391803741455,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 1.051,
      "step": 570
    },
    {
      "epoch": 0.232,
      "grad_norm": 4.148479461669922,
      "learning_rate": 4.6133333333333334e-05,
      "loss": 0.9703,
      "step": 580
    },
    {
      "epoch": 0.236,
      "grad_norm": 3.3006603717803955,
      "learning_rate": 4.606666666666667e-05,
      "loss": 0.9682,
      "step": 590
    },
    {
      "epoch": 0.24,
      "grad_norm": 7.591488361358643,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.0242,
      "step": 600
    },
    {
      "epoch": 0.244,
      "grad_norm": 3.4758667945861816,
      "learning_rate": 4.5933333333333336e-05,
      "loss": 0.9246,
      "step": 610
    },
    {
      "epoch": 0.248,
      "grad_norm": 3.6969377994537354,
      "learning_rate": 4.5866666666666666e-05,
      "loss": 0.9313,
      "step": 620
    },
    {
      "epoch": 0.252,
      "grad_norm": 3.0522217750549316,
      "learning_rate": 4.58e-05,
      "loss": 0.9903,
      "step": 630
    },
    {
      "epoch": 0.256,
      "grad_norm": 5.809128761291504,
      "learning_rate": 4.573333333333333e-05,
      "loss": 0.8476,
      "step": 640
    },
    {
      "epoch": 0.26,
      "grad_norm": 4.2836761474609375,
      "learning_rate": 4.566666666666667e-05,
      "loss": 0.9495,
      "step": 650
    },
    {
      "epoch": 0.264,
      "grad_norm": 2.8533151149749756,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 0.978,
      "step": 660
    },
    {
      "epoch": 0.268,
      "grad_norm": 3.5970358848571777,
      "learning_rate": 4.553333333333333e-05,
      "loss": 0.9356,
      "step": 670
    },
    {
      "epoch": 0.272,
      "grad_norm": 5.616231441497803,
      "learning_rate": 4.546666666666667e-05,
      "loss": 0.9791,
      "step": 680
    },
    {
      "epoch": 0.276,
      "grad_norm": 3.9284048080444336,
      "learning_rate": 4.5400000000000006e-05,
      "loss": 0.8719,
      "step": 690
    },
    {
      "epoch": 0.28,
      "grad_norm": 7.2248687744140625,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.9467,
      "step": 700
    },
    {
      "epoch": 0.284,
      "grad_norm": 2.73689866065979,
      "learning_rate": 4.526666666666667e-05,
      "loss": 0.8627,
      "step": 710
    },
    {
      "epoch": 0.288,
      "grad_norm": 4.5793657302856445,
      "learning_rate": 4.52e-05,
      "loss": 0.8773,
      "step": 720
    },
    {
      "epoch": 0.292,
      "grad_norm": 5.122891426086426,
      "learning_rate": 4.513333333333333e-05,
      "loss": 0.9458,
      "step": 730
    },
    {
      "epoch": 0.296,
      "grad_norm": 2.741896152496338,
      "learning_rate": 4.5066666666666667e-05,
      "loss": 0.8799,
      "step": 740
    },
    {
      "epoch": 0.3,
      "grad_norm": 4.573403835296631,
      "learning_rate": 4.5e-05,
      "loss": 0.9815,
      "step": 750
    },
    {
      "epoch": 0.304,
      "grad_norm": 2.7009999752044678,
      "learning_rate": 4.493333333333333e-05,
      "loss": 0.844,
      "step": 760
    },
    {
      "epoch": 0.308,
      "grad_norm": 3.0039944648742676,
      "learning_rate": 4.486666666666667e-05,
      "loss": 0.8505,
      "step": 770
    },
    {
      "epoch": 0.312,
      "grad_norm": 2.8380789756774902,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 0.9657,
      "step": 780
    },
    {
      "epoch": 0.316,
      "grad_norm": 3.1858460903167725,
      "learning_rate": 4.473333333333334e-05,
      "loss": 0.9,
      "step": 790
    },
    {
      "epoch": 0.32,
      "grad_norm": 2.947809934616089,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.8493,
      "step": 800
    },
    {
      "epoch": 0.324,
      "grad_norm": 3.55155086517334,
      "learning_rate": 4.46e-05,
      "loss": 0.8716,
      "step": 810
    },
    {
      "epoch": 0.328,
      "grad_norm": 3.57077956199646,
      "learning_rate": 4.4533333333333336e-05,
      "loss": 0.9169,
      "step": 820
    },
    {
      "epoch": 0.332,
      "grad_norm": 3.3089702129364014,
      "learning_rate": 4.4466666666666666e-05,
      "loss": 0.9047,
      "step": 830
    },
    {
      "epoch": 0.336,
      "grad_norm": 1.7216730117797852,
      "learning_rate": 4.44e-05,
      "loss": 0.8728,
      "step": 840
    },
    {
      "epoch": 0.34,
      "grad_norm": 3.1899962425231934,
      "learning_rate": 4.433333333333334e-05,
      "loss": 0.8904,
      "step": 850
    },
    {
      "epoch": 0.344,
      "grad_norm": 2.2641942501068115,
      "learning_rate": 4.426666666666667e-05,
      "loss": 0.8427,
      "step": 860
    },
    {
      "epoch": 0.348,
      "grad_norm": 2.408015727996826,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 0.8224,
      "step": 870
    },
    {
      "epoch": 0.352,
      "grad_norm": 3.381687879562378,
      "learning_rate": 4.413333333333334e-05,
      "loss": 0.8789,
      "step": 880
    },
    {
      "epoch": 0.356,
      "grad_norm": 3.483842372894287,
      "learning_rate": 4.406666666666667e-05,
      "loss": 0.8763,
      "step": 890
    },
    {
      "epoch": 0.36,
      "grad_norm": 3.2790334224700928,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.8104,
      "step": 900
    },
    {
      "epoch": 0.364,
      "grad_norm": 3.3574461936950684,
      "learning_rate": 4.3933333333333335e-05,
      "loss": 0.8332,
      "step": 910
    },
    {
      "epoch": 0.368,
      "grad_norm": 3.772747039794922,
      "learning_rate": 4.3866666666666665e-05,
      "loss": 0.8776,
      "step": 920
    },
    {
      "epoch": 0.372,
      "grad_norm": 2.8272314071655273,
      "learning_rate": 4.38e-05,
      "loss": 0.8455,
      "step": 930
    },
    {
      "epoch": 0.376,
      "grad_norm": 8.393729209899902,
      "learning_rate": 4.373333333333334e-05,
      "loss": 0.896,
      "step": 940
    },
    {
      "epoch": 0.38,
      "grad_norm": 2.020709276199341,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 0.8594,
      "step": 950
    },
    {
      "epoch": 0.384,
      "grad_norm": 2.480116844177246,
      "learning_rate": 4.36e-05,
      "loss": 0.8283,
      "step": 960
    },
    {
      "epoch": 0.388,
      "grad_norm": 3.0122570991516113,
      "learning_rate": 4.353333333333334e-05,
      "loss": 0.8536,
      "step": 970
    },
    {
      "epoch": 0.392,
      "grad_norm": 3.887324094772339,
      "learning_rate": 4.346666666666667e-05,
      "loss": 0.8525,
      "step": 980
    },
    {
      "epoch": 0.396,
      "grad_norm": 3.5525898933410645,
      "learning_rate": 4.3400000000000005e-05,
      "loss": 0.792,
      "step": 990
    },
    {
      "epoch": 0.4,
      "grad_norm": 2.4415128231048584,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.8563,
      "step": 1000
    },
    {
      "epoch": 0.404,
      "grad_norm": 4.159916877746582,
      "learning_rate": 4.3266666666666664e-05,
      "loss": 0.8183,
      "step": 1010
    },
    {
      "epoch": 0.408,
      "grad_norm": 2.8620681762695312,
      "learning_rate": 4.32e-05,
      "loss": 0.9099,
      "step": 1020
    },
    {
      "epoch": 0.412,
      "grad_norm": 3.819164991378784,
      "learning_rate": 4.3133333333333336e-05,
      "loss": 0.7817,
      "step": 1030
    },
    {
      "epoch": 0.416,
      "grad_norm": 2.4335081577301025,
      "learning_rate": 4.3066666666666665e-05,
      "loss": 0.8677,
      "step": 1040
    },
    {
      "epoch": 0.42,
      "grad_norm": 2.125619888305664,
      "learning_rate": 4.3e-05,
      "loss": 0.822,
      "step": 1050
    },
    {
      "epoch": 0.424,
      "grad_norm": 4.415353298187256,
      "learning_rate": 4.293333333333334e-05,
      "loss": 0.7276,
      "step": 1060
    },
    {
      "epoch": 0.428,
      "grad_norm": 4.230154991149902,
      "learning_rate": 4.286666666666667e-05,
      "loss": 0.7481,
      "step": 1070
    },
    {
      "epoch": 0.432,
      "grad_norm": 2.280895471572876,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 0.8188,
      "step": 1080
    },
    {
      "epoch": 0.436,
      "grad_norm": 2.6855969429016113,
      "learning_rate": 4.273333333333333e-05,
      "loss": 0.8431,
      "step": 1090
    },
    {
      "epoch": 0.44,
      "grad_norm": 2.3084092140197754,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.753,
      "step": 1100
    },
    {
      "epoch": 0.444,
      "grad_norm": 3.1639938354492188,
      "learning_rate": 4.26e-05,
      "loss": 0.8857,
      "step": 1110
    },
    {
      "epoch": 0.448,
      "grad_norm": 4.680550575256348,
      "learning_rate": 4.2533333333333335e-05,
      "loss": 0.8349,
      "step": 1120
    },
    {
      "epoch": 0.452,
      "grad_norm": 2.5488297939300537,
      "learning_rate": 4.246666666666667e-05,
      "loss": 0.8056,
      "step": 1130
    },
    {
      "epoch": 0.456,
      "grad_norm": 5.814664840698242,
      "learning_rate": 4.24e-05,
      "loss": 0.8636,
      "step": 1140
    },
    {
      "epoch": 0.46,
      "grad_norm": 3.213355541229248,
      "learning_rate": 4.233333333333334e-05,
      "loss": 0.7962,
      "step": 1150
    },
    {
      "epoch": 0.464,
      "grad_norm": 2.5805606842041016,
      "learning_rate": 4.226666666666667e-05,
      "loss": 0.7993,
      "step": 1160
    },
    {
      "epoch": 0.468,
      "grad_norm": 3.467942237854004,
      "learning_rate": 4.22e-05,
      "loss": 0.8535,
      "step": 1170
    },
    {
      "epoch": 0.472,
      "grad_norm": 3.9840426445007324,
      "learning_rate": 4.213333333333334e-05,
      "loss": 0.8863,
      "step": 1180
    },
    {
      "epoch": 0.476,
      "grad_norm": 2.5241658687591553,
      "learning_rate": 4.206666666666667e-05,
      "loss": 0.8263,
      "step": 1190
    },
    {
      "epoch": 0.48,
      "grad_norm": 3.037869930267334,
      "learning_rate": 4.2e-05,
      "loss": 0.7118,
      "step": 1200
    },
    {
      "epoch": 0.484,
      "grad_norm": 3.623422145843506,
      "learning_rate": 4.1933333333333334e-05,
      "loss": 0.8077,
      "step": 1210
    },
    {
      "epoch": 0.488,
      "grad_norm": 2.97379207611084,
      "learning_rate": 4.186666666666667e-05,
      "loss": 0.7212,
      "step": 1220
    },
    {
      "epoch": 0.492,
      "grad_norm": 2.264741897583008,
      "learning_rate": 4.18e-05,
      "loss": 0.8104,
      "step": 1230
    },
    {
      "epoch": 0.496,
      "grad_norm": 4.650484561920166,
      "learning_rate": 4.1733333333333336e-05,
      "loss": 0.7658,
      "step": 1240
    },
    {
      "epoch": 0.5,
      "grad_norm": 3.3497586250305176,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.8104,
      "step": 1250
    },
    {
      "epoch": 0.504,
      "grad_norm": 3.0442118644714355,
      "learning_rate": 4.16e-05,
      "loss": 0.7415,
      "step": 1260
    },
    {
      "epoch": 0.508,
      "grad_norm": 3.5548007488250732,
      "learning_rate": 4.153333333333334e-05,
      "loss": 0.8283,
      "step": 1270
    },
    {
      "epoch": 0.512,
      "grad_norm": 2.891464948654175,
      "learning_rate": 4.146666666666667e-05,
      "loss": 0.7497,
      "step": 1280
    },
    {
      "epoch": 0.516,
      "grad_norm": 4.168405532836914,
      "learning_rate": 4.14e-05,
      "loss": 0.6843,
      "step": 1290
    },
    {
      "epoch": 0.52,
      "grad_norm": 3.304628849029541,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.775,
      "step": 1300
    },
    {
      "epoch": 0.524,
      "grad_norm": 4.6941375732421875,
      "learning_rate": 4.126666666666667e-05,
      "loss": 0.6714,
      "step": 1310
    },
    {
      "epoch": 0.528,
      "grad_norm": 2.1515188217163086,
      "learning_rate": 4.12e-05,
      "loss": 0.6726,
      "step": 1320
    },
    {
      "epoch": 0.532,
      "grad_norm": 2.9897992610931396,
      "learning_rate": 4.1133333333333335e-05,
      "loss": 0.6048,
      "step": 1330
    },
    {
      "epoch": 0.536,
      "grad_norm": 2.79042911529541,
      "learning_rate": 4.106666666666667e-05,
      "loss": 0.6796,
      "step": 1340
    },
    {
      "epoch": 0.54,
      "grad_norm": 2.397592067718506,
      "learning_rate": 4.1e-05,
      "loss": 0.7121,
      "step": 1350
    },
    {
      "epoch": 0.544,
      "grad_norm": 3.629549026489258,
      "learning_rate": 4.093333333333334e-05,
      "loss": 0.5842,
      "step": 1360
    },
    {
      "epoch": 0.548,
      "grad_norm": 4.450104713439941,
      "learning_rate": 4.086666666666667e-05,
      "loss": 0.7045,
      "step": 1370
    },
    {
      "epoch": 0.552,
      "grad_norm": 2.6625587940216064,
      "learning_rate": 4.08e-05,
      "loss": 0.6807,
      "step": 1380
    },
    {
      "epoch": 0.556,
      "grad_norm": 2.355917453765869,
      "learning_rate": 4.073333333333333e-05,
      "loss": 0.5872,
      "step": 1390
    },
    {
      "epoch": 0.56,
      "grad_norm": 2.616015911102295,
      "learning_rate": 4.066666666666667e-05,
      "loss": 0.6275,
      "step": 1400
    },
    {
      "epoch": 0.564,
      "grad_norm": 2.5507564544677734,
      "learning_rate": 4.0600000000000004e-05,
      "loss": 0.6333,
      "step": 1410
    },
    {
      "epoch": 0.568,
      "grad_norm": 4.595039367675781,
      "learning_rate": 4.0533333333333334e-05,
      "loss": 0.6199,
      "step": 1420
    },
    {
      "epoch": 0.572,
      "grad_norm": 2.7531979084014893,
      "learning_rate": 4.046666666666667e-05,
      "loss": 0.577,
      "step": 1430
    },
    {
      "epoch": 0.576,
      "grad_norm": 2.921034336090088,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 0.6511,
      "step": 1440
    },
    {
      "epoch": 0.58,
      "grad_norm": 2.393723487854004,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 0.6329,
      "step": 1450
    },
    {
      "epoch": 0.584,
      "grad_norm": 4.925509452819824,
      "learning_rate": 4.026666666666667e-05,
      "loss": 0.5825,
      "step": 1460
    },
    {
      "epoch": 0.588,
      "grad_norm": 2.250483751296997,
      "learning_rate": 4.02e-05,
      "loss": 0.5734,
      "step": 1470
    },
    {
      "epoch": 0.592,
      "grad_norm": 2.4804186820983887,
      "learning_rate": 4.013333333333333e-05,
      "loss": 0.6242,
      "step": 1480
    },
    {
      "epoch": 0.596,
      "grad_norm": 2.349843978881836,
      "learning_rate": 4.006666666666667e-05,
      "loss": 0.6027,
      "step": 1490
    },
    {
      "epoch": 0.6,
      "grad_norm": 2.0274641513824463,
      "learning_rate": 4e-05,
      "loss": 0.53,
      "step": 1500
    },
    {
      "epoch": 0.604,
      "grad_norm": 3.524812698364258,
      "learning_rate": 3.993333333333333e-05,
      "loss": 0.5796,
      "step": 1510
    },
    {
      "epoch": 0.608,
      "grad_norm": 3.4912350177764893,
      "learning_rate": 3.986666666666667e-05,
      "loss": 0.5729,
      "step": 1520
    },
    {
      "epoch": 0.612,
      "grad_norm": 3.803772449493408,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 0.6235,
      "step": 1530
    },
    {
      "epoch": 0.616,
      "grad_norm": 2.948676347732544,
      "learning_rate": 3.9733333333333335e-05,
      "loss": 0.56,
      "step": 1540
    },
    {
      "epoch": 0.62,
      "grad_norm": 2.4099178314208984,
      "learning_rate": 3.966666666666667e-05,
      "loss": 0.534,
      "step": 1550
    },
    {
      "epoch": 0.624,
      "grad_norm": 3.8791074752807617,
      "learning_rate": 3.960000000000001e-05,
      "loss": 0.5486,
      "step": 1560
    },
    {
      "epoch": 0.628,
      "grad_norm": 2.6658120155334473,
      "learning_rate": 3.9533333333333337e-05,
      "loss": 0.5702,
      "step": 1570
    },
    {
      "epoch": 0.632,
      "grad_norm": 3.643357276916504,
      "learning_rate": 3.9466666666666666e-05,
      "loss": 0.5405,
      "step": 1580
    },
    {
      "epoch": 0.636,
      "grad_norm": 3.7368481159210205,
      "learning_rate": 3.94e-05,
      "loss": 0.5884,
      "step": 1590
    },
    {
      "epoch": 0.64,
      "grad_norm": 4.507457256317139,
      "learning_rate": 3.933333333333333e-05,
      "loss": 0.6393,
      "step": 1600
    },
    {
      "epoch": 0.644,
      "grad_norm": 2.6060569286346436,
      "learning_rate": 3.926666666666667e-05,
      "loss": 0.6065,
      "step": 1610
    },
    {
      "epoch": 0.648,
      "grad_norm": 3.4844818115234375,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 0.5493,
      "step": 1620
    },
    {
      "epoch": 0.652,
      "grad_norm": 2.7315618991851807,
      "learning_rate": 3.9133333333333334e-05,
      "loss": 0.5648,
      "step": 1630
    },
    {
      "epoch": 0.656,
      "grad_norm": 3.1144633293151855,
      "learning_rate": 3.906666666666667e-05,
      "loss": 0.6468,
      "step": 1640
    },
    {
      "epoch": 0.66,
      "grad_norm": 3.221350908279419,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.5801,
      "step": 1650
    },
    {
      "epoch": 0.664,
      "grad_norm": 2.8242828845977783,
      "learning_rate": 3.8933333333333336e-05,
      "loss": 0.5841,
      "step": 1660
    },
    {
      "epoch": 0.668,
      "grad_norm": 2.6660828590393066,
      "learning_rate": 3.8866666666666665e-05,
      "loss": 0.5487,
      "step": 1670
    },
    {
      "epoch": 0.672,
      "grad_norm": 4.472079753875732,
      "learning_rate": 3.88e-05,
      "loss": 0.6455,
      "step": 1680
    },
    {
      "epoch": 0.676,
      "grad_norm": 2.230858087539673,
      "learning_rate": 3.873333333333333e-05,
      "loss": 0.5828,
      "step": 1690
    },
    {
      "epoch": 0.68,
      "grad_norm": 2.4814939498901367,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.6587,
      "step": 1700
    },
    {
      "epoch": 0.684,
      "grad_norm": 5.341796398162842,
      "learning_rate": 3.86e-05,
      "loss": 0.5439,
      "step": 1710
    },
    {
      "epoch": 0.688,
      "grad_norm": 2.378390312194824,
      "learning_rate": 3.853333333333334e-05,
      "loss": 0.5774,
      "step": 1720
    },
    {
      "epoch": 0.692,
      "grad_norm": 2.5849661827087402,
      "learning_rate": 3.846666666666667e-05,
      "loss": 0.5572,
      "step": 1730
    },
    {
      "epoch": 0.696,
      "grad_norm": 2.6625542640686035,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 0.5636,
      "step": 1740
    },
    {
      "epoch": 0.7,
      "grad_norm": 3.150127649307251,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.5787,
      "step": 1750
    },
    {
      "epoch": 0.704,
      "grad_norm": 3.1253371238708496,
      "learning_rate": 3.8266666666666664e-05,
      "loss": 0.5302,
      "step": 1760
    },
    {
      "epoch": 0.708,
      "grad_norm": 1.6950234174728394,
      "learning_rate": 3.82e-05,
      "loss": 0.6553,
      "step": 1770
    },
    {
      "epoch": 0.712,
      "grad_norm": 2.9119319915771484,
      "learning_rate": 3.8133333333333336e-05,
      "loss": 0.6116,
      "step": 1780
    },
    {
      "epoch": 0.716,
      "grad_norm": 1.8854632377624512,
      "learning_rate": 3.8066666666666666e-05,
      "loss": 0.5726,
      "step": 1790
    },
    {
      "epoch": 0.72,
      "grad_norm": 4.826165199279785,
      "learning_rate": 3.8e-05,
      "loss": 0.5949,
      "step": 1800
    },
    {
      "epoch": 0.724,
      "grad_norm": 2.3930139541625977,
      "learning_rate": 3.793333333333334e-05,
      "loss": 0.5264,
      "step": 1810
    },
    {
      "epoch": 0.728,
      "grad_norm": 2.6812596321105957,
      "learning_rate": 3.786666666666667e-05,
      "loss": 0.5917,
      "step": 1820
    },
    {
      "epoch": 0.732,
      "grad_norm": 4.702988624572754,
      "learning_rate": 3.7800000000000004e-05,
      "loss": 0.6345,
      "step": 1830
    },
    {
      "epoch": 0.736,
      "grad_norm": 3.031723976135254,
      "learning_rate": 3.773333333333334e-05,
      "loss": 0.5683,
      "step": 1840
    },
    {
      "epoch": 0.74,
      "grad_norm": 2.224245071411133,
      "learning_rate": 3.766666666666667e-05,
      "loss": 0.5815,
      "step": 1850
    },
    {
      "epoch": 0.744,
      "grad_norm": 3.3976798057556152,
      "learning_rate": 3.76e-05,
      "loss": 0.5297,
      "step": 1860
    },
    {
      "epoch": 0.748,
      "grad_norm": 7.137392997741699,
      "learning_rate": 3.7533333333333335e-05,
      "loss": 0.6413,
      "step": 1870
    },
    {
      "epoch": 0.752,
      "grad_norm": 2.514483690261841,
      "learning_rate": 3.7466666666666665e-05,
      "loss": 0.5973,
      "step": 1880
    },
    {
      "epoch": 0.756,
      "grad_norm": 4.437960147857666,
      "learning_rate": 3.74e-05,
      "loss": 0.5747,
      "step": 1890
    },
    {
      "epoch": 0.76,
      "grad_norm": 2.013364315032959,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.5374,
      "step": 1900
    },
    {
      "epoch": 0.764,
      "grad_norm": 2.5569825172424316,
      "learning_rate": 3.726666666666667e-05,
      "loss": 0.6434,
      "step": 1910
    },
    {
      "epoch": 0.768,
      "grad_norm": 3.168689250946045,
      "learning_rate": 3.72e-05,
      "loss": 0.5817,
      "step": 1920
    },
    {
      "epoch": 0.772,
      "grad_norm": 1.9558907747268677,
      "learning_rate": 3.713333333333334e-05,
      "loss": 0.5853,
      "step": 1930
    },
    {
      "epoch": 0.776,
      "grad_norm": 2.6139724254608154,
      "learning_rate": 3.706666666666667e-05,
      "loss": 0.5309,
      "step": 1940
    },
    {
      "epoch": 0.78,
      "grad_norm": 3.3015825748443604,
      "learning_rate": 3.7e-05,
      "loss": 0.5355,
      "step": 1950
    },
    {
      "epoch": 0.784,
      "grad_norm": 2.973893880844116,
      "learning_rate": 3.6933333333333334e-05,
      "loss": 0.5602,
      "step": 1960
    },
    {
      "epoch": 0.788,
      "grad_norm": 2.910062313079834,
      "learning_rate": 3.6866666666666664e-05,
      "loss": 0.5482,
      "step": 1970
    },
    {
      "epoch": 0.792,
      "grad_norm": 3.8285646438598633,
      "learning_rate": 3.68e-05,
      "loss": 0.5509,
      "step": 1980
    },
    {
      "epoch": 0.796,
      "grad_norm": 2.379591226577759,
      "learning_rate": 3.6733333333333336e-05,
      "loss": 0.5636,
      "step": 1990
    },
    {
      "epoch": 0.8,
      "grad_norm": 2.2032365798950195,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.6036,
      "step": 2000
    },
    {
      "epoch": 0.804,
      "grad_norm": 2.5789804458618164,
      "learning_rate": 3.66e-05,
      "loss": 0.6428,
      "step": 2010
    },
    {
      "epoch": 0.808,
      "grad_norm": 2.9477293491363525,
      "learning_rate": 3.653333333333334e-05,
      "loss": 0.5342,
      "step": 2020
    },
    {
      "epoch": 0.812,
      "grad_norm": 2.6374905109405518,
      "learning_rate": 3.646666666666667e-05,
      "loss": 0.6292,
      "step": 2030
    },
    {
      "epoch": 0.816,
      "grad_norm": 2.820051908493042,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 0.5771,
      "step": 2040
    },
    {
      "epoch": 0.82,
      "grad_norm": 2.833366870880127,
      "learning_rate": 3.633333333333333e-05,
      "loss": 0.5723,
      "step": 2050
    },
    {
      "epoch": 0.824,
      "grad_norm": 2.3468167781829834,
      "learning_rate": 3.626666666666667e-05,
      "loss": 0.5875,
      "step": 2060
    },
    {
      "epoch": 0.828,
      "grad_norm": 2.085479974746704,
      "learning_rate": 3.62e-05,
      "loss": 0.6495,
      "step": 2070
    },
    {
      "epoch": 0.832,
      "grad_norm": 4.188352584838867,
      "learning_rate": 3.6133333333333335e-05,
      "loss": 0.5047,
      "step": 2080
    },
    {
      "epoch": 0.836,
      "grad_norm": 2.898477792739868,
      "learning_rate": 3.606666666666667e-05,
      "loss": 0.5465,
      "step": 2090
    },
    {
      "epoch": 0.84,
      "grad_norm": 2.7571022510528564,
      "learning_rate": 3.6e-05,
      "loss": 0.6576,
      "step": 2100
    },
    {
      "epoch": 0.844,
      "grad_norm": 2.745126724243164,
      "learning_rate": 3.593333333333334e-05,
      "loss": 0.5551,
      "step": 2110
    },
    {
      "epoch": 0.848,
      "grad_norm": 2.3731582164764404,
      "learning_rate": 3.586666666666667e-05,
      "loss": 0.5552,
      "step": 2120
    },
    {
      "epoch": 0.852,
      "grad_norm": 2.702160596847534,
      "learning_rate": 3.58e-05,
      "loss": 0.6034,
      "step": 2130
    },
    {
      "epoch": 0.856,
      "grad_norm": 2.413212299346924,
      "learning_rate": 3.573333333333333e-05,
      "loss": 0.5867,
      "step": 2140
    },
    {
      "epoch": 0.86,
      "grad_norm": 3.158259630203247,
      "learning_rate": 3.566666666666667e-05,
      "loss": 0.5787,
      "step": 2150
    },
    {
      "epoch": 0.864,
      "grad_norm": 3.400913715362549,
      "learning_rate": 3.56e-05,
      "loss": 0.6125,
      "step": 2160
    },
    {
      "epoch": 0.868,
      "grad_norm": 2.330561637878418,
      "learning_rate": 3.5533333333333334e-05,
      "loss": 0.5253,
      "step": 2170
    },
    {
      "epoch": 0.872,
      "grad_norm": 2.7027385234832764,
      "learning_rate": 3.546666666666667e-05,
      "loss": 0.5772,
      "step": 2180
    },
    {
      "epoch": 0.876,
      "grad_norm": 2.385591983795166,
      "learning_rate": 3.54e-05,
      "loss": 0.4571,
      "step": 2190
    },
    {
      "epoch": 0.88,
      "grad_norm": 3.9631474018096924,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 0.5791,
      "step": 2200
    },
    {
      "epoch": 0.884,
      "grad_norm": 3.1323630809783936,
      "learning_rate": 3.526666666666667e-05,
      "loss": 0.632,
      "step": 2210
    },
    {
      "epoch": 0.888,
      "grad_norm": 3.5228593349456787,
      "learning_rate": 3.52e-05,
      "loss": 0.5503,
      "step": 2220
    },
    {
      "epoch": 0.892,
      "grad_norm": 1.777799367904663,
      "learning_rate": 3.513333333333334e-05,
      "loss": 0.5041,
      "step": 2230
    },
    {
      "epoch": 0.896,
      "grad_norm": 2.337846517562866,
      "learning_rate": 3.506666666666667e-05,
      "loss": 0.5717,
      "step": 2240
    },
    {
      "epoch": 0.9,
      "grad_norm": 3.1416966915130615,
      "learning_rate": 3.5e-05,
      "loss": 0.6461,
      "step": 2250
    },
    {
      "epoch": 0.904,
      "grad_norm": 2.271394729614258,
      "learning_rate": 3.493333333333333e-05,
      "loss": 0.5783,
      "step": 2260
    },
    {
      "epoch": 0.908,
      "grad_norm": 3.5203680992126465,
      "learning_rate": 3.486666666666667e-05,
      "loss": 0.5684,
      "step": 2270
    },
    {
      "epoch": 0.912,
      "grad_norm": 1.9915258884429932,
      "learning_rate": 3.48e-05,
      "loss": 0.5293,
      "step": 2280
    },
    {
      "epoch": 0.916,
      "grad_norm": 2.1141374111175537,
      "learning_rate": 3.4733333333333335e-05,
      "loss": 0.6135,
      "step": 2290
    },
    {
      "epoch": 0.92,
      "grad_norm": 3.405907154083252,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.5803,
      "step": 2300
    },
    {
      "epoch": 0.924,
      "grad_norm": 2.774629592895508,
      "learning_rate": 3.46e-05,
      "loss": 0.6292,
      "step": 2310
    },
    {
      "epoch": 0.928,
      "grad_norm": 2.1899149417877197,
      "learning_rate": 3.453333333333334e-05,
      "loss": 0.5179,
      "step": 2320
    },
    {
      "epoch": 0.932,
      "grad_norm": 2.342841625213623,
      "learning_rate": 3.4466666666666666e-05,
      "loss": 0.5874,
      "step": 2330
    },
    {
      "epoch": 0.936,
      "grad_norm": 2.101956605911255,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.5268,
      "step": 2340
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.9867162704467773,
      "learning_rate": 3.433333333333333e-05,
      "loss": 0.6017,
      "step": 2350
    },
    {
      "epoch": 0.944,
      "grad_norm": 2.518064022064209,
      "learning_rate": 3.426666666666667e-05,
      "loss": 0.6518,
      "step": 2360
    },
    {
      "epoch": 0.948,
      "grad_norm": 2.5219621658325195,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 0.5425,
      "step": 2370
    },
    {
      "epoch": 0.952,
      "grad_norm": 2.472081422805786,
      "learning_rate": 3.4133333333333334e-05,
      "loss": 0.5858,
      "step": 2380
    },
    {
      "epoch": 0.956,
      "grad_norm": 2.466832160949707,
      "learning_rate": 3.406666666666667e-05,
      "loss": 0.6088,
      "step": 2390
    },
    {
      "epoch": 0.96,
      "grad_norm": 3.074538230895996,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.5702,
      "step": 2400
    },
    {
      "epoch": 0.964,
      "grad_norm": 2.704977512359619,
      "learning_rate": 3.3933333333333336e-05,
      "loss": 0.6138,
      "step": 2410
    },
    {
      "epoch": 0.968,
      "grad_norm": 2.7185416221618652,
      "learning_rate": 3.3866666666666665e-05,
      "loss": 0.605,
      "step": 2420
    },
    {
      "epoch": 0.972,
      "grad_norm": 3.712261199951172,
      "learning_rate": 3.38e-05,
      "loss": 0.5527,
      "step": 2430
    },
    {
      "epoch": 0.976,
      "grad_norm": 4.244352340698242,
      "learning_rate": 3.373333333333333e-05,
      "loss": 0.527,
      "step": 2440
    },
    {
      "epoch": 0.98,
      "grad_norm": 2.3077878952026367,
      "learning_rate": 3.366666666666667e-05,
      "loss": 0.5932,
      "step": 2450
    },
    {
      "epoch": 0.984,
      "grad_norm": 2.3273377418518066,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.6383,
      "step": 2460
    },
    {
      "epoch": 0.988,
      "grad_norm": 2.4155538082122803,
      "learning_rate": 3.353333333333333e-05,
      "loss": 0.5815,
      "step": 2470
    },
    {
      "epoch": 0.992,
      "grad_norm": 2.305757522583008,
      "learning_rate": 3.346666666666667e-05,
      "loss": 0.5076,
      "step": 2480
    },
    {
      "epoch": 0.996,
      "grad_norm": 1.6186977624893188,
      "learning_rate": 3.3400000000000005e-05,
      "loss": 0.5753,
      "step": 2490
    },
    {
      "epoch": 1.0,
      "grad_norm": 2.383261203765869,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.6374,
      "step": 2500
    },
    {
      "epoch": 1.004,
      "grad_norm": 2.101515054702759,
      "learning_rate": 3.326666666666667e-05,
      "loss": 0.5652,
      "step": 2510
    },
    {
      "epoch": 1.008,
      "grad_norm": 2.324270248413086,
      "learning_rate": 3.32e-05,
      "loss": 0.614,
      "step": 2520
    },
    {
      "epoch": 1.012,
      "grad_norm": 2.1009957790374756,
      "learning_rate": 3.313333333333333e-05,
      "loss": 0.5427,
      "step": 2530
    },
    {
      "epoch": 1.016,
      "grad_norm": 2.3584911823272705,
      "learning_rate": 3.3066666666666666e-05,
      "loss": 0.5325,
      "step": 2540
    },
    {
      "epoch": 1.02,
      "grad_norm": 2.661850929260254,
      "learning_rate": 3.3e-05,
      "loss": 0.5805,
      "step": 2550
    },
    {
      "epoch": 1.024,
      "grad_norm": 2.187189817428589,
      "learning_rate": 3.293333333333333e-05,
      "loss": 0.5193,
      "step": 2560
    },
    {
      "epoch": 1.028,
      "grad_norm": 2.1654305458068848,
      "learning_rate": 3.286666666666667e-05,
      "loss": 0.5976,
      "step": 2570
    },
    {
      "epoch": 1.032,
      "grad_norm": 3.2180747985839844,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 0.5415,
      "step": 2580
    },
    {
      "epoch": 1.036,
      "grad_norm": 3.1908929347991943,
      "learning_rate": 3.2733333333333334e-05,
      "loss": 0.5355,
      "step": 2590
    },
    {
      "epoch": 1.04,
      "grad_norm": 3.166839122772217,
      "learning_rate": 3.266666666666667e-05,
      "loss": 0.5636,
      "step": 2600
    },
    {
      "epoch": 1.044,
      "grad_norm": 2.390941619873047,
      "learning_rate": 3.26e-05,
      "loss": 0.5513,
      "step": 2610
    },
    {
      "epoch": 1.048,
      "grad_norm": 2.7590601444244385,
      "learning_rate": 3.253333333333333e-05,
      "loss": 0.4929,
      "step": 2620
    },
    {
      "epoch": 1.052,
      "grad_norm": 3.6217360496520996,
      "learning_rate": 3.2466666666666665e-05,
      "loss": 0.5885,
      "step": 2630
    },
    {
      "epoch": 1.056,
      "grad_norm": 2.831172466278076,
      "learning_rate": 3.24e-05,
      "loss": 0.4906,
      "step": 2640
    },
    {
      "epoch": 1.06,
      "grad_norm": 2.6660735607147217,
      "learning_rate": 3.233333333333333e-05,
      "loss": 0.5849,
      "step": 2650
    },
    {
      "epoch": 1.064,
      "grad_norm": 2.6779816150665283,
      "learning_rate": 3.226666666666667e-05,
      "loss": 0.5717,
      "step": 2660
    },
    {
      "epoch": 1.068,
      "grad_norm": 2.8703250885009766,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 0.5708,
      "step": 2670
    },
    {
      "epoch": 1.072,
      "grad_norm": 2.5337412357330322,
      "learning_rate": 3.213333333333334e-05,
      "loss": 0.5222,
      "step": 2680
    },
    {
      "epoch": 1.076,
      "grad_norm": 2.8947184085845947,
      "learning_rate": 3.206666666666667e-05,
      "loss": 0.59,
      "step": 2690
    },
    {
      "epoch": 1.08,
      "grad_norm": 2.2015790939331055,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.5117,
      "step": 2700
    },
    {
      "epoch": 1.084,
      "grad_norm": 2.4936985969543457,
      "learning_rate": 3.1933333333333335e-05,
      "loss": 0.6303,
      "step": 2710
    },
    {
      "epoch": 1.088,
      "grad_norm": 5.140611171722412,
      "learning_rate": 3.1866666666666664e-05,
      "loss": 0.5217,
      "step": 2720
    },
    {
      "epoch": 1.092,
      "grad_norm": 2.141317844390869,
      "learning_rate": 3.18e-05,
      "loss": 0.6006,
      "step": 2730
    },
    {
      "epoch": 1.096,
      "grad_norm": 2.8395304679870605,
      "learning_rate": 3.173333333333334e-05,
      "loss": 0.4972,
      "step": 2740
    },
    {
      "epoch": 1.1,
      "grad_norm": 2.1236813068389893,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.6151,
      "step": 2750
    },
    {
      "epoch": 1.104,
      "grad_norm": 2.6448774337768555,
      "learning_rate": 3.16e-05,
      "loss": 0.547,
      "step": 2760
    },
    {
      "epoch": 1.108,
      "grad_norm": 2.6999447345733643,
      "learning_rate": 3.153333333333334e-05,
      "loss": 0.5991,
      "step": 2770
    },
    {
      "epoch": 1.112,
      "grad_norm": 3.071772575378418,
      "learning_rate": 3.146666666666667e-05,
      "loss": 0.5398,
      "step": 2780
    },
    {
      "epoch": 1.116,
      "grad_norm": 4.109529495239258,
      "learning_rate": 3.1400000000000004e-05,
      "loss": 0.5612,
      "step": 2790
    },
    {
      "epoch": 1.12,
      "grad_norm": 2.472710132598877,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 0.6473,
      "step": 2800
    },
    {
      "epoch": 1.124,
      "grad_norm": 2.09323787689209,
      "learning_rate": 3.126666666666666e-05,
      "loss": 0.5137,
      "step": 2810
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 2.8422276973724365,
      "learning_rate": 3.12e-05,
      "loss": 0.5195,
      "step": 2820
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 2.380981206893921,
      "learning_rate": 3.1133333333333336e-05,
      "loss": 0.5168,
      "step": 2830
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 44.75027084350586,
      "learning_rate": 3.1066666666666665e-05,
      "loss": 0.5288,
      "step": 2840
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 2.603722333908081,
      "learning_rate": 3.1e-05,
      "loss": 0.5285,
      "step": 2850
    },
    {
      "epoch": 1.144,
      "grad_norm": 1.8930091857910156,
      "learning_rate": 3.093333333333334e-05,
      "loss": 0.5833,
      "step": 2860
    },
    {
      "epoch": 1.148,
      "grad_norm": 2.5535736083984375,
      "learning_rate": 3.086666666666667e-05,
      "loss": 0.6163,
      "step": 2870
    },
    {
      "epoch": 1.152,
      "grad_norm": 2.43931245803833,
      "learning_rate": 3.08e-05,
      "loss": 0.6117,
      "step": 2880
    },
    {
      "epoch": 1.156,
      "grad_norm": 2.4827685356140137,
      "learning_rate": 3.073333333333334e-05,
      "loss": 0.5753,
      "step": 2890
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.956377387046814,
      "learning_rate": 3.066666666666667e-05,
      "loss": 0.5965,
      "step": 2900
    },
    {
      "epoch": 1.164,
      "grad_norm": 3.410949945449829,
      "learning_rate": 3.06e-05,
      "loss": 0.5677,
      "step": 2910
    },
    {
      "epoch": 1.168,
      "grad_norm": 2.7929728031158447,
      "learning_rate": 3.0533333333333335e-05,
      "loss": 0.4914,
      "step": 2920
    },
    {
      "epoch": 1.172,
      "grad_norm": 2.4004483222961426,
      "learning_rate": 3.0466666666666664e-05,
      "loss": 0.6018,
      "step": 2930
    },
    {
      "epoch": 1.176,
      "grad_norm": 2.918816566467285,
      "learning_rate": 3.04e-05,
      "loss": 0.5304,
      "step": 2940
    },
    {
      "epoch": 1.18,
      "grad_norm": 3.0770721435546875,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 0.5596,
      "step": 2950
    },
    {
      "epoch": 1.184,
      "grad_norm": 2.594449758529663,
      "learning_rate": 3.0266666666666666e-05,
      "loss": 0.574,
      "step": 2960
    },
    {
      "epoch": 1.188,
      "grad_norm": 2.6278111934661865,
      "learning_rate": 3.02e-05,
      "loss": 0.515,
      "step": 2970
    },
    {
      "epoch": 1.192,
      "grad_norm": 2.71134877204895,
      "learning_rate": 3.0133333333333335e-05,
      "loss": 0.4593,
      "step": 2980
    },
    {
      "epoch": 1.196,
      "grad_norm": 2.007244825363159,
      "learning_rate": 3.006666666666667e-05,
      "loss": 0.6496,
      "step": 2990
    },
    {
      "epoch": 1.2,
      "grad_norm": 2.178554058074951,
      "learning_rate": 3e-05,
      "loss": 0.5585,
      "step": 3000
    },
    {
      "epoch": 1.204,
      "grad_norm": 2.0500736236572266,
      "learning_rate": 2.9933333333333337e-05,
      "loss": 0.5675,
      "step": 3010
    },
    {
      "epoch": 1.208,
      "grad_norm": 2.1186227798461914,
      "learning_rate": 2.986666666666667e-05,
      "loss": 0.6433,
      "step": 3020
    },
    {
      "epoch": 1.212,
      "grad_norm": 2.826533317565918,
      "learning_rate": 2.98e-05,
      "loss": 0.4598,
      "step": 3030
    },
    {
      "epoch": 1.216,
      "grad_norm": 3.3082082271575928,
      "learning_rate": 2.9733333333333336e-05,
      "loss": 0.5797,
      "step": 3040
    },
    {
      "epoch": 1.22,
      "grad_norm": 1.9431699514389038,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 0.5777,
      "step": 3050
    },
    {
      "epoch": 1.224,
      "grad_norm": 2.2149078845977783,
      "learning_rate": 2.96e-05,
      "loss": 0.4665,
      "step": 3060
    },
    {
      "epoch": 1.228,
      "grad_norm": 3.0878548622131348,
      "learning_rate": 2.9533333333333334e-05,
      "loss": 0.5357,
      "step": 3070
    },
    {
      "epoch": 1.232,
      "grad_norm": 2.933716058731079,
      "learning_rate": 2.946666666666667e-05,
      "loss": 0.5971,
      "step": 3080
    },
    {
      "epoch": 1.236,
      "grad_norm": 2.199892044067383,
      "learning_rate": 2.94e-05,
      "loss": 0.5217,
      "step": 3090
    },
    {
      "epoch": 1.24,
      "grad_norm": 2.210402011871338,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 0.5742,
      "step": 3100
    },
    {
      "epoch": 1.244,
      "grad_norm": 2.3698651790618896,
      "learning_rate": 2.926666666666667e-05,
      "loss": 0.5684,
      "step": 3110
    },
    {
      "epoch": 1.248,
      "grad_norm": 2.682136058807373,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 0.4758,
      "step": 3120
    },
    {
      "epoch": 1.252,
      "grad_norm": 2.5584003925323486,
      "learning_rate": 2.9133333333333334e-05,
      "loss": 0.6226,
      "step": 3130
    },
    {
      "epoch": 1.256,
      "grad_norm": 2.6435208320617676,
      "learning_rate": 2.906666666666667e-05,
      "loss": 0.5958,
      "step": 3140
    },
    {
      "epoch": 1.26,
      "grad_norm": 2.351266384124756,
      "learning_rate": 2.9e-05,
      "loss": 0.5715,
      "step": 3150
    },
    {
      "epoch": 1.264,
      "grad_norm": 1.948455572128296,
      "learning_rate": 2.8933333333333333e-05,
      "loss": 0.5858,
      "step": 3160
    },
    {
      "epoch": 1.268,
      "grad_norm": 2.390603542327881,
      "learning_rate": 2.886666666666667e-05,
      "loss": 0.53,
      "step": 3170
    },
    {
      "epoch": 1.272,
      "grad_norm": 2.264281749725342,
      "learning_rate": 2.88e-05,
      "loss": 0.6503,
      "step": 3180
    },
    {
      "epoch": 1.276,
      "grad_norm": 2.306478261947632,
      "learning_rate": 2.8733333333333335e-05,
      "loss": 0.4926,
      "step": 3190
    },
    {
      "epoch": 1.28,
      "grad_norm": 1.7660861015319824,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 0.5141,
      "step": 3200
    },
    {
      "epoch": 1.284,
      "grad_norm": 2.8526182174682617,
      "learning_rate": 2.86e-05,
      "loss": 0.5391,
      "step": 3210
    },
    {
      "epoch": 1.288,
      "grad_norm": 2.2063589096069336,
      "learning_rate": 2.8533333333333333e-05,
      "loss": 0.6244,
      "step": 3220
    },
    {
      "epoch": 1.292,
      "grad_norm": 2.372760772705078,
      "learning_rate": 2.846666666666667e-05,
      "loss": 0.4666,
      "step": 3230
    },
    {
      "epoch": 1.296,
      "grad_norm": 2.6642117500305176,
      "learning_rate": 2.84e-05,
      "loss": 0.606,
      "step": 3240
    },
    {
      "epoch": 1.3,
      "grad_norm": 1.835018515586853,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.4751,
      "step": 3250
    },
    {
      "epoch": 1.304,
      "grad_norm": 2.144289016723633,
      "learning_rate": 2.8266666666666668e-05,
      "loss": 0.5378,
      "step": 3260
    },
    {
      "epoch": 1.308,
      "grad_norm": 2.03112530708313,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 0.6499,
      "step": 3270
    },
    {
      "epoch": 1.312,
      "grad_norm": 2.084523916244507,
      "learning_rate": 2.8133333333333334e-05,
      "loss": 0.593,
      "step": 3280
    },
    {
      "epoch": 1.316,
      "grad_norm": 2.4675369262695312,
      "learning_rate": 2.806666666666667e-05,
      "loss": 0.5439,
      "step": 3290
    },
    {
      "epoch": 1.32,
      "grad_norm": 2.3889400959014893,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.5244,
      "step": 3300
    },
    {
      "epoch": 1.324,
      "grad_norm": 2.1612050533294678,
      "learning_rate": 2.7933333333333332e-05,
      "loss": 0.568,
      "step": 3310
    },
    {
      "epoch": 1.328,
      "grad_norm": 2.7764699459075928,
      "learning_rate": 2.786666666666667e-05,
      "loss": 0.5279,
      "step": 3320
    },
    {
      "epoch": 1.332,
      "grad_norm": 2.5509068965911865,
      "learning_rate": 2.7800000000000005e-05,
      "loss": 0.5053,
      "step": 3330
    },
    {
      "epoch": 1.336,
      "grad_norm": 2.2519688606262207,
      "learning_rate": 2.7733333333333334e-05,
      "loss": 0.5151,
      "step": 3340
    },
    {
      "epoch": 1.34,
      "grad_norm": 2.631394624710083,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 0.5954,
      "step": 3350
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 2.280141592025757,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 0.4988,
      "step": 3360
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 3.914807081222534,
      "learning_rate": 2.7533333333333333e-05,
      "loss": 0.551,
      "step": 3370
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 3.116687774658203,
      "learning_rate": 2.746666666666667e-05,
      "loss": 0.4683,
      "step": 3380
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 2.2520134449005127,
      "learning_rate": 2.7400000000000002e-05,
      "loss": 0.511,
      "step": 3390
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 2.497022867202759,
      "learning_rate": 2.733333333333333e-05,
      "loss": 0.5001,
      "step": 3400
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 4.335107803344727,
      "learning_rate": 2.7266666666666668e-05,
      "loss": 0.5234,
      "step": 3410
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 2.5183634757995605,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 0.5889,
      "step": 3420
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 2.662985324859619,
      "learning_rate": 2.7133333333333333e-05,
      "loss": 0.5145,
      "step": 3430
    },
    {
      "epoch": 1.376,
      "grad_norm": 2.1365253925323486,
      "learning_rate": 2.706666666666667e-05,
      "loss": 0.5244,
      "step": 3440
    },
    {
      "epoch": 1.38,
      "grad_norm": 2.3664395809173584,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.5215,
      "step": 3450
    },
    {
      "epoch": 1.384,
      "grad_norm": 2.27449893951416,
      "learning_rate": 2.6933333333333332e-05,
      "loss": 0.579,
      "step": 3460
    },
    {
      "epoch": 1.388,
      "grad_norm": 2.381026268005371,
      "learning_rate": 2.6866666666666668e-05,
      "loss": 0.5548,
      "step": 3470
    },
    {
      "epoch": 1.392,
      "grad_norm": 2.5347256660461426,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 0.5944,
      "step": 3480
    },
    {
      "epoch": 1.396,
      "grad_norm": 2.658463478088379,
      "learning_rate": 2.6733333333333334e-05,
      "loss": 0.6101,
      "step": 3490
    },
    {
      "epoch": 1.4,
      "grad_norm": 2.324704647064209,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.5072,
      "step": 3500
    },
    {
      "epoch": 1.404,
      "grad_norm": 2.2549397945404053,
      "learning_rate": 2.6600000000000003e-05,
      "loss": 0.6623,
      "step": 3510
    },
    {
      "epoch": 1.408,
      "grad_norm": 2.535644769668579,
      "learning_rate": 2.6533333333333332e-05,
      "loss": 0.5905,
      "step": 3520
    },
    {
      "epoch": 1.412,
      "grad_norm": 2.404960870742798,
      "learning_rate": 2.646666666666667e-05,
      "loss": 0.5298,
      "step": 3530
    },
    {
      "epoch": 1.416,
      "grad_norm": 1.989544153213501,
      "learning_rate": 2.64e-05,
      "loss": 0.5893,
      "step": 3540
    },
    {
      "epoch": 1.42,
      "grad_norm": 2.535506248474121,
      "learning_rate": 2.633333333333333e-05,
      "loss": 0.4792,
      "step": 3550
    },
    {
      "epoch": 1.424,
      "grad_norm": 3.232142686843872,
      "learning_rate": 2.6266666666666667e-05,
      "loss": 0.4746,
      "step": 3560
    },
    {
      "epoch": 1.428,
      "grad_norm": 2.3660287857055664,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 0.4966,
      "step": 3570
    },
    {
      "epoch": 1.432,
      "grad_norm": 1.797318935394287,
      "learning_rate": 2.6133333333333333e-05,
      "loss": 0.5757,
      "step": 3580
    },
    {
      "epoch": 1.436,
      "grad_norm": 2.1036300659179688,
      "learning_rate": 2.6066666666666666e-05,
      "loss": 0.5155,
      "step": 3590
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.7093980312347412,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.5389,
      "step": 3600
    },
    {
      "epoch": 1.444,
      "grad_norm": 2.6205174922943115,
      "learning_rate": 2.5933333333333338e-05,
      "loss": 0.5098,
      "step": 3610
    },
    {
      "epoch": 1.448,
      "grad_norm": 2.2114243507385254,
      "learning_rate": 2.5866666666666667e-05,
      "loss": 0.5319,
      "step": 3620
    },
    {
      "epoch": 1.452,
      "grad_norm": 2.4589736461639404,
      "learning_rate": 2.58e-05,
      "loss": 0.5305,
      "step": 3630
    },
    {
      "epoch": 1.456,
      "grad_norm": 3.0318057537078857,
      "learning_rate": 2.5733333333333337e-05,
      "loss": 0.5551,
      "step": 3640
    },
    {
      "epoch": 1.46,
      "grad_norm": 4.976061820983887,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 0.5838,
      "step": 3650
    },
    {
      "epoch": 1.464,
      "grad_norm": 3.007889986038208,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.5199,
      "step": 3660
    },
    {
      "epoch": 1.468,
      "grad_norm": 1.80058753490448,
      "learning_rate": 2.553333333333334e-05,
      "loss": 0.5603,
      "step": 3670
    },
    {
      "epoch": 1.472,
      "grad_norm": 2.450958728790283,
      "learning_rate": 2.5466666666666668e-05,
      "loss": 0.6245,
      "step": 3680
    },
    {
      "epoch": 1.476,
      "grad_norm": 1.7243884801864624,
      "learning_rate": 2.54e-05,
      "loss": 0.4937,
      "step": 3690
    },
    {
      "epoch": 1.48,
      "grad_norm": 3.2881319522857666,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 0.5023,
      "step": 3700
    },
    {
      "epoch": 1.484,
      "grad_norm": 2.2397043704986572,
      "learning_rate": 2.5266666666666666e-05,
      "loss": 0.5533,
      "step": 3710
    },
    {
      "epoch": 1.488,
      "grad_norm": 2.387552261352539,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 0.5447,
      "step": 3720
    },
    {
      "epoch": 1.492,
      "grad_norm": 2.475931167602539,
      "learning_rate": 2.5133333333333336e-05,
      "loss": 0.619,
      "step": 3730
    },
    {
      "epoch": 1.496,
      "grad_norm": 2.852412700653076,
      "learning_rate": 2.5066666666666665e-05,
      "loss": 0.5912,
      "step": 3740
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.8181626796722412,
      "learning_rate": 2.5e-05,
      "loss": 0.4895,
      "step": 3750
    },
    {
      "epoch": 1.504,
      "grad_norm": 2.363276243209839,
      "learning_rate": 2.4933333333333334e-05,
      "loss": 0.5097,
      "step": 3760
    },
    {
      "epoch": 1.508,
      "grad_norm": 3.0123438835144043,
      "learning_rate": 2.486666666666667e-05,
      "loss": 0.451,
      "step": 3770
    },
    {
      "epoch": 1.512,
      "grad_norm": 3.1521031856536865,
      "learning_rate": 2.48e-05,
      "loss": 0.5925,
      "step": 3780
    },
    {
      "epoch": 1.516,
      "grad_norm": 2.3171818256378174,
      "learning_rate": 2.4733333333333333e-05,
      "loss": 0.5341,
      "step": 3790
    },
    {
      "epoch": 1.52,
      "grad_norm": 2.152252674102783,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.5746,
      "step": 3800
    },
    {
      "epoch": 1.524,
      "grad_norm": 1.5899426937103271,
      "learning_rate": 2.46e-05,
      "loss": 0.5369,
      "step": 3810
    },
    {
      "epoch": 1.528,
      "grad_norm": 2.171128034591675,
      "learning_rate": 2.4533333333333334e-05,
      "loss": 0.5264,
      "step": 3820
    },
    {
      "epoch": 1.532,
      "grad_norm": 2.55289888381958,
      "learning_rate": 2.4466666666666667e-05,
      "loss": 0.5662,
      "step": 3830
    },
    {
      "epoch": 1.536,
      "grad_norm": 2.6579113006591797,
      "learning_rate": 2.44e-05,
      "loss": 0.5269,
      "step": 3840
    },
    {
      "epoch": 1.54,
      "grad_norm": 2.823261022567749,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 0.5366,
      "step": 3850
    },
    {
      "epoch": 1.544,
      "grad_norm": 2.362548351287842,
      "learning_rate": 2.426666666666667e-05,
      "loss": 0.5299,
      "step": 3860
    },
    {
      "epoch": 1.548,
      "grad_norm": 2.1668684482574463,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 0.5304,
      "step": 3870
    },
    {
      "epoch": 1.552,
      "grad_norm": 2.6746761798858643,
      "learning_rate": 2.4133333333333335e-05,
      "loss": 0.542,
      "step": 3880
    },
    {
      "epoch": 1.556,
      "grad_norm": 1.8749092817306519,
      "learning_rate": 2.4066666666666668e-05,
      "loss": 0.5588,
      "step": 3890
    },
    {
      "epoch": 1.56,
      "grad_norm": 2.2996628284454346,
      "learning_rate": 2.4e-05,
      "loss": 0.5153,
      "step": 3900
    },
    {
      "epoch": 1.564,
      "grad_norm": 2.1766889095306396,
      "learning_rate": 2.3933333333333337e-05,
      "loss": 0.5815,
      "step": 3910
    },
    {
      "epoch": 1.568,
      "grad_norm": 1.9473545551300049,
      "learning_rate": 2.3866666666666666e-05,
      "loss": 0.5113,
      "step": 3920
    },
    {
      "epoch": 1.572,
      "grad_norm": 2.4603986740112305,
      "learning_rate": 2.38e-05,
      "loss": 0.5964,
      "step": 3930
    },
    {
      "epoch": 1.576,
      "grad_norm": 3.1083552837371826,
      "learning_rate": 2.3733333333333335e-05,
      "loss": 0.5458,
      "step": 3940
    },
    {
      "epoch": 1.58,
      "grad_norm": 2.1669747829437256,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 0.651,
      "step": 3950
    },
    {
      "epoch": 1.584,
      "grad_norm": 2.4661030769348145,
      "learning_rate": 2.36e-05,
      "loss": 0.6927,
      "step": 3960
    },
    {
      "epoch": 1.588,
      "grad_norm": 2.3945329189300537,
      "learning_rate": 2.3533333333333334e-05,
      "loss": 0.5847,
      "step": 3970
    },
    {
      "epoch": 1.592,
      "grad_norm": 2.287825107574463,
      "learning_rate": 2.3466666666666667e-05,
      "loss": 0.5244,
      "step": 3980
    },
    {
      "epoch": 1.596,
      "grad_norm": 1.9354435205459595,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 0.5713,
      "step": 3990
    },
    {
      "epoch": 1.6,
      "grad_norm": 19.06871795654297,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.4607,
      "step": 4000
    },
    {
      "epoch": 1.604,
      "grad_norm": 2.787069320678711,
      "learning_rate": 2.326666666666667e-05,
      "loss": 0.5817,
      "step": 4010
    },
    {
      "epoch": 1.608,
      "grad_norm": 4.205909729003906,
      "learning_rate": 2.32e-05,
      "loss": 0.5023,
      "step": 4020
    },
    {
      "epoch": 1.612,
      "grad_norm": 2.079136610031128,
      "learning_rate": 2.3133333333333334e-05,
      "loss": 0.5115,
      "step": 4030
    },
    {
      "epoch": 1.616,
      "grad_norm": 2.0835771560668945,
      "learning_rate": 2.3066666666666667e-05,
      "loss": 0.5526,
      "step": 4040
    },
    {
      "epoch": 1.62,
      "grad_norm": 3.6681902408599854,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.5443,
      "step": 4050
    },
    {
      "epoch": 1.624,
      "grad_norm": 2.323775291442871,
      "learning_rate": 2.2933333333333333e-05,
      "loss": 0.5861,
      "step": 4060
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 3.2278997898101807,
      "learning_rate": 2.2866666666666666e-05,
      "loss": 0.503,
      "step": 4070
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 2.4011828899383545,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 0.4443,
      "step": 4080
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 2.175121545791626,
      "learning_rate": 2.2733333333333335e-05,
      "loss": 0.5043,
      "step": 4090
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 2.5369837284088135,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 0.5583,
      "step": 4100
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 2.773266315460205,
      "learning_rate": 2.26e-05,
      "loss": 0.5616,
      "step": 4110
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 2.48209547996521,
      "learning_rate": 2.2533333333333333e-05,
      "loss": 0.533,
      "step": 4120
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 3.633354663848877,
      "learning_rate": 2.2466666666666666e-05,
      "loss": 0.5126,
      "step": 4130
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 2.214877128601074,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.5602,
      "step": 4140
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 1.8239303827285767,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 0.5521,
      "step": 4150
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 2.26461124420166,
      "learning_rate": 2.2266666666666668e-05,
      "loss": 0.488,
      "step": 4160
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 3.436048984527588,
      "learning_rate": 2.22e-05,
      "loss": 0.5263,
      "step": 4170
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 2.6111061573028564,
      "learning_rate": 2.2133333333333334e-05,
      "loss": 0.5196,
      "step": 4180
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 2.244016408920288,
      "learning_rate": 2.206666666666667e-05,
      "loss": 0.5217,
      "step": 4190
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 1.816572666168213,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.5443,
      "step": 4200
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 2.4394075870513916,
      "learning_rate": 2.1933333333333332e-05,
      "loss": 0.501,
      "step": 4210
    },
    {
      "epoch": 1.688,
      "grad_norm": 3.5939176082611084,
      "learning_rate": 2.186666666666667e-05,
      "loss": 0.589,
      "step": 4220
    },
    {
      "epoch": 1.692,
      "grad_norm": 3.435979127883911,
      "learning_rate": 2.18e-05,
      "loss": 0.6002,
      "step": 4230
    },
    {
      "epoch": 1.696,
      "grad_norm": 1.576867938041687,
      "learning_rate": 2.1733333333333334e-05,
      "loss": 0.5656,
      "step": 4240
    },
    {
      "epoch": 1.7,
      "grad_norm": 2.742910861968994,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.4622,
      "step": 4250
    },
    {
      "epoch": 1.704,
      "grad_norm": 2.083153486251831,
      "learning_rate": 2.16e-05,
      "loss": 0.5383,
      "step": 4260
    },
    {
      "epoch": 1.708,
      "grad_norm": 1.756696343421936,
      "learning_rate": 2.1533333333333333e-05,
      "loss": 0.6184,
      "step": 4270
    },
    {
      "epoch": 1.712,
      "grad_norm": 2.558929204940796,
      "learning_rate": 2.146666666666667e-05,
      "loss": 0.5806,
      "step": 4280
    },
    {
      "epoch": 1.716,
      "grad_norm": 2.4422638416290283,
      "learning_rate": 2.1400000000000002e-05,
      "loss": 0.5815,
      "step": 4290
    },
    {
      "epoch": 1.72,
      "grad_norm": 1.7119942903518677,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.5894,
      "step": 4300
    },
    {
      "epoch": 1.724,
      "grad_norm": 2.4739432334899902,
      "learning_rate": 2.1266666666666667e-05,
      "loss": 0.5739,
      "step": 4310
    },
    {
      "epoch": 1.728,
      "grad_norm": 2.033318519592285,
      "learning_rate": 2.12e-05,
      "loss": 0.47,
      "step": 4320
    },
    {
      "epoch": 1.732,
      "grad_norm": 2.3351690769195557,
      "learning_rate": 2.1133333333333337e-05,
      "loss": 0.5189,
      "step": 4330
    },
    {
      "epoch": 1.736,
      "grad_norm": 1.9580572843551636,
      "learning_rate": 2.106666666666667e-05,
      "loss": 0.5435,
      "step": 4340
    },
    {
      "epoch": 1.74,
      "grad_norm": 2.353485345840454,
      "learning_rate": 2.1e-05,
      "loss": 0.533,
      "step": 4350
    },
    {
      "epoch": 1.744,
      "grad_norm": 1.8489799499511719,
      "learning_rate": 2.0933333333333335e-05,
      "loss": 0.5588,
      "step": 4360
    },
    {
      "epoch": 1.748,
      "grad_norm": 1.9759413003921509,
      "learning_rate": 2.0866666666666668e-05,
      "loss": 0.5791,
      "step": 4370
    },
    {
      "epoch": 1.752,
      "grad_norm": 1.9619678258895874,
      "learning_rate": 2.08e-05,
      "loss": 0.5112,
      "step": 4380
    },
    {
      "epoch": 1.756,
      "grad_norm": 2.565727949142456,
      "learning_rate": 2.0733333333333334e-05,
      "loss": 0.5018,
      "step": 4390
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.9110404253005981,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 0.4991,
      "step": 4400
    },
    {
      "epoch": 1.764,
      "grad_norm": 1.760087013244629,
      "learning_rate": 2.06e-05,
      "loss": 0.5826,
      "step": 4410
    },
    {
      "epoch": 1.768,
      "grad_norm": 2.1998398303985596,
      "learning_rate": 2.0533333333333336e-05,
      "loss": 0.5724,
      "step": 4420
    },
    {
      "epoch": 1.772,
      "grad_norm": 2.670938730239868,
      "learning_rate": 2.046666666666667e-05,
      "loss": 0.4758,
      "step": 4430
    },
    {
      "epoch": 1.776,
      "grad_norm": 1.7674425840377808,
      "learning_rate": 2.04e-05,
      "loss": 0.578,
      "step": 4440
    },
    {
      "epoch": 1.78,
      "grad_norm": 2.17170786857605,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 0.4958,
      "step": 4450
    },
    {
      "epoch": 1.784,
      "grad_norm": 2.577695369720459,
      "learning_rate": 2.0266666666666667e-05,
      "loss": 0.5223,
      "step": 4460
    },
    {
      "epoch": 1.788,
      "grad_norm": 2.3242743015289307,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 0.6242,
      "step": 4470
    },
    {
      "epoch": 1.792,
      "grad_norm": 2.242922782897949,
      "learning_rate": 2.0133333333333336e-05,
      "loss": 0.6671,
      "step": 4480
    },
    {
      "epoch": 1.796,
      "grad_norm": 2.11889386177063,
      "learning_rate": 2.0066666666666665e-05,
      "loss": 0.5151,
      "step": 4490
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.8066719770431519,
      "learning_rate": 2e-05,
      "loss": 0.6455,
      "step": 4500
    },
    {
      "epoch": 1.804,
      "grad_norm": 2.8094253540039062,
      "learning_rate": 1.9933333333333334e-05,
      "loss": 0.5882,
      "step": 4510
    },
    {
      "epoch": 1.808,
      "grad_norm": 3.0659677982330322,
      "learning_rate": 1.9866666666666667e-05,
      "loss": 0.5956,
      "step": 4520
    },
    {
      "epoch": 1.812,
      "grad_norm": 5.280172348022461,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 0.4778,
      "step": 4530
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 2.232700824737549,
      "learning_rate": 1.9733333333333333e-05,
      "loss": 0.5685,
      "step": 4540
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 1.9129289388656616,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.4851,
      "step": 4550
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 2.108018636703491,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.5036,
      "step": 4560
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 2.12245512008667,
      "learning_rate": 1.9533333333333335e-05,
      "loss": 0.4711,
      "step": 4570
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 2.1452364921569824,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 0.5984,
      "step": 4580
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 1.961944818496704,
      "learning_rate": 1.94e-05,
      "loss": 0.5287,
      "step": 4590
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 1.9676533937454224,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.5316,
      "step": 4600
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 3.090869665145874,
      "learning_rate": 1.926666666666667e-05,
      "loss": 0.5351,
      "step": 4610
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 3.1781814098358154,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.5152,
      "step": 4620
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 1.751853346824646,
      "learning_rate": 1.9133333333333332e-05,
      "loss": 0.5475,
      "step": 4630
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 1.9599031209945679,
      "learning_rate": 1.9066666666666668e-05,
      "loss": 0.5697,
      "step": 4640
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 2.0660877227783203,
      "learning_rate": 1.9e-05,
      "loss": 0.4786,
      "step": 4650
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 3.1387202739715576,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 0.4871,
      "step": 4660
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 2.3605523109436035,
      "learning_rate": 1.886666666666667e-05,
      "loss": 0.5933,
      "step": 4670
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 2.6465938091278076,
      "learning_rate": 1.88e-05,
      "loss": 0.5751,
      "step": 4680
    },
    {
      "epoch": 1.876,
      "grad_norm": 2.3510377407073975,
      "learning_rate": 1.8733333333333332e-05,
      "loss": 0.6681,
      "step": 4690
    },
    {
      "epoch": 1.88,
      "grad_norm": 2.874154567718506,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.5593,
      "step": 4700
    },
    {
      "epoch": 1.884,
      "grad_norm": 2.715630054473877,
      "learning_rate": 1.86e-05,
      "loss": 0.6289,
      "step": 4710
    },
    {
      "epoch": 1.888,
      "grad_norm": 2.060349464416504,
      "learning_rate": 1.8533333333333334e-05,
      "loss": 0.5292,
      "step": 4720
    },
    {
      "epoch": 1.892,
      "grad_norm": 1.830480694770813,
      "learning_rate": 1.8466666666666667e-05,
      "loss": 0.5265,
      "step": 4730
    },
    {
      "epoch": 1.896,
      "grad_norm": 2.0549144744873047,
      "learning_rate": 1.84e-05,
      "loss": 0.5227,
      "step": 4740
    },
    {
      "epoch": 1.9,
      "grad_norm": 2.847236394882202,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.548,
      "step": 4750
    },
    {
      "epoch": 1.904,
      "grad_norm": 2.1123225688934326,
      "learning_rate": 1.826666666666667e-05,
      "loss": 0.5734,
      "step": 4760
    },
    {
      "epoch": 1.908,
      "grad_norm": 1.9362133741378784,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 0.5254,
      "step": 4770
    },
    {
      "epoch": 1.912,
      "grad_norm": 2.2437615394592285,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 0.5366,
      "step": 4780
    },
    {
      "epoch": 1.916,
      "grad_norm": 2.3114254474639893,
      "learning_rate": 1.8066666666666668e-05,
      "loss": 0.5788,
      "step": 4790
    },
    {
      "epoch": 1.92,
      "grad_norm": 2.009161949157715,
      "learning_rate": 1.8e-05,
      "loss": 0.5598,
      "step": 4800
    },
    {
      "epoch": 1.924,
      "grad_norm": 2.0719475746154785,
      "learning_rate": 1.7933333333333337e-05,
      "loss": 0.5149,
      "step": 4810
    },
    {
      "epoch": 1.928,
      "grad_norm": 2.2413129806518555,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 0.5378,
      "step": 4820
    },
    {
      "epoch": 1.932,
      "grad_norm": 2.7008862495422363,
      "learning_rate": 1.78e-05,
      "loss": 0.558,
      "step": 4830
    },
    {
      "epoch": 1.936,
      "grad_norm": 2.5141913890838623,
      "learning_rate": 1.7733333333333335e-05,
      "loss": 0.546,
      "step": 4840
    },
    {
      "epoch": 1.94,
      "grad_norm": 1.883105993270874,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 0.5142,
      "step": 4850
    },
    {
      "epoch": 1.944,
      "grad_norm": 2.1004738807678223,
      "learning_rate": 1.76e-05,
      "loss": 0.5348,
      "step": 4860
    },
    {
      "epoch": 1.948,
      "grad_norm": 1.990462064743042,
      "learning_rate": 1.7533333333333334e-05,
      "loss": 0.4881,
      "step": 4870
    },
    {
      "epoch": 1.952,
      "grad_norm": 2.0590450763702393,
      "learning_rate": 1.7466666666666667e-05,
      "loss": 0.5227,
      "step": 4880
    },
    {
      "epoch": 1.956,
      "grad_norm": 2.5600404739379883,
      "learning_rate": 1.74e-05,
      "loss": 0.5849,
      "step": 4890
    },
    {
      "epoch": 1.96,
      "grad_norm": 2.0442988872528076,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.4738,
      "step": 4900
    },
    {
      "epoch": 1.964,
      "grad_norm": 2.00295352935791,
      "learning_rate": 1.726666666666667e-05,
      "loss": 0.5217,
      "step": 4910
    },
    {
      "epoch": 1.968,
      "grad_norm": 2.1189842224121094,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.4916,
      "step": 4920
    },
    {
      "epoch": 1.972,
      "grad_norm": 2.1725523471832275,
      "learning_rate": 1.7133333333333334e-05,
      "loss": 0.6244,
      "step": 4930
    },
    {
      "epoch": 1.976,
      "grad_norm": 2.2087409496307373,
      "learning_rate": 1.7066666666666667e-05,
      "loss": 0.4756,
      "step": 4940
    },
    {
      "epoch": 1.98,
      "grad_norm": 1.858964204788208,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.6271,
      "step": 4950
    },
    {
      "epoch": 1.984,
      "grad_norm": 1.7065292596817017,
      "learning_rate": 1.6933333333333333e-05,
      "loss": 0.5106,
      "step": 4960
    },
    {
      "epoch": 1.988,
      "grad_norm": 2.6821751594543457,
      "learning_rate": 1.6866666666666666e-05,
      "loss": 0.5517,
      "step": 4970
    },
    {
      "epoch": 1.992,
      "grad_norm": 2.0604734420776367,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.4837,
      "step": 4980
    },
    {
      "epoch": 1.996,
      "grad_norm": 1.9423762559890747,
      "learning_rate": 1.6733333333333335e-05,
      "loss": 0.5051,
      "step": 4990
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.6063151359558105,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.5812,
      "step": 5000
    },
    {
      "epoch": 2.004,
      "grad_norm": 1.9268015623092651,
      "learning_rate": 1.66e-05,
      "loss": 0.5687,
      "step": 5010
    },
    {
      "epoch": 2.008,
      "grad_norm": 2.1471962928771973,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 0.5737,
      "step": 5020
    },
    {
      "epoch": 2.012,
      "grad_norm": 2.3292109966278076,
      "learning_rate": 1.6466666666666666e-05,
      "loss": 0.5369,
      "step": 5030
    },
    {
      "epoch": 2.016,
      "grad_norm": 2.5969924926757812,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 0.5371,
      "step": 5040
    },
    {
      "epoch": 2.02,
      "grad_norm": 1.9879010915756226,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.4649,
      "step": 5050
    },
    {
      "epoch": 2.024,
      "grad_norm": 2.1154017448425293,
      "learning_rate": 1.6266666666666665e-05,
      "loss": 0.5959,
      "step": 5060
    },
    {
      "epoch": 2.028,
      "grad_norm": 2.5543694496154785,
      "learning_rate": 1.62e-05,
      "loss": 0.5888,
      "step": 5070
    },
    {
      "epoch": 2.032,
      "grad_norm": 2.2136495113372803,
      "learning_rate": 1.6133333333333334e-05,
      "loss": 0.5204,
      "step": 5080
    },
    {
      "epoch": 2.036,
      "grad_norm": 1.9073597192764282,
      "learning_rate": 1.606666666666667e-05,
      "loss": 0.5481,
      "step": 5090
    },
    {
      "epoch": 2.04,
      "grad_norm": 2.5787761211395264,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.5233,
      "step": 5100
    },
    {
      "epoch": 2.044,
      "grad_norm": 2.3238162994384766,
      "learning_rate": 1.5933333333333332e-05,
      "loss": 0.5993,
      "step": 5110
    },
    {
      "epoch": 2.048,
      "grad_norm": 1.96769380569458,
      "learning_rate": 1.586666666666667e-05,
      "loss": 0.5245,
      "step": 5120
    },
    {
      "epoch": 2.052,
      "grad_norm": 2.3425583839416504,
      "learning_rate": 1.58e-05,
      "loss": 0.5495,
      "step": 5130
    },
    {
      "epoch": 2.056,
      "grad_norm": 2.173201560974121,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 0.5095,
      "step": 5140
    },
    {
      "epoch": 2.06,
      "grad_norm": 2.6006345748901367,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 0.4754,
      "step": 5150
    },
    {
      "epoch": 2.064,
      "grad_norm": 2.157214879989624,
      "learning_rate": 1.56e-05,
      "loss": 0.4907,
      "step": 5160
    },
    {
      "epoch": 2.068,
      "grad_norm": 2.2984840869903564,
      "learning_rate": 1.5533333333333333e-05,
      "loss": 0.5683,
      "step": 5170
    },
    {
      "epoch": 2.072,
      "grad_norm": 2.9868111610412598,
      "learning_rate": 1.546666666666667e-05,
      "loss": 0.4982,
      "step": 5180
    },
    {
      "epoch": 2.076,
      "grad_norm": 1.725494384765625,
      "learning_rate": 1.54e-05,
      "loss": 0.4911,
      "step": 5190
    },
    {
      "epoch": 2.08,
      "grad_norm": 3.2266595363616943,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 0.4995,
      "step": 5200
    },
    {
      "epoch": 2.084,
      "grad_norm": 2.5290913581848145,
      "learning_rate": 1.5266666666666667e-05,
      "loss": 0.5847,
      "step": 5210
    },
    {
      "epoch": 2.088,
      "grad_norm": 1.9886412620544434,
      "learning_rate": 1.52e-05,
      "loss": 0.5326,
      "step": 5220
    },
    {
      "epoch": 2.092,
      "grad_norm": 2.370490550994873,
      "learning_rate": 1.5133333333333333e-05,
      "loss": 0.5134,
      "step": 5230
    },
    {
      "epoch": 2.096,
      "grad_norm": 2.788187026977539,
      "learning_rate": 1.5066666666666668e-05,
      "loss": 0.5014,
      "step": 5240
    },
    {
      "epoch": 2.1,
      "grad_norm": 2.175339460372925,
      "learning_rate": 1.5e-05,
      "loss": 0.5461,
      "step": 5250
    },
    {
      "epoch": 2.104,
      "grad_norm": 1.72512948513031,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 0.4923,
      "step": 5260
    },
    {
      "epoch": 2.108,
      "grad_norm": 2.4187726974487305,
      "learning_rate": 1.4866666666666668e-05,
      "loss": 0.5555,
      "step": 5270
    },
    {
      "epoch": 2.112,
      "grad_norm": 2.128826141357422,
      "learning_rate": 1.48e-05,
      "loss": 0.5114,
      "step": 5280
    },
    {
      "epoch": 2.116,
      "grad_norm": 2.2366113662719727,
      "learning_rate": 1.4733333333333335e-05,
      "loss": 0.4962,
      "step": 5290
    },
    {
      "epoch": 2.12,
      "grad_norm": 2.3171308040618896,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 0.5001,
      "step": 5300
    },
    {
      "epoch": 2.124,
      "grad_norm": 2.3129730224609375,
      "learning_rate": 1.4599999999999999e-05,
      "loss": 0.5221,
      "step": 5310
    },
    {
      "epoch": 2.128,
      "grad_norm": 2.5912976264953613,
      "learning_rate": 1.4533333333333335e-05,
      "loss": 0.5098,
      "step": 5320
    },
    {
      "epoch": 2.132,
      "grad_norm": 2.5978214740753174,
      "learning_rate": 1.4466666666666667e-05,
      "loss": 0.5836,
      "step": 5330
    },
    {
      "epoch": 2.136,
      "grad_norm": 1.8498557806015015,
      "learning_rate": 1.44e-05,
      "loss": 0.5025,
      "step": 5340
    },
    {
      "epoch": 2.14,
      "grad_norm": 2.287787675857544,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 0.5585,
      "step": 5350
    },
    {
      "epoch": 2.144,
      "grad_norm": 2.4594879150390625,
      "learning_rate": 1.4266666666666667e-05,
      "loss": 0.5186,
      "step": 5360
    },
    {
      "epoch": 2.148,
      "grad_norm": 1.8137781620025635,
      "learning_rate": 1.42e-05,
      "loss": 0.5408,
      "step": 5370
    },
    {
      "epoch": 2.152,
      "grad_norm": 2.004619836807251,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 0.5006,
      "step": 5380
    },
    {
      "epoch": 2.156,
      "grad_norm": 2.220845937728882,
      "learning_rate": 1.4066666666666667e-05,
      "loss": 0.5158,
      "step": 5390
    },
    {
      "epoch": 2.16,
      "grad_norm": 2.2658944129943848,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.5895,
      "step": 5400
    },
    {
      "epoch": 2.164,
      "grad_norm": 2.994100570678711,
      "learning_rate": 1.3933333333333334e-05,
      "loss": 0.4578,
      "step": 5410
    },
    {
      "epoch": 2.168,
      "grad_norm": 1.960695743560791,
      "learning_rate": 1.3866666666666667e-05,
      "loss": 0.552,
      "step": 5420
    },
    {
      "epoch": 2.172,
      "grad_norm": 1.7189912796020508,
      "learning_rate": 1.3800000000000002e-05,
      "loss": 0.5062,
      "step": 5430
    },
    {
      "epoch": 2.176,
      "grad_norm": 2.1373908519744873,
      "learning_rate": 1.3733333333333335e-05,
      "loss": 0.5938,
      "step": 5440
    },
    {
      "epoch": 2.18,
      "grad_norm": 2.5250306129455566,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 0.6174,
      "step": 5450
    },
    {
      "epoch": 2.184,
      "grad_norm": 1.791917085647583,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.4967,
      "step": 5460
    },
    {
      "epoch": 2.188,
      "grad_norm": 2.285585880279541,
      "learning_rate": 1.3533333333333335e-05,
      "loss": 0.4928,
      "step": 5470
    },
    {
      "epoch": 2.192,
      "grad_norm": 3.5855302810668945,
      "learning_rate": 1.3466666666666666e-05,
      "loss": 0.5363,
      "step": 5480
    },
    {
      "epoch": 2.196,
      "grad_norm": 2.0543978214263916,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 0.4939,
      "step": 5490
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.9361047744750977,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.5285,
      "step": 5500
    },
    {
      "epoch": 2.204,
      "grad_norm": 2.9473040103912354,
      "learning_rate": 1.3266666666666666e-05,
      "loss": 0.5356,
      "step": 5510
    },
    {
      "epoch": 2.208,
      "grad_norm": 2.2679250240325928,
      "learning_rate": 1.32e-05,
      "loss": 0.5359,
      "step": 5520
    },
    {
      "epoch": 2.212,
      "grad_norm": 1.7356536388397217,
      "learning_rate": 1.3133333333333334e-05,
      "loss": 0.474,
      "step": 5530
    },
    {
      "epoch": 2.216,
      "grad_norm": 2.2165427207946777,
      "learning_rate": 1.3066666666666666e-05,
      "loss": 0.4684,
      "step": 5540
    },
    {
      "epoch": 2.22,
      "grad_norm": 2.165163516998291,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.5001,
      "step": 5550
    },
    {
      "epoch": 2.224,
      "grad_norm": 1.7320935726165771,
      "learning_rate": 1.2933333333333334e-05,
      "loss": 0.5155,
      "step": 5560
    },
    {
      "epoch": 2.228,
      "grad_norm": 2.812762498855591,
      "learning_rate": 1.2866666666666668e-05,
      "loss": 0.5756,
      "step": 5570
    },
    {
      "epoch": 2.232,
      "grad_norm": 2.082085132598877,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.4094,
      "step": 5580
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 3.4311366081237793,
      "learning_rate": 1.2733333333333334e-05,
      "loss": 0.528,
      "step": 5590
    },
    {
      "epoch": 2.24,
      "grad_norm": 3.0518460273742676,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 0.5305,
      "step": 5600
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 2.5309958457946777,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 0.5389,
      "step": 5610
    },
    {
      "epoch": 2.248,
      "grad_norm": 2.332059860229492,
      "learning_rate": 1.2533333333333332e-05,
      "loss": 0.4559,
      "step": 5620
    },
    {
      "epoch": 2.252,
      "grad_norm": 2.269517183303833,
      "learning_rate": 1.2466666666666667e-05,
      "loss": 0.5136,
      "step": 5630
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 2.262687921524048,
      "learning_rate": 1.24e-05,
      "loss": 0.5433,
      "step": 5640
    },
    {
      "epoch": 2.26,
      "grad_norm": 2.313384771347046,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 0.5485,
      "step": 5650
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 2.3574540615081787,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 0.5288,
      "step": 5660
    },
    {
      "epoch": 2.268,
      "grad_norm": 2.0985491275787354,
      "learning_rate": 1.22e-05,
      "loss": 0.5111,
      "step": 5670
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 2.262803316116333,
      "learning_rate": 1.2133333333333335e-05,
      "loss": 0.5239,
      "step": 5680
    },
    {
      "epoch": 2.276,
      "grad_norm": 2.5567376613616943,
      "learning_rate": 1.2066666666666667e-05,
      "loss": 0.5151,
      "step": 5690
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 2.0447123050689697,
      "learning_rate": 1.2e-05,
      "loss": 0.4616,
      "step": 5700
    },
    {
      "epoch": 2.284,
      "grad_norm": 4.104591369628906,
      "learning_rate": 1.1933333333333333e-05,
      "loss": 0.5989,
      "step": 5710
    },
    {
      "epoch": 2.288,
      "grad_norm": 2.562457799911499,
      "learning_rate": 1.1866666666666668e-05,
      "loss": 0.5431,
      "step": 5720
    },
    {
      "epoch": 2.292,
      "grad_norm": 3.1470563411712646,
      "learning_rate": 1.18e-05,
      "loss": 0.5257,
      "step": 5730
    },
    {
      "epoch": 2.296,
      "grad_norm": 2.2908530235290527,
      "learning_rate": 1.1733333333333333e-05,
      "loss": 0.5659,
      "step": 5740
    },
    {
      "epoch": 2.3,
      "grad_norm": 2.7490358352661133,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.6282,
      "step": 5750
    },
    {
      "epoch": 2.304,
      "grad_norm": 2.08878231048584,
      "learning_rate": 1.16e-05,
      "loss": 0.5839,
      "step": 5760
    },
    {
      "epoch": 2.308,
      "grad_norm": 2.044708013534546,
      "learning_rate": 1.1533333333333334e-05,
      "loss": 0.4512,
      "step": 5770
    },
    {
      "epoch": 2.312,
      "grad_norm": 2.0334479808807373,
      "learning_rate": 1.1466666666666666e-05,
      "loss": 0.491,
      "step": 5780
    },
    {
      "epoch": 2.316,
      "grad_norm": 2.157508134841919,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 0.5439,
      "step": 5790
    },
    {
      "epoch": 2.32,
      "grad_norm": 3.199862003326416,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.606,
      "step": 5800
    },
    {
      "epoch": 2.324,
      "grad_norm": 2.7161781787872314,
      "learning_rate": 1.1266666666666667e-05,
      "loss": 0.5131,
      "step": 5810
    },
    {
      "epoch": 2.328,
      "grad_norm": 1.6622834205627441,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.6632,
      "step": 5820
    },
    {
      "epoch": 2.332,
      "grad_norm": 3.12506365776062,
      "learning_rate": 1.1133333333333334e-05,
      "loss": 0.4532,
      "step": 5830
    },
    {
      "epoch": 2.336,
      "grad_norm": 2.0794835090637207,
      "learning_rate": 1.1066666666666667e-05,
      "loss": 0.5413,
      "step": 5840
    },
    {
      "epoch": 2.34,
      "grad_norm": 2.043395757675171,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.4538,
      "step": 5850
    },
    {
      "epoch": 2.344,
      "grad_norm": 1.8081257343292236,
      "learning_rate": 1.0933333333333334e-05,
      "loss": 0.4632,
      "step": 5860
    },
    {
      "epoch": 2.348,
      "grad_norm": 2.035989761352539,
      "learning_rate": 1.0866666666666667e-05,
      "loss": 0.5315,
      "step": 5870
    },
    {
      "epoch": 2.352,
      "grad_norm": 2.055539131164551,
      "learning_rate": 1.08e-05,
      "loss": 0.4772,
      "step": 5880
    },
    {
      "epoch": 2.356,
      "grad_norm": 2.036921739578247,
      "learning_rate": 1.0733333333333334e-05,
      "loss": 0.5789,
      "step": 5890
    },
    {
      "epoch": 2.36,
      "grad_norm": 2.3737170696258545,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.52,
      "step": 5900
    },
    {
      "epoch": 2.364,
      "grad_norm": 2.5431299209594727,
      "learning_rate": 1.06e-05,
      "loss": 0.5749,
      "step": 5910
    },
    {
      "epoch": 2.368,
      "grad_norm": 2.2848737239837646,
      "learning_rate": 1.0533333333333335e-05,
      "loss": 0.5005,
      "step": 5920
    },
    {
      "epoch": 2.372,
      "grad_norm": 2.1330811977386475,
      "learning_rate": 1.0466666666666668e-05,
      "loss": 0.5153,
      "step": 5930
    },
    {
      "epoch": 2.376,
      "grad_norm": 2.8186588287353516,
      "learning_rate": 1.04e-05,
      "loss": 0.6014,
      "step": 5940
    },
    {
      "epoch": 2.38,
      "grad_norm": 1.8302453756332397,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 0.5361,
      "step": 5950
    },
    {
      "epoch": 2.384,
      "grad_norm": 2.063227891921997,
      "learning_rate": 1.0266666666666668e-05,
      "loss": 0.5129,
      "step": 5960
    },
    {
      "epoch": 2.388,
      "grad_norm": 5.33302640914917,
      "learning_rate": 1.02e-05,
      "loss": 0.5482,
      "step": 5970
    },
    {
      "epoch": 2.392,
      "grad_norm": 2.1581320762634277,
      "learning_rate": 1.0133333333333333e-05,
      "loss": 0.4972,
      "step": 5980
    },
    {
      "epoch": 2.396,
      "grad_norm": 2.4132821559906006,
      "learning_rate": 1.0066666666666668e-05,
      "loss": 0.5556,
      "step": 5990
    },
    {
      "epoch": 2.4,
      "grad_norm": 2.2828283309936523,
      "learning_rate": 1e-05,
      "loss": 0.5588,
      "step": 6000
    },
    {
      "epoch": 2.404,
      "grad_norm": 1.608449101448059,
      "learning_rate": 9.933333333333334e-06,
      "loss": 0.5223,
      "step": 6010
    },
    {
      "epoch": 2.408,
      "grad_norm": 2.5481460094451904,
      "learning_rate": 9.866666666666667e-06,
      "loss": 0.5302,
      "step": 6020
    },
    {
      "epoch": 2.412,
      "grad_norm": 2.319958448410034,
      "learning_rate": 9.800000000000001e-06,
      "loss": 0.5682,
      "step": 6030
    },
    {
      "epoch": 2.416,
      "grad_norm": 2.1243882179260254,
      "learning_rate": 9.733333333333334e-06,
      "loss": 0.5712,
      "step": 6040
    },
    {
      "epoch": 2.42,
      "grad_norm": 2.3021719455718994,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.4867,
      "step": 6050
    },
    {
      "epoch": 2.424,
      "grad_norm": 2.5813076496124268,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.4467,
      "step": 6060
    },
    {
      "epoch": 2.428,
      "grad_norm": 2.316950559616089,
      "learning_rate": 9.533333333333334e-06,
      "loss": 0.5908,
      "step": 6070
    },
    {
      "epoch": 2.432,
      "grad_norm": 2.1763217449188232,
      "learning_rate": 9.466666666666667e-06,
      "loss": 0.4736,
      "step": 6080
    },
    {
      "epoch": 2.436,
      "grad_norm": 2.108563184738159,
      "learning_rate": 9.4e-06,
      "loss": 0.4675,
      "step": 6090
    },
    {
      "epoch": 2.44,
      "grad_norm": 2.4303436279296875,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.5561,
      "step": 6100
    },
    {
      "epoch": 2.444,
      "grad_norm": 2.315274953842163,
      "learning_rate": 9.266666666666667e-06,
      "loss": 0.5648,
      "step": 6110
    },
    {
      "epoch": 2.448,
      "grad_norm": 2.1150619983673096,
      "learning_rate": 9.2e-06,
      "loss": 0.6014,
      "step": 6120
    },
    {
      "epoch": 2.452,
      "grad_norm": 2.06052303314209,
      "learning_rate": 9.133333333333335e-06,
      "loss": 0.5088,
      "step": 6130
    },
    {
      "epoch": 2.456,
      "grad_norm": 2.1758296489715576,
      "learning_rate": 9.066666666666667e-06,
      "loss": 0.4913,
      "step": 6140
    },
    {
      "epoch": 2.46,
      "grad_norm": 2.412534236907959,
      "learning_rate": 9e-06,
      "loss": 0.5801,
      "step": 6150
    },
    {
      "epoch": 2.464,
      "grad_norm": 1.943315863609314,
      "learning_rate": 8.933333333333333e-06,
      "loss": 0.5571,
      "step": 6160
    },
    {
      "epoch": 2.468,
      "grad_norm": 2.29164457321167,
      "learning_rate": 8.866666666666668e-06,
      "loss": 0.5478,
      "step": 6170
    },
    {
      "epoch": 2.472,
      "grad_norm": 2.4028422832489014,
      "learning_rate": 8.8e-06,
      "loss": 0.537,
      "step": 6180
    },
    {
      "epoch": 2.476,
      "grad_norm": 2.2342095375061035,
      "learning_rate": 8.733333333333333e-06,
      "loss": 0.6605,
      "step": 6190
    },
    {
      "epoch": 2.48,
      "grad_norm": 2.413715124130249,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.6601,
      "step": 6200
    },
    {
      "epoch": 2.484,
      "grad_norm": 2.295849084854126,
      "learning_rate": 8.599999999999999e-06,
      "loss": 0.5112,
      "step": 6210
    },
    {
      "epoch": 2.488,
      "grad_norm": 1.9310402870178223,
      "learning_rate": 8.533333333333334e-06,
      "loss": 0.5897,
      "step": 6220
    },
    {
      "epoch": 2.492,
      "grad_norm": 2.1279759407043457,
      "learning_rate": 8.466666666666666e-06,
      "loss": 0.5072,
      "step": 6230
    },
    {
      "epoch": 2.496,
      "grad_norm": 2.8058390617370605,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.5765,
      "step": 6240
    },
    {
      "epoch": 2.5,
      "grad_norm": 2.048311471939087,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.4861,
      "step": 6250
    },
    {
      "epoch": 2.504,
      "grad_norm": 3.381383180618286,
      "learning_rate": 8.266666666666667e-06,
      "loss": 0.5707,
      "step": 6260
    },
    {
      "epoch": 2.508,
      "grad_norm": 3.1024887561798096,
      "learning_rate": 8.200000000000001e-06,
      "loss": 0.4819,
      "step": 6270
    },
    {
      "epoch": 2.512,
      "grad_norm": 2.6500964164733887,
      "learning_rate": 8.133333333333332e-06,
      "loss": 0.4338,
      "step": 6280
    },
    {
      "epoch": 2.516,
      "grad_norm": 2.404740333557129,
      "learning_rate": 8.066666666666667e-06,
      "loss": 0.4592,
      "step": 6290
    },
    {
      "epoch": 2.52,
      "grad_norm": 2.392773389816284,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.6016,
      "step": 6300
    },
    {
      "epoch": 2.524,
      "grad_norm": 2.072258472442627,
      "learning_rate": 7.933333333333334e-06,
      "loss": 0.4739,
      "step": 6310
    },
    {
      "epoch": 2.528,
      "grad_norm": 2.434253215789795,
      "learning_rate": 7.866666666666667e-06,
      "loss": 0.4039,
      "step": 6320
    },
    {
      "epoch": 2.532,
      "grad_norm": 2.3363914489746094,
      "learning_rate": 7.8e-06,
      "loss": 0.5035,
      "step": 6330
    },
    {
      "epoch": 2.536,
      "grad_norm": 2.2566850185394287,
      "learning_rate": 7.733333333333334e-06,
      "loss": 0.4931,
      "step": 6340
    },
    {
      "epoch": 2.54,
      "grad_norm": 2.8379666805267334,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.6054,
      "step": 6350
    },
    {
      "epoch": 2.544,
      "grad_norm": 2.8697643280029297,
      "learning_rate": 7.6e-06,
      "loss": 0.5859,
      "step": 6360
    },
    {
      "epoch": 2.548,
      "grad_norm": 2.116535186767578,
      "learning_rate": 7.533333333333334e-06,
      "loss": 0.5386,
      "step": 6370
    },
    {
      "epoch": 2.552,
      "grad_norm": 1.881447434425354,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 0.5157,
      "step": 6380
    },
    {
      "epoch": 2.556,
      "grad_norm": 2.973104476928711,
      "learning_rate": 7.4e-06,
      "loss": 0.5276,
      "step": 6390
    },
    {
      "epoch": 2.56,
      "grad_norm": 2.139194965362549,
      "learning_rate": 7.333333333333334e-06,
      "loss": 0.4998,
      "step": 6400
    },
    {
      "epoch": 2.564,
      "grad_norm": 2.6282052993774414,
      "learning_rate": 7.266666666666668e-06,
      "loss": 0.4843,
      "step": 6410
    },
    {
      "epoch": 2.568,
      "grad_norm": 2.2755801677703857,
      "learning_rate": 7.2e-06,
      "loss": 0.5471,
      "step": 6420
    },
    {
      "epoch": 2.572,
      "grad_norm": 3.2505383491516113,
      "learning_rate": 7.133333333333333e-06,
      "loss": 0.579,
      "step": 6430
    },
    {
      "epoch": 2.576,
      "grad_norm": 2.261986494064331,
      "learning_rate": 7.066666666666667e-06,
      "loss": 0.6163,
      "step": 6440
    },
    {
      "epoch": 2.58,
      "grad_norm": 2.3392539024353027,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.5485,
      "step": 6450
    },
    {
      "epoch": 2.584,
      "grad_norm": 2.1654696464538574,
      "learning_rate": 6.933333333333334e-06,
      "loss": 0.5649,
      "step": 6460
    },
    {
      "epoch": 2.588,
      "grad_norm": 3.0784926414489746,
      "learning_rate": 6.866666666666667e-06,
      "loss": 0.4983,
      "step": 6470
    },
    {
      "epoch": 2.592,
      "grad_norm": 2.0948524475097656,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.5804,
      "step": 6480
    },
    {
      "epoch": 2.596,
      "grad_norm": 2.4834156036376953,
      "learning_rate": 6.733333333333333e-06,
      "loss": 0.4567,
      "step": 6490
    },
    {
      "epoch": 2.6,
      "grad_norm": 2.347059726715088,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.529,
      "step": 6500
    },
    {
      "epoch": 2.604,
      "grad_norm": 2.2843194007873535,
      "learning_rate": 6.6e-06,
      "loss": 0.4764,
      "step": 6510
    },
    {
      "epoch": 2.608,
      "grad_norm": 1.69950532913208,
      "learning_rate": 6.533333333333333e-06,
      "loss": 0.5204,
      "step": 6520
    },
    {
      "epoch": 2.612,
      "grad_norm": 2.753669500350952,
      "learning_rate": 6.466666666666667e-06,
      "loss": 0.5972,
      "step": 6530
    },
    {
      "epoch": 2.616,
      "grad_norm": 2.0240907669067383,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.4538,
      "step": 6540
    },
    {
      "epoch": 2.62,
      "grad_norm": 2.8404884338378906,
      "learning_rate": 6.333333333333334e-06,
      "loss": 0.5645,
      "step": 6550
    },
    {
      "epoch": 2.624,
      "grad_norm": 2.8485355377197266,
      "learning_rate": 6.266666666666666e-06,
      "loss": 0.4701,
      "step": 6560
    },
    {
      "epoch": 2.628,
      "grad_norm": 1.9572522640228271,
      "learning_rate": 6.2e-06,
      "loss": 0.5028,
      "step": 6570
    },
    {
      "epoch": 2.632,
      "grad_norm": 2.291546106338501,
      "learning_rate": 6.133333333333334e-06,
      "loss": 0.5781,
      "step": 6580
    },
    {
      "epoch": 2.636,
      "grad_norm": 2.5694046020507812,
      "learning_rate": 6.066666666666667e-06,
      "loss": 0.553,
      "step": 6590
    },
    {
      "epoch": 2.64,
      "grad_norm": 2.080482006072998,
      "learning_rate": 6e-06,
      "loss": 0.5423,
      "step": 6600
    },
    {
      "epoch": 2.644,
      "grad_norm": 3.2087111473083496,
      "learning_rate": 5.933333333333334e-06,
      "loss": 0.5786,
      "step": 6610
    },
    {
      "epoch": 2.648,
      "grad_norm": 2.171872854232788,
      "learning_rate": 5.866666666666667e-06,
      "loss": 0.5331,
      "step": 6620
    },
    {
      "epoch": 2.652,
      "grad_norm": 2.1573166847229004,
      "learning_rate": 5.8e-06,
      "loss": 0.4659,
      "step": 6630
    },
    {
      "epoch": 2.656,
      "grad_norm": 3.6289162635803223,
      "learning_rate": 5.733333333333333e-06,
      "loss": 0.4439,
      "step": 6640
    },
    {
      "epoch": 2.66,
      "grad_norm": 2.5462167263031006,
      "learning_rate": 5.666666666666667e-06,
      "loss": 0.5006,
      "step": 6650
    },
    {
      "epoch": 2.664,
      "grad_norm": 2.2730934619903564,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.4898,
      "step": 6660
    },
    {
      "epoch": 2.668,
      "grad_norm": 2.7687203884124756,
      "learning_rate": 5.5333333333333334e-06,
      "loss": 0.5681,
      "step": 6670
    },
    {
      "epoch": 2.672,
      "grad_norm": 2.4542124271392822,
      "learning_rate": 5.466666666666667e-06,
      "loss": 0.5515,
      "step": 6680
    },
    {
      "epoch": 2.676,
      "grad_norm": 2.4534425735473633,
      "learning_rate": 5.4e-06,
      "loss": 0.5471,
      "step": 6690
    },
    {
      "epoch": 2.68,
      "grad_norm": 2.1312737464904785,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.4909,
      "step": 6700
    },
    {
      "epoch": 2.684,
      "grad_norm": 1.729435682296753,
      "learning_rate": 5.266666666666667e-06,
      "loss": 0.627,
      "step": 6710
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 2.4366250038146973,
      "learning_rate": 5.2e-06,
      "loss": 0.5542,
      "step": 6720
    },
    {
      "epoch": 2.692,
      "grad_norm": 3.6909372806549072,
      "learning_rate": 5.133333333333334e-06,
      "loss": 0.4385,
      "step": 6730
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 2.7058162689208984,
      "learning_rate": 5.066666666666667e-06,
      "loss": 0.5729,
      "step": 6740
    },
    {
      "epoch": 2.7,
      "grad_norm": 2.3201186656951904,
      "learning_rate": 5e-06,
      "loss": 0.5343,
      "step": 6750
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 2.6339285373687744,
      "learning_rate": 4.933333333333333e-06,
      "loss": 0.4989,
      "step": 6760
    },
    {
      "epoch": 2.708,
      "grad_norm": 1.9222674369812012,
      "learning_rate": 4.866666666666667e-06,
      "loss": 0.4769,
      "step": 6770
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 2.311417579650879,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.5943,
      "step": 6780
    },
    {
      "epoch": 2.716,
      "grad_norm": 2.803532123565674,
      "learning_rate": 4.7333333333333335e-06,
      "loss": 0.5447,
      "step": 6790
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 1.9865522384643555,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.5064,
      "step": 6800
    },
    {
      "epoch": 2.724,
      "grad_norm": 2.321540355682373,
      "learning_rate": 4.6e-06,
      "loss": 0.5253,
      "step": 6810
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 4.752779483795166,
      "learning_rate": 4.533333333333334e-06,
      "loss": 0.511,
      "step": 6820
    },
    {
      "epoch": 2.732,
      "grad_norm": 2.1945271492004395,
      "learning_rate": 4.4666666666666665e-06,
      "loss": 0.519,
      "step": 6830
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 2.434866189956665,
      "learning_rate": 4.4e-06,
      "loss": 0.5434,
      "step": 6840
    },
    {
      "epoch": 2.74,
      "grad_norm": 2.8641350269317627,
      "learning_rate": 4.333333333333334e-06,
      "loss": 0.6247,
      "step": 6850
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 2.2310545444488525,
      "learning_rate": 4.266666666666667e-06,
      "loss": 0.5331,
      "step": 6860
    },
    {
      "epoch": 2.748,
      "grad_norm": 2.3947150707244873,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 0.5107,
      "step": 6870
    },
    {
      "epoch": 2.752,
      "grad_norm": 1.753283977508545,
      "learning_rate": 4.133333333333333e-06,
      "loss": 0.491,
      "step": 6880
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 2.7448010444641113,
      "learning_rate": 4.066666666666666e-06,
      "loss": 0.5534,
      "step": 6890
    },
    {
      "epoch": 2.76,
      "grad_norm": 2.395968437194824,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.4903,
      "step": 6900
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 2.660090923309326,
      "learning_rate": 3.9333333333333335e-06,
      "loss": 0.4539,
      "step": 6910
    },
    {
      "epoch": 2.768,
      "grad_norm": 2.2512238025665283,
      "learning_rate": 3.866666666666667e-06,
      "loss": 0.5003,
      "step": 6920
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 2.7196505069732666,
      "learning_rate": 3.8e-06,
      "loss": 0.6096,
      "step": 6930
    },
    {
      "epoch": 2.776,
      "grad_norm": 2.4882969856262207,
      "learning_rate": 3.7333333333333337e-06,
      "loss": 0.5213,
      "step": 6940
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 2.595634698867798,
      "learning_rate": 3.666666666666667e-06,
      "loss": 0.5422,
      "step": 6950
    },
    {
      "epoch": 2.784,
      "grad_norm": 2.3563506603240967,
      "learning_rate": 3.6e-06,
      "loss": 0.4993,
      "step": 6960
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 2.59794545173645,
      "learning_rate": 3.5333333333333335e-06,
      "loss": 0.5001,
      "step": 6970
    },
    {
      "epoch": 2.792,
      "grad_norm": 2.377596139907837,
      "learning_rate": 3.466666666666667e-06,
      "loss": 0.502,
      "step": 6980
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 2.751861810684204,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 0.4812,
      "step": 6990
    },
    {
      "epoch": 2.8,
      "grad_norm": 1.9962857961654663,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.5192,
      "step": 7000
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 2.0599379539489746,
      "learning_rate": 3.2666666666666666e-06,
      "loss": 0.4891,
      "step": 7010
    },
    {
      "epoch": 2.808,
      "grad_norm": 2.769022226333618,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.5583,
      "step": 7020
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 1.9740689992904663,
      "learning_rate": 3.133333333333333e-06,
      "loss": 0.5951,
      "step": 7030
    },
    {
      "epoch": 2.816,
      "grad_norm": 2.5317561626434326,
      "learning_rate": 3.066666666666667e-06,
      "loss": 0.5651,
      "step": 7040
    },
    {
      "epoch": 2.82,
      "grad_norm": 2.1381995677948,
      "learning_rate": 3e-06,
      "loss": 0.5189,
      "step": 7050
    },
    {
      "epoch": 2.824,
      "grad_norm": 2.7097878456115723,
      "learning_rate": 2.9333333333333333e-06,
      "loss": 0.6072,
      "step": 7060
    },
    {
      "epoch": 2.828,
      "grad_norm": 2.7839038372039795,
      "learning_rate": 2.8666666666666666e-06,
      "loss": 0.5285,
      "step": 7070
    },
    {
      "epoch": 2.832,
      "grad_norm": 2.339339256286621,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.5089,
      "step": 7080
    },
    {
      "epoch": 2.836,
      "grad_norm": 2.456737995147705,
      "learning_rate": 2.7333333333333336e-06,
      "loss": 0.4366,
      "step": 7090
    },
    {
      "epoch": 2.84,
      "grad_norm": 2.3044118881225586,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.5055,
      "step": 7100
    },
    {
      "epoch": 2.844,
      "grad_norm": 1.7509939670562744,
      "learning_rate": 2.6e-06,
      "loss": 0.5197,
      "step": 7110
    },
    {
      "epoch": 2.848,
      "grad_norm": 2.2891080379486084,
      "learning_rate": 2.5333333333333334e-06,
      "loss": 0.5691,
      "step": 7120
    },
    {
      "epoch": 2.852,
      "grad_norm": 3.26037335395813,
      "learning_rate": 2.4666666666666666e-06,
      "loss": 0.6639,
      "step": 7130
    },
    {
      "epoch": 2.856,
      "grad_norm": 2.3857786655426025,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.5063,
      "step": 7140
    },
    {
      "epoch": 2.86,
      "grad_norm": 2.051727533340454,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.5379,
      "step": 7150
    },
    {
      "epoch": 2.864,
      "grad_norm": 2.7983875274658203,
      "learning_rate": 2.266666666666667e-06,
      "loss": 0.5052,
      "step": 7160
    },
    {
      "epoch": 2.868,
      "grad_norm": 1.6618045568466187,
      "learning_rate": 2.2e-06,
      "loss": 0.4583,
      "step": 7170
    },
    {
      "epoch": 2.872,
      "grad_norm": 1.9784044027328491,
      "learning_rate": 2.1333333333333334e-06,
      "loss": 0.5021,
      "step": 7180
    },
    {
      "epoch": 2.876,
      "grad_norm": 2.000333070755005,
      "learning_rate": 2.0666666666666666e-06,
      "loss": 0.5179,
      "step": 7190
    },
    {
      "epoch": 2.88,
      "grad_norm": 2.0156846046447754,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.4545,
      "step": 7200
    },
    {
      "epoch": 2.884,
      "grad_norm": 2.6939287185668945,
      "learning_rate": 1.9333333333333336e-06,
      "loss": 0.495,
      "step": 7210
    },
    {
      "epoch": 2.888,
      "grad_norm": 1.9652858972549438,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 0.4677,
      "step": 7220
    },
    {
      "epoch": 2.892,
      "grad_norm": 2.3834118843078613,
      "learning_rate": 1.8e-06,
      "loss": 0.4897,
      "step": 7230
    },
    {
      "epoch": 2.896,
      "grad_norm": 2.772148609161377,
      "learning_rate": 1.7333333333333334e-06,
      "loss": 0.5337,
      "step": 7240
    },
    {
      "epoch": 2.9,
      "grad_norm": 2.449237585067749,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.4973,
      "step": 7250
    },
    {
      "epoch": 2.904,
      "grad_norm": 2.154978036880493,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.526,
      "step": 7260
    },
    {
      "epoch": 2.908,
      "grad_norm": 3.3866875171661377,
      "learning_rate": 1.5333333333333334e-06,
      "loss": 0.5438,
      "step": 7270
    },
    {
      "epoch": 2.912,
      "grad_norm": 1.8687779903411865,
      "learning_rate": 1.4666666666666667e-06,
      "loss": 0.6208,
      "step": 7280
    },
    {
      "epoch": 2.916,
      "grad_norm": 2.4281344413757324,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 0.5252,
      "step": 7290
    },
    {
      "epoch": 2.92,
      "grad_norm": 3.526326894760132,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.5901,
      "step": 7300
    },
    {
      "epoch": 2.924,
      "grad_norm": 2.6179251670837402,
      "learning_rate": 1.2666666666666667e-06,
      "loss": 0.5721,
      "step": 7310
    },
    {
      "epoch": 2.928,
      "grad_norm": 2.640552520751953,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.6209,
      "step": 7320
    },
    {
      "epoch": 2.932,
      "grad_norm": 3.142765522003174,
      "learning_rate": 1.1333333333333334e-06,
      "loss": 0.5707,
      "step": 7330
    },
    {
      "epoch": 2.936,
      "grad_norm": 2.120779275894165,
      "learning_rate": 1.0666666666666667e-06,
      "loss": 0.5377,
      "step": 7340
    },
    {
      "epoch": 2.94,
      "grad_norm": 2.2061269283294678,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.4687,
      "step": 7350
    },
    {
      "epoch": 2.944,
      "grad_norm": 2.7673003673553467,
      "learning_rate": 9.333333333333334e-07,
      "loss": 0.5001,
      "step": 7360
    },
    {
      "epoch": 2.948,
      "grad_norm": 2.1837143898010254,
      "learning_rate": 8.666666666666667e-07,
      "loss": 0.5753,
      "step": 7370
    },
    {
      "epoch": 2.952,
      "grad_norm": 2.783665418624878,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.4969,
      "step": 7380
    },
    {
      "epoch": 2.956,
      "grad_norm": 3.3297064304351807,
      "learning_rate": 7.333333333333333e-07,
      "loss": 0.6198,
      "step": 7390
    },
    {
      "epoch": 2.96,
      "grad_norm": 1.8338443040847778,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.5866,
      "step": 7400
    },
    {
      "epoch": 2.964,
      "grad_norm": 1.740478515625,
      "learning_rate": 6.000000000000001e-07,
      "loss": 0.6338,
      "step": 7410
    },
    {
      "epoch": 2.968,
      "grad_norm": 2.45808744430542,
      "learning_rate": 5.333333333333333e-07,
      "loss": 0.5394,
      "step": 7420
    },
    {
      "epoch": 2.972,
      "grad_norm": 2.5171470642089844,
      "learning_rate": 4.666666666666667e-07,
      "loss": 0.5914,
      "step": 7430
    },
    {
      "epoch": 2.976,
      "grad_norm": 2.2022852897644043,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.5691,
      "step": 7440
    },
    {
      "epoch": 2.98,
      "grad_norm": 2.6135194301605225,
      "learning_rate": 3.3333333333333335e-07,
      "loss": 0.5559,
      "step": 7450
    },
    {
      "epoch": 2.984,
      "grad_norm": 2.073153257369995,
      "learning_rate": 2.6666666666666667e-07,
      "loss": 0.4943,
      "step": 7460
    },
    {
      "epoch": 2.988,
      "grad_norm": 1.9395534992218018,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 0.5741,
      "step": 7470
    },
    {
      "epoch": 2.992,
      "grad_norm": 1.9372080564498901,
      "learning_rate": 1.3333333333333334e-07,
      "loss": 0.5647,
      "step": 7480
    },
    {
      "epoch": 2.996,
      "grad_norm": 2.830901622772217,
      "learning_rate": 6.666666666666667e-08,
      "loss": 0.518,
      "step": 7490
    },
    {
      "epoch": 3.0,
      "grad_norm": 2.353891134262085,
      "learning_rate": 0.0,
      "loss": 0.5605,
      "step": 7500
    }
  ],
  "logging_steps": 10,
  "max_steps": 7500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.561738338304e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
