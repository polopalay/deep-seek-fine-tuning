Trong quá trình fine-tuning với O-LoRa, có 1 vấn đề là với lượng dữ liệu nhỏ, nhiều dữ liệu bị lặp ý sẽ dẫn đến việc ép trực giao đột ngột có thể khiến mô hình không kịp thích nghi. Điều này dẫn đến hiện tượng loss tăng đột biến ngay sau bước orthogonalization.
Nguyên nhân chính là vì các vector trong ma trận A và B của LoRA đang dần học các hướng trùng lặp. Khi bị ép trực giao ngay lập tức các vector đó buộc phải thay đổi hướng một cách không tự nhiên — mô hình “mất trí nhớ” về những gì vừa học, từ đó làm gián đoạn quá trình tối ưu hóa đang diễn ra.
Thay vì dùng QR decomposition, ta áp dụng một hình thức trực giao hóa mềm bằng cách sử dụng chuẩn hóa vector (L2 normalization): F.normalize(A, p=2, dim=1)


0: LM Loss: 8.6405, CLoRA Loss: 0.2400
0: LM Loss: 8.0518, CLoRA Loss: 0.2400
  2%|███▏                                                                                                                                                                                        | 1/60 [00:22<22:06, 22.49s/it]1: LM Loss: 8.1945, CLoRA Loss: 0.2400
Orthogonalized 196 COLoRA modules
1: LM Loss: 8.6781, CLoRA Loss: 0.2400
  3%|██████▎                                                                                                                                                                                     | 2/60 [01:23<43:53, 45.40s/it]2: LM Loss: 8.5931, CLoRA Loss: 0.2400
2: LM Loss: 8.2263, CLoRA Loss: 0.2400
Orthogonalized 196 COLoRA modules
  5%|█████████▍                                                                                                                                                                                  | 3/60 [02:19<47:38, 50.15s/it]3: LM Loss: 8.3619, CLoRA Loss: 0.2109
3: LM Loss: 8.7895, CLoRA Loss: 0.2109
  7%|████████████▌                                                                                                                                                                               | 4/60 [03:14<48:26, 51.90s/it]4: LM Loss: 7.7643, CLoRA Loss: 0.2109
Orthogonalized 196 COLoRA modules
4: LM Loss: 7.5716, CLoRA Loss: 0.2109
  8%|███████████████▋                                                                                                                                                                            | 5/60 [04:11<49:13, 53.69s/it]5: LM Loss: 8.1664, CLoRA Loss: 0.2109
5: LM Loss: 7.7687, CLoRA Loss: 0.2109
Orthogonalized 196 COLoRA modules
 10%|██████████████████▊                                                                                                                                                                         | 6/60 [05:04<48:11, 53.55s/it]6: LM Loss: 7.6300, CLoRA Loss: 0.2109
6: LM Loss: 7.9598, CLoRA Loss: 0.2109
 12%|█████████████████████▉                                                                                                                                                                      | 7/60 [06:05<49:36, 56.15s/it]7: LM Loss: 7.4850, CLoRA Loss: 0.2109
Orthogonalized 196 COLoRA modules
7: LM Loss: 6.9913, CLoRA Loss: 0.2109
 13%|█████████████████████████                                                                                                                                                                   | 8/60 [07:00<48:12, 55.62s/it]8: LM Loss: 8.3068, CLoRA Loss: 0.2109
8: LM Loss: 7.8385, CLoRA Loss: 0.2109
Orthogonalized 196 COLoRA modules
 15%|████████████████████████████▏                                                                                                                                                               | 9/60 [08:02<48:57, 57.59s/it]9: LM Loss: 7.0145, CLoRA Loss: 0.2109
9: LM Loss: 7.5360, CLoRA Loss: 0.2109
 17%|███████████████████████████████▏                                                                                                                                                           | 10/60 [09:03<48:47, 58.54s/it]10: LM Loss: 8.5000, CLoRA Loss: 0.2109
Orthogonalized 196 COLoRA modules
10: LM Loss: 7.6324, CLoRA Loss: 0.2109
