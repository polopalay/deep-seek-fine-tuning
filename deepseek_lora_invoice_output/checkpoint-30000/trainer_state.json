{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 30000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002,
      "grad_norm": 3.8980329036712646,
      "learning_rate": 4.996666666666667e-05,
      "loss": 6.5663,
      "step": 20
    },
    {
      "epoch": 0.004,
      "grad_norm": 4.280694961547852,
      "learning_rate": 4.993333333333334e-05,
      "loss": 5.9775,
      "step": 40
    },
    {
      "epoch": 0.006,
      "grad_norm": 4.286524295806885,
      "learning_rate": 4.99e-05,
      "loss": 5.1559,
      "step": 60
    },
    {
      "epoch": 0.008,
      "grad_norm": 4.180159091949463,
      "learning_rate": 4.986666666666667e-05,
      "loss": 4.7241,
      "step": 80
    },
    {
      "epoch": 0.01,
      "grad_norm": 4.647509574890137,
      "learning_rate": 4.9833333333333336e-05,
      "loss": 4.0591,
      "step": 100
    },
    {
      "epoch": 0.012,
      "grad_norm": 4.573578834533691,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 3.6577,
      "step": 120
    },
    {
      "epoch": 0.014,
      "grad_norm": 4.829085350036621,
      "learning_rate": 4.9766666666666666e-05,
      "loss": 3.4325,
      "step": 140
    },
    {
      "epoch": 0.016,
      "grad_norm": 4.136877536773682,
      "learning_rate": 4.973333333333334e-05,
      "loss": 2.9596,
      "step": 160
    },
    {
      "epoch": 0.018,
      "grad_norm": 8.41654109954834,
      "learning_rate": 4.97e-05,
      "loss": 2.8176,
      "step": 180
    },
    {
      "epoch": 0.02,
      "grad_norm": 4.869631290435791,
      "learning_rate": 4.966666666666667e-05,
      "loss": 2.7923,
      "step": 200
    },
    {
      "epoch": 0.022,
      "grad_norm": 4.543134689331055,
      "learning_rate": 4.963333333333334e-05,
      "loss": 2.6126,
      "step": 220
    },
    {
      "epoch": 0.024,
      "grad_norm": 3.764467477798462,
      "learning_rate": 4.96e-05,
      "loss": 2.6805,
      "step": 240
    },
    {
      "epoch": 0.026,
      "grad_norm": 4.746095657348633,
      "learning_rate": 4.956666666666667e-05,
      "loss": 2.3573,
      "step": 260
    },
    {
      "epoch": 0.028,
      "grad_norm": 4.149470329284668,
      "learning_rate": 4.9533333333333336e-05,
      "loss": 2.4491,
      "step": 280
    },
    {
      "epoch": 0.03,
      "grad_norm": 4.736706733703613,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 2.3456,
      "step": 300
    },
    {
      "epoch": 0.032,
      "grad_norm": 4.669076919555664,
      "learning_rate": 4.9466666666666665e-05,
      "loss": 2.1929,
      "step": 320
    },
    {
      "epoch": 0.034,
      "grad_norm": 5.896118640899658,
      "learning_rate": 4.943333333333334e-05,
      "loss": 2.3253,
      "step": 340
    },
    {
      "epoch": 0.036,
      "grad_norm": 4.500416278839111,
      "learning_rate": 4.94e-05,
      "loss": 2.2014,
      "step": 360
    },
    {
      "epoch": 0.038,
      "grad_norm": 6.368517875671387,
      "learning_rate": 4.936666666666667e-05,
      "loss": 2.1646,
      "step": 380
    },
    {
      "epoch": 0.04,
      "grad_norm": 5.91015100479126,
      "learning_rate": 4.933333333333334e-05,
      "loss": 2.1994,
      "step": 400
    },
    {
      "epoch": 0.042,
      "grad_norm": 5.074048042297363,
      "learning_rate": 4.93e-05,
      "loss": 2.1103,
      "step": 420
    },
    {
      "epoch": 0.044,
      "grad_norm": 6.128055572509766,
      "learning_rate": 4.926666666666667e-05,
      "loss": 2.2066,
      "step": 440
    },
    {
      "epoch": 0.046,
      "grad_norm": 4.619884490966797,
      "learning_rate": 4.9233333333333335e-05,
      "loss": 2.1574,
      "step": 460
    },
    {
      "epoch": 0.048,
      "grad_norm": 6.4635748863220215,
      "learning_rate": 4.92e-05,
      "loss": 2.0797,
      "step": 480
    },
    {
      "epoch": 0.05,
      "grad_norm": 6.931548118591309,
      "learning_rate": 4.9166666666666665e-05,
      "loss": 1.9786,
      "step": 500
    },
    {
      "epoch": 0.052,
      "grad_norm": 6.3924736976623535,
      "learning_rate": 4.913333333333334e-05,
      "loss": 2.0966,
      "step": 520
    },
    {
      "epoch": 0.054,
      "grad_norm": 6.577085494995117,
      "learning_rate": 4.91e-05,
      "loss": 2.043,
      "step": 540
    },
    {
      "epoch": 0.056,
      "grad_norm": 5.1308135986328125,
      "learning_rate": 4.906666666666667e-05,
      "loss": 2.0519,
      "step": 560
    },
    {
      "epoch": 0.058,
      "grad_norm": 5.054885387420654,
      "learning_rate": 4.903333333333334e-05,
      "loss": 2.0479,
      "step": 580
    },
    {
      "epoch": 0.06,
      "grad_norm": 5.849750995635986,
      "learning_rate": 4.9e-05,
      "loss": 1.8848,
      "step": 600
    },
    {
      "epoch": 0.062,
      "grad_norm": 7.004734516143799,
      "learning_rate": 4.8966666666666667e-05,
      "loss": 1.8447,
      "step": 620
    },
    {
      "epoch": 0.064,
      "grad_norm": 5.927133083343506,
      "learning_rate": 4.8933333333333335e-05,
      "loss": 1.904,
      "step": 640
    },
    {
      "epoch": 0.066,
      "grad_norm": 6.413360595703125,
      "learning_rate": 4.89e-05,
      "loss": 1.9473,
      "step": 660
    },
    {
      "epoch": 0.068,
      "grad_norm": 4.977111339569092,
      "learning_rate": 4.886666666666667e-05,
      "loss": 1.8993,
      "step": 680
    },
    {
      "epoch": 0.07,
      "grad_norm": 8.968652725219727,
      "learning_rate": 4.883333333333334e-05,
      "loss": 1.8698,
      "step": 700
    },
    {
      "epoch": 0.072,
      "grad_norm": 6.70922327041626,
      "learning_rate": 4.88e-05,
      "loss": 1.8398,
      "step": 720
    },
    {
      "epoch": 0.074,
      "grad_norm": 6.461608409881592,
      "learning_rate": 4.876666666666667e-05,
      "loss": 1.8706,
      "step": 740
    },
    {
      "epoch": 0.076,
      "grad_norm": 5.00422477722168,
      "learning_rate": 4.8733333333333337e-05,
      "loss": 1.7815,
      "step": 760
    },
    {
      "epoch": 0.078,
      "grad_norm": 6.966536045074463,
      "learning_rate": 4.87e-05,
      "loss": 1.8618,
      "step": 780
    },
    {
      "epoch": 0.08,
      "grad_norm": 8.513169288635254,
      "learning_rate": 4.866666666666667e-05,
      "loss": 1.7755,
      "step": 800
    },
    {
      "epoch": 0.082,
      "grad_norm": 42.253963470458984,
      "learning_rate": 4.8633333333333334e-05,
      "loss": 1.697,
      "step": 820
    },
    {
      "epoch": 0.084,
      "grad_norm": 6.297754764556885,
      "learning_rate": 4.86e-05,
      "loss": 1.5614,
      "step": 840
    },
    {
      "epoch": 0.086,
      "grad_norm": 40.57698059082031,
      "learning_rate": 4.856666666666667e-05,
      "loss": 1.7045,
      "step": 860
    },
    {
      "epoch": 0.088,
      "grad_norm": 31.80829429626465,
      "learning_rate": 4.853333333333334e-05,
      "loss": 1.6306,
      "step": 880
    },
    {
      "epoch": 0.09,
      "grad_norm": 6.130700588226318,
      "learning_rate": 4.85e-05,
      "loss": 1.4918,
      "step": 900
    },
    {
      "epoch": 0.092,
      "grad_norm": 7.76030969619751,
      "learning_rate": 4.8466666666666675e-05,
      "loss": 1.6023,
      "step": 920
    },
    {
      "epoch": 0.094,
      "grad_norm": 5.77816915512085,
      "learning_rate": 4.8433333333333336e-05,
      "loss": 1.5973,
      "step": 940
    },
    {
      "epoch": 0.096,
      "grad_norm": 7.409061431884766,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 1.4075,
      "step": 960
    },
    {
      "epoch": 0.098,
      "grad_norm": 7.56076192855835,
      "learning_rate": 4.836666666666667e-05,
      "loss": 1.5171,
      "step": 980
    },
    {
      "epoch": 0.1,
      "grad_norm": 10.278071403503418,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 1.5836,
      "step": 1000
    },
    {
      "epoch": 0.102,
      "grad_norm": 24.194183349609375,
      "learning_rate": 4.83e-05,
      "loss": 1.5094,
      "step": 1020
    },
    {
      "epoch": 0.104,
      "grad_norm": 16.291736602783203,
      "learning_rate": 4.826666666666667e-05,
      "loss": 1.6055,
      "step": 1040
    },
    {
      "epoch": 0.106,
      "grad_norm": 12.774667739868164,
      "learning_rate": 4.823333333333334e-05,
      "loss": 1.6098,
      "step": 1060
    },
    {
      "epoch": 0.108,
      "grad_norm": 9.98667049407959,
      "learning_rate": 4.82e-05,
      "loss": 1.5956,
      "step": 1080
    },
    {
      "epoch": 0.11,
      "grad_norm": 6.444921493530273,
      "learning_rate": 4.8166666666666674e-05,
      "loss": 1.4694,
      "step": 1100
    },
    {
      "epoch": 0.112,
      "grad_norm": 7.86366605758667,
      "learning_rate": 4.8133333333333336e-05,
      "loss": 1.4781,
      "step": 1120
    },
    {
      "epoch": 0.114,
      "grad_norm": 6.653502941131592,
      "learning_rate": 4.8100000000000004e-05,
      "loss": 1.4038,
      "step": 1140
    },
    {
      "epoch": 0.116,
      "grad_norm": 6.864247798919678,
      "learning_rate": 4.806666666666667e-05,
      "loss": 1.4288,
      "step": 1160
    },
    {
      "epoch": 0.118,
      "grad_norm": 6.675746917724609,
      "learning_rate": 4.803333333333333e-05,
      "loss": 1.4699,
      "step": 1180
    },
    {
      "epoch": 0.12,
      "grad_norm": 6.4832611083984375,
      "learning_rate": 4.8e-05,
      "loss": 1.4487,
      "step": 1200
    },
    {
      "epoch": 0.122,
      "grad_norm": 10.015910148620605,
      "learning_rate": 4.796666666666667e-05,
      "loss": 1.4182,
      "step": 1220
    },
    {
      "epoch": 0.124,
      "grad_norm": 7.076039791107178,
      "learning_rate": 4.793333333333334e-05,
      "loss": 1.3925,
      "step": 1240
    },
    {
      "epoch": 0.126,
      "grad_norm": 5.401561260223389,
      "learning_rate": 4.79e-05,
      "loss": 1.3622,
      "step": 1260
    },
    {
      "epoch": 0.128,
      "grad_norm": 6.3080034255981445,
      "learning_rate": 4.7866666666666674e-05,
      "loss": 1.3719,
      "step": 1280
    },
    {
      "epoch": 0.13,
      "grad_norm": 6.549924373626709,
      "learning_rate": 4.7833333333333335e-05,
      "loss": 1.4164,
      "step": 1300
    },
    {
      "epoch": 0.132,
      "grad_norm": 5.7770233154296875,
      "learning_rate": 4.78e-05,
      "loss": 1.371,
      "step": 1320
    },
    {
      "epoch": 0.134,
      "grad_norm": 6.942556381225586,
      "learning_rate": 4.776666666666667e-05,
      "loss": 1.4153,
      "step": 1340
    },
    {
      "epoch": 0.136,
      "grad_norm": 6.263159275054932,
      "learning_rate": 4.773333333333333e-05,
      "loss": 1.36,
      "step": 1360
    },
    {
      "epoch": 0.138,
      "grad_norm": 6.855639934539795,
      "learning_rate": 4.77e-05,
      "loss": 1.3323,
      "step": 1380
    },
    {
      "epoch": 0.14,
      "grad_norm": 5.475780963897705,
      "learning_rate": 4.766666666666667e-05,
      "loss": 1.3229,
      "step": 1400
    },
    {
      "epoch": 0.142,
      "grad_norm": 5.284062385559082,
      "learning_rate": 4.763333333333334e-05,
      "loss": 1.32,
      "step": 1420
    },
    {
      "epoch": 0.144,
      "grad_norm": 6.5642595291137695,
      "learning_rate": 4.76e-05,
      "loss": 1.3325,
      "step": 1440
    },
    {
      "epoch": 0.146,
      "grad_norm": 5.741341590881348,
      "learning_rate": 4.756666666666667e-05,
      "loss": 1.2708,
      "step": 1460
    },
    {
      "epoch": 0.148,
      "grad_norm": 6.708582878112793,
      "learning_rate": 4.7533333333333334e-05,
      "loss": 1.4186,
      "step": 1480
    },
    {
      "epoch": 0.15,
      "grad_norm": 7.014008045196533,
      "learning_rate": 4.75e-05,
      "loss": 1.3237,
      "step": 1500
    },
    {
      "epoch": 0.152,
      "grad_norm": 6.745012283325195,
      "learning_rate": 4.746666666666667e-05,
      "loss": 1.3526,
      "step": 1520
    },
    {
      "epoch": 0.154,
      "grad_norm": 6.875639915466309,
      "learning_rate": 4.743333333333333e-05,
      "loss": 1.3088,
      "step": 1540
    },
    {
      "epoch": 0.156,
      "grad_norm": 6.744178295135498,
      "learning_rate": 4.74e-05,
      "loss": 1.2984,
      "step": 1560
    },
    {
      "epoch": 0.158,
      "grad_norm": 7.733397960662842,
      "learning_rate": 4.736666666666667e-05,
      "loss": 1.2623,
      "step": 1580
    },
    {
      "epoch": 0.16,
      "grad_norm": 6.550693511962891,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 1.2214,
      "step": 1600
    },
    {
      "epoch": 0.162,
      "grad_norm": 9.469169616699219,
      "learning_rate": 4.73e-05,
      "loss": 1.2606,
      "step": 1620
    },
    {
      "epoch": 0.164,
      "grad_norm": 4.914788246154785,
      "learning_rate": 4.726666666666667e-05,
      "loss": 1.312,
      "step": 1640
    },
    {
      "epoch": 0.166,
      "grad_norm": 7.483841419219971,
      "learning_rate": 4.7233333333333334e-05,
      "loss": 1.2554,
      "step": 1660
    },
    {
      "epoch": 0.168,
      "grad_norm": 5.7187299728393555,
      "learning_rate": 4.72e-05,
      "loss": 1.1879,
      "step": 1680
    },
    {
      "epoch": 0.17,
      "grad_norm": 8.477075576782227,
      "learning_rate": 4.716666666666667e-05,
      "loss": 1.2464,
      "step": 1700
    },
    {
      "epoch": 0.172,
      "grad_norm": 5.497802257537842,
      "learning_rate": 4.713333333333333e-05,
      "loss": 1.163,
      "step": 1720
    },
    {
      "epoch": 0.174,
      "grad_norm": 4.377004146575928,
      "learning_rate": 4.71e-05,
      "loss": 1.2636,
      "step": 1740
    },
    {
      "epoch": 0.176,
      "grad_norm": 6.09780216217041,
      "learning_rate": 4.706666666666667e-05,
      "loss": 1.2248,
      "step": 1760
    },
    {
      "epoch": 0.178,
      "grad_norm": 8.296425819396973,
      "learning_rate": 4.7033333333333336e-05,
      "loss": 1.2146,
      "step": 1780
    },
    {
      "epoch": 0.18,
      "grad_norm": 5.018105506896973,
      "learning_rate": 4.7e-05,
      "loss": 1.1719,
      "step": 1800
    },
    {
      "epoch": 0.182,
      "grad_norm": 7.996996879577637,
      "learning_rate": 4.696666666666667e-05,
      "loss": 1.2262,
      "step": 1820
    },
    {
      "epoch": 0.184,
      "grad_norm": 5.117391586303711,
      "learning_rate": 4.6933333333333333e-05,
      "loss": 1.1653,
      "step": 1840
    },
    {
      "epoch": 0.186,
      "grad_norm": 5.234899044036865,
      "learning_rate": 4.69e-05,
      "loss": 1.1356,
      "step": 1860
    },
    {
      "epoch": 0.188,
      "grad_norm": 5.773532867431641,
      "learning_rate": 4.686666666666667e-05,
      "loss": 1.216,
      "step": 1880
    },
    {
      "epoch": 0.19,
      "grad_norm": 7.653670310974121,
      "learning_rate": 4.683333333333334e-05,
      "loss": 1.2388,
      "step": 1900
    },
    {
      "epoch": 0.192,
      "grad_norm": 4.568027973175049,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 1.1929,
      "step": 1920
    },
    {
      "epoch": 0.194,
      "grad_norm": 7.809937477111816,
      "learning_rate": 4.676666666666667e-05,
      "loss": 1.1638,
      "step": 1940
    },
    {
      "epoch": 0.196,
      "grad_norm": 6.341979503631592,
      "learning_rate": 4.6733333333333335e-05,
      "loss": 1.2078,
      "step": 1960
    },
    {
      "epoch": 0.198,
      "grad_norm": 5.438144683837891,
      "learning_rate": 4.6700000000000003e-05,
      "loss": 1.1097,
      "step": 1980
    },
    {
      "epoch": 0.2,
      "grad_norm": 5.060567378997803,
      "learning_rate": 4.666666666666667e-05,
      "loss": 1.2701,
      "step": 2000
    },
    {
      "epoch": 0.202,
      "grad_norm": 6.587687969207764,
      "learning_rate": 4.663333333333333e-05,
      "loss": 1.1958,
      "step": 2020
    },
    {
      "epoch": 0.204,
      "grad_norm": 6.633285045623779,
      "learning_rate": 4.660000000000001e-05,
      "loss": 1.1781,
      "step": 2040
    },
    {
      "epoch": 0.206,
      "grad_norm": 4.428033828735352,
      "learning_rate": 4.656666666666667e-05,
      "loss": 1.2422,
      "step": 2060
    },
    {
      "epoch": 0.208,
      "grad_norm": 4.2872090339660645,
      "learning_rate": 4.653333333333334e-05,
      "loss": 1.1876,
      "step": 2080
    },
    {
      "epoch": 0.21,
      "grad_norm": 5.754640579223633,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 1.1088,
      "step": 2100
    },
    {
      "epoch": 0.212,
      "grad_norm": 9.553414344787598,
      "learning_rate": 4.646666666666667e-05,
      "loss": 1.1747,
      "step": 2120
    },
    {
      "epoch": 0.214,
      "grad_norm": 5.859338760375977,
      "learning_rate": 4.6433333333333335e-05,
      "loss": 1.191,
      "step": 2140
    },
    {
      "epoch": 0.216,
      "grad_norm": 9.505379676818848,
      "learning_rate": 4.64e-05,
      "loss": 1.14,
      "step": 2160
    },
    {
      "epoch": 0.218,
      "grad_norm": 3.817110776901245,
      "learning_rate": 4.636666666666667e-05,
      "loss": 1.1963,
      "step": 2180
    },
    {
      "epoch": 0.22,
      "grad_norm": 5.447274684906006,
      "learning_rate": 4.633333333333333e-05,
      "loss": 1.1345,
      "step": 2200
    },
    {
      "epoch": 0.222,
      "grad_norm": 8.106063842773438,
      "learning_rate": 4.630000000000001e-05,
      "loss": 1.1198,
      "step": 2220
    },
    {
      "epoch": 0.224,
      "grad_norm": 5.09104585647583,
      "learning_rate": 4.626666666666667e-05,
      "loss": 1.1875,
      "step": 2240
    },
    {
      "epoch": 0.226,
      "grad_norm": 3.978426218032837,
      "learning_rate": 4.623333333333334e-05,
      "loss": 1.1476,
      "step": 2260
    },
    {
      "epoch": 0.228,
      "grad_norm": 4.431326866149902,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 1.1458,
      "step": 2280
    },
    {
      "epoch": 0.23,
      "grad_norm": 6.885031223297119,
      "learning_rate": 4.6166666666666666e-05,
      "loss": 1.163,
      "step": 2300
    },
    {
      "epoch": 0.232,
      "grad_norm": 6.950577259063721,
      "learning_rate": 4.6133333333333334e-05,
      "loss": 1.1481,
      "step": 2320
    },
    {
      "epoch": 0.234,
      "grad_norm": 5.447340488433838,
      "learning_rate": 4.61e-05,
      "loss": 1.2224,
      "step": 2340
    },
    {
      "epoch": 0.236,
      "grad_norm": 8.869805335998535,
      "learning_rate": 4.606666666666667e-05,
      "loss": 1.1838,
      "step": 2360
    },
    {
      "epoch": 0.238,
      "grad_norm": 7.473484516143799,
      "learning_rate": 4.603333333333333e-05,
      "loss": 1.1395,
      "step": 2380
    },
    {
      "epoch": 0.24,
      "grad_norm": 5.169826507568359,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.1293,
      "step": 2400
    },
    {
      "epoch": 0.242,
      "grad_norm": 3.997716188430786,
      "learning_rate": 4.596666666666667e-05,
      "loss": 1.1094,
      "step": 2420
    },
    {
      "epoch": 0.244,
      "grad_norm": 4.380395889282227,
      "learning_rate": 4.5933333333333336e-05,
      "loss": 1.1124,
      "step": 2440
    },
    {
      "epoch": 0.246,
      "grad_norm": 4.255224227905273,
      "learning_rate": 4.5900000000000004e-05,
      "loss": 1.0781,
      "step": 2460
    },
    {
      "epoch": 0.248,
      "grad_norm": 5.887856960296631,
      "learning_rate": 4.5866666666666666e-05,
      "loss": 1.1004,
      "step": 2480
    },
    {
      "epoch": 0.25,
      "grad_norm": 4.197272777557373,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 1.1429,
      "step": 2500
    },
    {
      "epoch": 0.252,
      "grad_norm": 4.78155517578125,
      "learning_rate": 4.58e-05,
      "loss": 1.1483,
      "step": 2520
    },
    {
      "epoch": 0.254,
      "grad_norm": 4.81786584854126,
      "learning_rate": 4.576666666666667e-05,
      "loss": 1.1553,
      "step": 2540
    },
    {
      "epoch": 0.256,
      "grad_norm": 4.744870185852051,
      "learning_rate": 4.573333333333333e-05,
      "loss": 1.1239,
      "step": 2560
    },
    {
      "epoch": 0.258,
      "grad_norm": 6.45673942565918,
      "learning_rate": 4.5700000000000006e-05,
      "loss": 1.1559,
      "step": 2580
    },
    {
      "epoch": 0.26,
      "grad_norm": 4.009333610534668,
      "learning_rate": 4.566666666666667e-05,
      "loss": 1.1446,
      "step": 2600
    },
    {
      "epoch": 0.262,
      "grad_norm": 4.5193586349487305,
      "learning_rate": 4.5633333333333336e-05,
      "loss": 1.0726,
      "step": 2620
    },
    {
      "epoch": 0.264,
      "grad_norm": 5.331731796264648,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 1.0894,
      "step": 2640
    },
    {
      "epoch": 0.266,
      "grad_norm": 4.222058296203613,
      "learning_rate": 4.556666666666667e-05,
      "loss": 1.0864,
      "step": 2660
    },
    {
      "epoch": 0.268,
      "grad_norm": 3.89007568359375,
      "learning_rate": 4.553333333333333e-05,
      "loss": 1.1085,
      "step": 2680
    },
    {
      "epoch": 0.27,
      "grad_norm": 4.5992326736450195,
      "learning_rate": 4.55e-05,
      "loss": 1.1452,
      "step": 2700
    },
    {
      "epoch": 0.272,
      "grad_norm": 4.2989068031311035,
      "learning_rate": 4.546666666666667e-05,
      "loss": 1.1061,
      "step": 2720
    },
    {
      "epoch": 0.274,
      "grad_norm": 4.244334697723389,
      "learning_rate": 4.543333333333333e-05,
      "loss": 1.1643,
      "step": 2740
    },
    {
      "epoch": 0.276,
      "grad_norm": 4.646675109863281,
      "learning_rate": 4.5400000000000006e-05,
      "loss": 1.1166,
      "step": 2760
    },
    {
      "epoch": 0.278,
      "grad_norm": 3.9836180210113525,
      "learning_rate": 4.536666666666667e-05,
      "loss": 1.1601,
      "step": 2780
    },
    {
      "epoch": 0.28,
      "grad_norm": 3.946401357650757,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 1.1362,
      "step": 2800
    },
    {
      "epoch": 0.282,
      "grad_norm": 6.841231822967529,
      "learning_rate": 4.53e-05,
      "loss": 1.1,
      "step": 2820
    },
    {
      "epoch": 0.284,
      "grad_norm": 4.415567874908447,
      "learning_rate": 4.526666666666667e-05,
      "loss": 1.0922,
      "step": 2840
    },
    {
      "epoch": 0.286,
      "grad_norm": 4.212378025054932,
      "learning_rate": 4.523333333333333e-05,
      "loss": 1.1133,
      "step": 2860
    },
    {
      "epoch": 0.288,
      "grad_norm": 5.59268856048584,
      "learning_rate": 4.52e-05,
      "loss": 1.1033,
      "step": 2880
    },
    {
      "epoch": 0.29,
      "grad_norm": 5.916106700897217,
      "learning_rate": 4.516666666666667e-05,
      "loss": 1.188,
      "step": 2900
    },
    {
      "epoch": 0.292,
      "grad_norm": 4.7094268798828125,
      "learning_rate": 4.513333333333333e-05,
      "loss": 1.1596,
      "step": 2920
    },
    {
      "epoch": 0.294,
      "grad_norm": 4.647007465362549,
      "learning_rate": 4.5100000000000005e-05,
      "loss": 1.0472,
      "step": 2940
    },
    {
      "epoch": 0.296,
      "grad_norm": 3.95182728767395,
      "learning_rate": 4.5066666666666667e-05,
      "loss": 1.0259,
      "step": 2960
    },
    {
      "epoch": 0.298,
      "grad_norm": 5.679205417633057,
      "learning_rate": 4.5033333333333335e-05,
      "loss": 1.17,
      "step": 2980
    },
    {
      "epoch": 0.3,
      "grad_norm": 5.606677532196045,
      "learning_rate": 4.5e-05,
      "loss": 1.1002,
      "step": 3000
    },
    {
      "epoch": 0.302,
      "grad_norm": 4.709538459777832,
      "learning_rate": 4.496666666666667e-05,
      "loss": 1.106,
      "step": 3020
    },
    {
      "epoch": 0.304,
      "grad_norm": 3.554351568222046,
      "learning_rate": 4.493333333333333e-05,
      "loss": 1.0795,
      "step": 3040
    },
    {
      "epoch": 0.306,
      "grad_norm": 3.951467990875244,
      "learning_rate": 4.49e-05,
      "loss": 1.1547,
      "step": 3060
    },
    {
      "epoch": 0.308,
      "grad_norm": 3.961266040802002,
      "learning_rate": 4.486666666666667e-05,
      "loss": 1.0946,
      "step": 3080
    },
    {
      "epoch": 0.31,
      "grad_norm": 4.519839286804199,
      "learning_rate": 4.483333333333333e-05,
      "loss": 1.0998,
      "step": 3100
    },
    {
      "epoch": 0.312,
      "grad_norm": 4.946537494659424,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 1.1396,
      "step": 3120
    },
    {
      "epoch": 0.314,
      "grad_norm": 3.5798721313476562,
      "learning_rate": 4.4766666666666666e-05,
      "loss": 1.1115,
      "step": 3140
    },
    {
      "epoch": 0.316,
      "grad_norm": 7.299167633056641,
      "learning_rate": 4.473333333333334e-05,
      "loss": 1.0471,
      "step": 3160
    },
    {
      "epoch": 0.318,
      "grad_norm": 3.852397918701172,
      "learning_rate": 4.47e-05,
      "loss": 1.0713,
      "step": 3180
    },
    {
      "epoch": 0.32,
      "grad_norm": 3.7632601261138916,
      "learning_rate": 4.466666666666667e-05,
      "loss": 1.1129,
      "step": 3200
    },
    {
      "epoch": 0.322,
      "grad_norm": 3.7461581230163574,
      "learning_rate": 4.463333333333334e-05,
      "loss": 1.1177,
      "step": 3220
    },
    {
      "epoch": 0.324,
      "grad_norm": 4.676697731018066,
      "learning_rate": 4.46e-05,
      "loss": 1.0365,
      "step": 3240
    },
    {
      "epoch": 0.326,
      "grad_norm": 3.5824363231658936,
      "learning_rate": 4.456666666666667e-05,
      "loss": 1.0619,
      "step": 3260
    },
    {
      "epoch": 0.328,
      "grad_norm": 4.554389476776123,
      "learning_rate": 4.4533333333333336e-05,
      "loss": 1.1361,
      "step": 3280
    },
    {
      "epoch": 0.33,
      "grad_norm": 3.729397773742676,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 1.103,
      "step": 3300
    },
    {
      "epoch": 0.332,
      "grad_norm": 5.325995445251465,
      "learning_rate": 4.4466666666666666e-05,
      "loss": 1.0491,
      "step": 3320
    },
    {
      "epoch": 0.334,
      "grad_norm": 3.8361001014709473,
      "learning_rate": 4.443333333333334e-05,
      "loss": 1.0461,
      "step": 3340
    },
    {
      "epoch": 0.336,
      "grad_norm": 6.931180953979492,
      "learning_rate": 4.44e-05,
      "loss": 1.111,
      "step": 3360
    },
    {
      "epoch": 0.338,
      "grad_norm": 4.067200183868408,
      "learning_rate": 4.436666666666667e-05,
      "loss": 1.144,
      "step": 3380
    },
    {
      "epoch": 0.34,
      "grad_norm": 3.528512477874756,
      "learning_rate": 4.433333333333334e-05,
      "loss": 1.1169,
      "step": 3400
    },
    {
      "epoch": 0.342,
      "grad_norm": 3.633042097091675,
      "learning_rate": 4.43e-05,
      "loss": 1.1464,
      "step": 3420
    },
    {
      "epoch": 0.344,
      "grad_norm": 5.0735979080200195,
      "learning_rate": 4.426666666666667e-05,
      "loss": 1.1215,
      "step": 3440
    },
    {
      "epoch": 0.346,
      "grad_norm": 3.911564588546753,
      "learning_rate": 4.4233333333333336e-05,
      "loss": 1.1773,
      "step": 3460
    },
    {
      "epoch": 0.348,
      "grad_norm": 4.125110149383545,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 1.1467,
      "step": 3480
    },
    {
      "epoch": 0.35,
      "grad_norm": 4.671987533569336,
      "learning_rate": 4.4166666666666665e-05,
      "loss": 1.0587,
      "step": 3500
    },
    {
      "epoch": 0.352,
      "grad_norm": 3.994235038757324,
      "learning_rate": 4.413333333333334e-05,
      "loss": 1.098,
      "step": 3520
    },
    {
      "epoch": 0.354,
      "grad_norm": 4.726217269897461,
      "learning_rate": 4.41e-05,
      "loss": 1.1161,
      "step": 3540
    },
    {
      "epoch": 0.356,
      "grad_norm": 49.770301818847656,
      "learning_rate": 4.406666666666667e-05,
      "loss": 1.1278,
      "step": 3560
    },
    {
      "epoch": 0.358,
      "grad_norm": 96.79383087158203,
      "learning_rate": 4.403333333333334e-05,
      "loss": 1.106,
      "step": 3580
    },
    {
      "epoch": 0.36,
      "grad_norm": 403.83392333984375,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.1835,
      "step": 3600
    },
    {
      "epoch": 0.362,
      "grad_norm": 472.6717224121094,
      "learning_rate": 4.396666666666667e-05,
      "loss": 1.4499,
      "step": 3620
    },
    {
      "epoch": 0.364,
      "grad_norm": 35986.3359375,
      "learning_rate": 4.3933333333333335e-05,
      "loss": 1.6249,
      "step": 3640
    },
    {
      "epoch": 0.366,
      "grad_norm": 14151.255859375,
      "learning_rate": 4.39e-05,
      "loss": 2.262,
      "step": 3660
    },
    {
      "epoch": 0.368,
      "grad_norm": 98781.1015625,
      "learning_rate": 4.3866666666666665e-05,
      "loss": 3.4281,
      "step": 3680
    },
    {
      "epoch": 0.37,
      "grad_norm": 141172.15625,
      "learning_rate": 4.383333333333334e-05,
      "loss": 4.5657,
      "step": 3700
    },
    {
      "epoch": 0.372,
      "grad_norm": 120958.5,
      "learning_rate": 4.38e-05,
      "loss": 4.2619,
      "step": 3720
    },
    {
      "epoch": 0.374,
      "grad_norm": 139831.015625,
      "learning_rate": 4.376666666666667e-05,
      "loss": 5.2159,
      "step": 3740
    },
    {
      "epoch": 0.376,
      "grad_norm": 8470.1748046875,
      "learning_rate": 4.373333333333334e-05,
      "loss": 5.1395,
      "step": 3760
    },
    {
      "epoch": 0.378,
      "grad_norm": 6100.0302734375,
      "learning_rate": 4.3700000000000005e-05,
      "loss": 5.8232,
      "step": 3780
    },
    {
      "epoch": 0.38,
      "grad_norm": 846.7156372070312,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 8.1369,
      "step": 3800
    },
    {
      "epoch": 0.382,
      "grad_norm": 512.4105224609375,
      "learning_rate": 4.3633333333333335e-05,
      "loss": 7.7666,
      "step": 3820
    },
    {
      "epoch": 0.384,
      "grad_norm": 360.0641784667969,
      "learning_rate": 4.36e-05,
      "loss": 5.904,
      "step": 3840
    },
    {
      "epoch": 0.386,
      "grad_norm": 604.0460205078125,
      "learning_rate": 4.3566666666666664e-05,
      "loss": 5.1597,
      "step": 3860
    },
    {
      "epoch": 0.388,
      "grad_norm": 1070.313232421875,
      "learning_rate": 4.353333333333334e-05,
      "loss": 4.5515,
      "step": 3880
    },
    {
      "epoch": 0.39,
      "grad_norm": 614.7106323242188,
      "learning_rate": 4.35e-05,
      "loss": 4.375,
      "step": 3900
    },
    {
      "epoch": 0.392,
      "grad_norm": 63.69440841674805,
      "learning_rate": 4.346666666666667e-05,
      "loss": 3.9714,
      "step": 3920
    },
    {
      "epoch": 0.394,
      "grad_norm": 225.08758544921875,
      "learning_rate": 4.3433333333333336e-05,
      "loss": 3.631,
      "step": 3940
    },
    {
      "epoch": 0.396,
      "grad_norm": 370.06048583984375,
      "learning_rate": 4.3400000000000005e-05,
      "loss": 3.4088,
      "step": 3960
    },
    {
      "epoch": 0.398,
      "grad_norm": 107.27424621582031,
      "learning_rate": 4.3366666666666666e-05,
      "loss": 3.2479,
      "step": 3980
    },
    {
      "epoch": 0.4,
      "grad_norm": 20.3950252532959,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 2.7116,
      "step": 4000
    },
    {
      "epoch": 0.402,
      "grad_norm": 182.60711669921875,
      "learning_rate": 4.33e-05,
      "loss": 2.3362,
      "step": 4020
    },
    {
      "epoch": 0.404,
      "grad_norm": 22.782386779785156,
      "learning_rate": 4.3266666666666664e-05,
      "loss": 2.2488,
      "step": 4040
    },
    {
      "epoch": 0.406,
      "grad_norm": 20.96333885192871,
      "learning_rate": 4.323333333333334e-05,
      "loss": 2.0266,
      "step": 4060
    },
    {
      "epoch": 0.408,
      "grad_norm": 15.741400718688965,
      "learning_rate": 4.32e-05,
      "loss": 1.9254,
      "step": 4080
    },
    {
      "epoch": 0.41,
      "grad_norm": 18.09710121154785,
      "learning_rate": 4.316666666666667e-05,
      "loss": 1.8742,
      "step": 4100
    },
    {
      "epoch": 0.412,
      "grad_norm": 16.32887077331543,
      "learning_rate": 4.3133333333333336e-05,
      "loss": 1.6954,
      "step": 4120
    },
    {
      "epoch": 0.414,
      "grad_norm": 11.66052532196045,
      "learning_rate": 4.3100000000000004e-05,
      "loss": 1.8259,
      "step": 4140
    },
    {
      "epoch": 0.416,
      "grad_norm": 14.090484619140625,
      "learning_rate": 4.3066666666666665e-05,
      "loss": 1.8205,
      "step": 4160
    },
    {
      "epoch": 0.418,
      "grad_norm": 14.986868858337402,
      "learning_rate": 4.3033333333333334e-05,
      "loss": 1.6184,
      "step": 4180
    },
    {
      "epoch": 0.42,
      "grad_norm": 9.40609073638916,
      "learning_rate": 4.3e-05,
      "loss": 1.8087,
      "step": 4200
    },
    {
      "epoch": 0.422,
      "grad_norm": 11.621297836303711,
      "learning_rate": 4.296666666666666e-05,
      "loss": 1.6197,
      "step": 4220
    },
    {
      "epoch": 0.424,
      "grad_norm": 11.838319778442383,
      "learning_rate": 4.293333333333334e-05,
      "loss": 1.5587,
      "step": 4240
    },
    {
      "epoch": 0.426,
      "grad_norm": 10.933357238769531,
      "learning_rate": 4.29e-05,
      "loss": 1.5638,
      "step": 4260
    },
    {
      "epoch": 0.428,
      "grad_norm": 18.23097801208496,
      "learning_rate": 4.286666666666667e-05,
      "loss": 1.5824,
      "step": 4280
    },
    {
      "epoch": 0.43,
      "grad_norm": 15.126986503601074,
      "learning_rate": 4.2833333333333335e-05,
      "loss": 1.4808,
      "step": 4300
    },
    {
      "epoch": 0.432,
      "grad_norm": 18.157743453979492,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 1.5534,
      "step": 4320
    },
    {
      "epoch": 0.434,
      "grad_norm": 14.981832504272461,
      "learning_rate": 4.2766666666666665e-05,
      "loss": 1.6228,
      "step": 4340
    },
    {
      "epoch": 0.436,
      "grad_norm": 9.923134803771973,
      "learning_rate": 4.273333333333333e-05,
      "loss": 1.5775,
      "step": 4360
    },
    {
      "epoch": 0.438,
      "grad_norm": 11.222564697265625,
      "learning_rate": 4.27e-05,
      "loss": 1.5225,
      "step": 4380
    },
    {
      "epoch": 0.44,
      "grad_norm": 18.752948760986328,
      "learning_rate": 4.266666666666667e-05,
      "loss": 1.5003,
      "step": 4400
    },
    {
      "epoch": 0.442,
      "grad_norm": 8.628447532653809,
      "learning_rate": 4.263333333333334e-05,
      "loss": 1.4602,
      "step": 4420
    },
    {
      "epoch": 0.444,
      "grad_norm": 13.296699523925781,
      "learning_rate": 4.26e-05,
      "loss": 1.4365,
      "step": 4440
    },
    {
      "epoch": 0.446,
      "grad_norm": 11.82705020904541,
      "learning_rate": 4.2566666666666674e-05,
      "loss": 1.4254,
      "step": 4460
    },
    {
      "epoch": 0.448,
      "grad_norm": 9.649151802062988,
      "learning_rate": 4.2533333333333335e-05,
      "loss": 1.5118,
      "step": 4480
    },
    {
      "epoch": 0.45,
      "grad_norm": 10.556960105895996,
      "learning_rate": 4.25e-05,
      "loss": 1.4509,
      "step": 4500
    },
    {
      "epoch": 0.452,
      "grad_norm": 9.275834083557129,
      "learning_rate": 4.246666666666667e-05,
      "loss": 1.4206,
      "step": 4520
    },
    {
      "epoch": 0.454,
      "grad_norm": 20.3100643157959,
      "learning_rate": 4.243333333333334e-05,
      "loss": 1.3405,
      "step": 4540
    },
    {
      "epoch": 0.456,
      "grad_norm": 10.732806205749512,
      "learning_rate": 4.24e-05,
      "loss": 1.4623,
      "step": 4560
    },
    {
      "epoch": 0.458,
      "grad_norm": 11.444316864013672,
      "learning_rate": 4.236666666666667e-05,
      "loss": 1.3514,
      "step": 4580
    },
    {
      "epoch": 0.46,
      "grad_norm": 7.986010551452637,
      "learning_rate": 4.233333333333334e-05,
      "loss": 1.3244,
      "step": 4600
    },
    {
      "epoch": 0.462,
      "grad_norm": 10.937310218811035,
      "learning_rate": 4.23e-05,
      "loss": 1.3592,
      "step": 4620
    },
    {
      "epoch": 0.464,
      "grad_norm": 11.762430191040039,
      "learning_rate": 4.226666666666667e-05,
      "loss": 1.4341,
      "step": 4640
    },
    {
      "epoch": 0.466,
      "grad_norm": 12.167478561401367,
      "learning_rate": 4.2233333333333334e-05,
      "loss": 1.3259,
      "step": 4660
    },
    {
      "epoch": 0.468,
      "grad_norm": 7.805748462677002,
      "learning_rate": 4.22e-05,
      "loss": 1.2853,
      "step": 4680
    },
    {
      "epoch": 0.47,
      "grad_norm": 12.044916152954102,
      "learning_rate": 4.216666666666667e-05,
      "loss": 1.2837,
      "step": 4700
    },
    {
      "epoch": 0.472,
      "grad_norm": 10.004934310913086,
      "learning_rate": 4.213333333333334e-05,
      "loss": 1.4176,
      "step": 4720
    },
    {
      "epoch": 0.474,
      "grad_norm": 11.702808380126953,
      "learning_rate": 4.21e-05,
      "loss": 1.2865,
      "step": 4740
    },
    {
      "epoch": 0.476,
      "grad_norm": 9.396081924438477,
      "learning_rate": 4.206666666666667e-05,
      "loss": 1.2438,
      "step": 4760
    },
    {
      "epoch": 0.478,
      "grad_norm": 8.693036079406738,
      "learning_rate": 4.2033333333333336e-05,
      "loss": 1.2694,
      "step": 4780
    },
    {
      "epoch": 0.48,
      "grad_norm": 10.433204650878906,
      "learning_rate": 4.2e-05,
      "loss": 1.3227,
      "step": 4800
    },
    {
      "epoch": 0.482,
      "grad_norm": 11.521235466003418,
      "learning_rate": 4.196666666666667e-05,
      "loss": 1.2717,
      "step": 4820
    },
    {
      "epoch": 0.484,
      "grad_norm": 11.969786643981934,
      "learning_rate": 4.1933333333333334e-05,
      "loss": 1.2398,
      "step": 4840
    },
    {
      "epoch": 0.486,
      "grad_norm": 11.84140396118164,
      "learning_rate": 4.19e-05,
      "loss": 1.1823,
      "step": 4860
    },
    {
      "epoch": 0.488,
      "grad_norm": 7.408459663391113,
      "learning_rate": 4.186666666666667e-05,
      "loss": 1.1986,
      "step": 4880
    },
    {
      "epoch": 0.49,
      "grad_norm": 8.83011531829834,
      "learning_rate": 4.183333333333334e-05,
      "loss": 1.2546,
      "step": 4900
    },
    {
      "epoch": 0.492,
      "grad_norm": 8.973308563232422,
      "learning_rate": 4.18e-05,
      "loss": 1.2749,
      "step": 4920
    },
    {
      "epoch": 0.494,
      "grad_norm": 9.446194648742676,
      "learning_rate": 4.176666666666667e-05,
      "loss": 1.1893,
      "step": 4940
    },
    {
      "epoch": 0.496,
      "grad_norm": 8.389249801635742,
      "learning_rate": 4.1733333333333336e-05,
      "loss": 1.2378,
      "step": 4960
    },
    {
      "epoch": 0.498,
      "grad_norm": 8.562559127807617,
      "learning_rate": 4.17e-05,
      "loss": 1.2369,
      "step": 4980
    },
    {
      "epoch": 0.5,
      "grad_norm": 10.230019569396973,
      "learning_rate": 4.166666666666667e-05,
      "loss": 1.2343,
      "step": 5000
    },
    {
      "epoch": 0.502,
      "grad_norm": 6.249126434326172,
      "learning_rate": 4.1633333333333333e-05,
      "loss": 1.1584,
      "step": 5020
    },
    {
      "epoch": 0.504,
      "grad_norm": 11.21701717376709,
      "learning_rate": 4.16e-05,
      "loss": 1.1414,
      "step": 5040
    },
    {
      "epoch": 0.506,
      "grad_norm": 12.71707820892334,
      "learning_rate": 4.156666666666667e-05,
      "loss": 1.2003,
      "step": 5060
    },
    {
      "epoch": 0.508,
      "grad_norm": 32.90320587158203,
      "learning_rate": 4.153333333333334e-05,
      "loss": 1.1276,
      "step": 5080
    },
    {
      "epoch": 0.51,
      "grad_norm": 60.39689636230469,
      "learning_rate": 4.15e-05,
      "loss": 1.2061,
      "step": 5100
    },
    {
      "epoch": 0.512,
      "grad_norm": 2809.962158203125,
      "learning_rate": 4.146666666666667e-05,
      "loss": 1.231,
      "step": 5120
    },
    {
      "epoch": 0.514,
      "grad_norm": 466.5355224609375,
      "learning_rate": 4.1433333333333335e-05,
      "loss": 1.1761,
      "step": 5140
    },
    {
      "epoch": 0.516,
      "grad_norm": 198.95303344726562,
      "learning_rate": 4.14e-05,
      "loss": 1.197,
      "step": 5160
    },
    {
      "epoch": 0.518,
      "grad_norm": 6439.41357421875,
      "learning_rate": 4.136666666666667e-05,
      "loss": 1.2646,
      "step": 5180
    },
    {
      "epoch": 0.52,
      "grad_norm": 221.97482299804688,
      "learning_rate": 4.133333333333333e-05,
      "loss": 1.2037,
      "step": 5200
    },
    {
      "epoch": 0.522,
      "grad_norm": 2414.43896484375,
      "learning_rate": 4.13e-05,
      "loss": 1.2706,
      "step": 5220
    },
    {
      "epoch": 0.524,
      "grad_norm": 20918.041015625,
      "learning_rate": 4.126666666666667e-05,
      "loss": 1.3255,
      "step": 5240
    },
    {
      "epoch": 0.526,
      "grad_norm": 5914.24658203125,
      "learning_rate": 4.123333333333334e-05,
      "loss": 1.4179,
      "step": 5260
    },
    {
      "epoch": 0.528,
      "grad_norm": 866.8895874023438,
      "learning_rate": 4.12e-05,
      "loss": 1.2722,
      "step": 5280
    },
    {
      "epoch": 0.53,
      "grad_norm": 373.9079284667969,
      "learning_rate": 4.116666666666667e-05,
      "loss": 1.1407,
      "step": 5300
    },
    {
      "epoch": 0.532,
      "grad_norm": 339.9829406738281,
      "learning_rate": 4.1133333333333335e-05,
      "loss": 1.24,
      "step": 5320
    },
    {
      "epoch": 0.534,
      "grad_norm": 1055.4368896484375,
      "learning_rate": 4.11e-05,
      "loss": 1.2041,
      "step": 5340
    },
    {
      "epoch": 0.536,
      "grad_norm": 1174.5472412109375,
      "learning_rate": 4.106666666666667e-05,
      "loss": 1.1952,
      "step": 5360
    },
    {
      "epoch": 0.538,
      "grad_norm": 141.1954803466797,
      "learning_rate": 4.103333333333333e-05,
      "loss": 1.1167,
      "step": 5380
    },
    {
      "epoch": 0.54,
      "grad_norm": 19.766870498657227,
      "learning_rate": 4.1e-05,
      "loss": 1.2061,
      "step": 5400
    },
    {
      "epoch": 0.542,
      "grad_norm": 53.34691619873047,
      "learning_rate": 4.096666666666667e-05,
      "loss": 1.1848,
      "step": 5420
    },
    {
      "epoch": 0.544,
      "grad_norm": 100.54895782470703,
      "learning_rate": 4.093333333333334e-05,
      "loss": 1.2265,
      "step": 5440
    },
    {
      "epoch": 0.546,
      "grad_norm": 313.996337890625,
      "learning_rate": 4.09e-05,
      "loss": 1.2691,
      "step": 5460
    },
    {
      "epoch": 0.548,
      "grad_norm": 122692.6875,
      "learning_rate": 4.086666666666667e-05,
      "loss": 1.3152,
      "step": 5480
    },
    {
      "epoch": 0.55,
      "grad_norm": 3197.016357421875,
      "learning_rate": 4.0833333333333334e-05,
      "loss": 1.4265,
      "step": 5500
    },
    {
      "epoch": 0.552,
      "grad_norm": 46657.6171875,
      "learning_rate": 4.08e-05,
      "loss": 1.2528,
      "step": 5520
    },
    {
      "epoch": 0.554,
      "grad_norm": 7814.1748046875,
      "learning_rate": 4.076666666666667e-05,
      "loss": 1.5681,
      "step": 5540
    },
    {
      "epoch": 0.556,
      "grad_norm": 192.65916442871094,
      "learning_rate": 4.073333333333333e-05,
      "loss": 1.5719,
      "step": 5560
    },
    {
      "epoch": 0.558,
      "grad_norm": 28142.01171875,
      "learning_rate": 4.07e-05,
      "loss": 1.4842,
      "step": 5580
    },
    {
      "epoch": 0.56,
      "grad_norm": 38381.58203125,
      "learning_rate": 4.066666666666667e-05,
      "loss": 1.7881,
      "step": 5600
    },
    {
      "epoch": 0.562,
      "grad_norm": 238352.6875,
      "learning_rate": 4.0633333333333336e-05,
      "loss": 2.4559,
      "step": 5620
    },
    {
      "epoch": 0.564,
      "grad_norm": 6354093.5,
      "learning_rate": 4.0600000000000004e-05,
      "loss": 3.3933,
      "step": 5640
    },
    {
      "epoch": 0.566,
      "grad_norm": 172357.03125,
      "learning_rate": 4.056666666666667e-05,
      "loss": 3.5028,
      "step": 5660
    },
    {
      "epoch": 0.568,
      "grad_norm": 9750937.0,
      "learning_rate": 4.0533333333333334e-05,
      "loss": 3.7178,
      "step": 5680
    },
    {
      "epoch": 0.57,
      "grad_norm": 131800.640625,
      "learning_rate": 4.05e-05,
      "loss": 3.8442,
      "step": 5700
    },
    {
      "epoch": 0.572,
      "grad_norm": 6581521.5,
      "learning_rate": 4.046666666666667e-05,
      "loss": 3.9606,
      "step": 5720
    },
    {
      "epoch": 0.574,
      "grad_norm": 16262372.0,
      "learning_rate": 4.043333333333333e-05,
      "loss": 4.0114,
      "step": 5740
    },
    {
      "epoch": 0.576,
      "grad_norm": 1705244.0,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 3.8354,
      "step": 5760
    },
    {
      "epoch": 0.578,
      "grad_norm": 32207582.0,
      "learning_rate": 4.036666666666667e-05,
      "loss": 4.0907,
      "step": 5780
    },
    {
      "epoch": 0.58,
      "grad_norm": 4443135.5,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 3.8058,
      "step": 5800
    },
    {
      "epoch": 0.582,
      "grad_norm": 10616262.0,
      "learning_rate": 4.0300000000000004e-05,
      "loss": 3.7412,
      "step": 5820
    },
    {
      "epoch": 0.584,
      "grad_norm": 2311841.5,
      "learning_rate": 4.026666666666667e-05,
      "loss": 3.6542,
      "step": 5840
    },
    {
      "epoch": 0.586,
      "grad_norm": 20168946.0,
      "learning_rate": 4.023333333333333e-05,
      "loss": 3.6557,
      "step": 5860
    },
    {
      "epoch": 0.588,
      "grad_norm": 9464340.0,
      "learning_rate": 4.02e-05,
      "loss": 3.8869,
      "step": 5880
    },
    {
      "epoch": 0.59,
      "grad_norm": 10685498.0,
      "learning_rate": 4.016666666666667e-05,
      "loss": 3.8694,
      "step": 5900
    },
    {
      "epoch": 0.592,
      "grad_norm": 756648.0625,
      "learning_rate": 4.013333333333333e-05,
      "loss": 3.832,
      "step": 5920
    },
    {
      "epoch": 0.594,
      "grad_norm": 17851054.0,
      "learning_rate": 4.0100000000000006e-05,
      "loss": 3.7873,
      "step": 5940
    },
    {
      "epoch": 0.596,
      "grad_norm": 1940497.125,
      "learning_rate": 4.006666666666667e-05,
      "loss": 3.5482,
      "step": 5960
    },
    {
      "epoch": 0.598,
      "grad_norm": 4302887.5,
      "learning_rate": 4.0033333333333335e-05,
      "loss": 3.5547,
      "step": 5980
    },
    {
      "epoch": 0.6,
      "grad_norm": 8916169.0,
      "learning_rate": 4e-05,
      "loss": 3.5845,
      "step": 6000
    },
    {
      "epoch": 0.602,
      "grad_norm": 2667464.25,
      "learning_rate": 3.996666666666667e-05,
      "loss": 3.6515,
      "step": 6020
    },
    {
      "epoch": 0.604,
      "grad_norm": 8500541.0,
      "learning_rate": 3.993333333333333e-05,
      "loss": 3.2895,
      "step": 6040
    },
    {
      "epoch": 0.606,
      "grad_norm": 2283612.25,
      "learning_rate": 3.99e-05,
      "loss": 2.9301,
      "step": 6060
    },
    {
      "epoch": 0.608,
      "grad_norm": 14495.685546875,
      "learning_rate": 3.986666666666667e-05,
      "loss": 2.7883,
      "step": 6080
    },
    {
      "epoch": 0.61,
      "grad_norm": 265422.875,
      "learning_rate": 3.983333333333333e-05,
      "loss": 2.4602,
      "step": 6100
    },
    {
      "epoch": 0.612,
      "grad_norm": 45627.5,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 2.4667,
      "step": 6120
    },
    {
      "epoch": 0.614,
      "grad_norm": 16714.048828125,
      "learning_rate": 3.9766666666666667e-05,
      "loss": 2.2582,
      "step": 6140
    },
    {
      "epoch": 0.616,
      "grad_norm": 6212.67529296875,
      "learning_rate": 3.9733333333333335e-05,
      "loss": 2.085,
      "step": 6160
    },
    {
      "epoch": 0.618,
      "grad_norm": 102612.9609375,
      "learning_rate": 3.97e-05,
      "loss": 1.9229,
      "step": 6180
    },
    {
      "epoch": 0.62,
      "grad_norm": 39458.2890625,
      "learning_rate": 3.966666666666667e-05,
      "loss": 1.8729,
      "step": 6200
    },
    {
      "epoch": 0.622,
      "grad_norm": 12852.0107421875,
      "learning_rate": 3.963333333333333e-05,
      "loss": 1.679,
      "step": 6220
    },
    {
      "epoch": 0.624,
      "grad_norm": 100244.046875,
      "learning_rate": 3.960000000000001e-05,
      "loss": 1.6839,
      "step": 6240
    },
    {
      "epoch": 0.626,
      "grad_norm": 48704.171875,
      "learning_rate": 3.956666666666667e-05,
      "loss": 1.5575,
      "step": 6260
    },
    {
      "epoch": 0.628,
      "grad_norm": 13423.828125,
      "learning_rate": 3.9533333333333337e-05,
      "loss": 1.501,
      "step": 6280
    },
    {
      "epoch": 0.63,
      "grad_norm": 15639.4638671875,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 1.5096,
      "step": 6300
    },
    {
      "epoch": 0.632,
      "grad_norm": 284816.21875,
      "learning_rate": 3.9466666666666666e-05,
      "loss": 1.4607,
      "step": 6320
    },
    {
      "epoch": 0.634,
      "grad_norm": 557205.0625,
      "learning_rate": 3.9433333333333334e-05,
      "loss": 1.7949,
      "step": 6340
    },
    {
      "epoch": 0.636,
      "grad_norm": 41859.78515625,
      "learning_rate": 3.94e-05,
      "loss": 1.7996,
      "step": 6360
    },
    {
      "epoch": 0.638,
      "grad_norm": 48816.234375,
      "learning_rate": 3.936666666666667e-05,
      "loss": 1.7386,
      "step": 6380
    },
    {
      "epoch": 0.64,
      "grad_norm": 27688.189453125,
      "learning_rate": 3.933333333333333e-05,
      "loss": 1.7192,
      "step": 6400
    },
    {
      "epoch": 0.642,
      "grad_norm": 3731.047119140625,
      "learning_rate": 3.9300000000000007e-05,
      "loss": 1.67,
      "step": 6420
    },
    {
      "epoch": 0.644,
      "grad_norm": 1212.997314453125,
      "learning_rate": 3.926666666666667e-05,
      "loss": 1.6406,
      "step": 6440
    },
    {
      "epoch": 0.646,
      "grad_norm": 1698.7666015625,
      "learning_rate": 3.9233333333333336e-05,
      "loss": 1.567,
      "step": 6460
    },
    {
      "epoch": 0.648,
      "grad_norm": 6045.13671875,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 1.4257,
      "step": 6480
    },
    {
      "epoch": 0.65,
      "grad_norm": 62385.4765625,
      "learning_rate": 3.9166666666666665e-05,
      "loss": 1.6005,
      "step": 6500
    },
    {
      "epoch": 0.652,
      "grad_norm": 75076.2421875,
      "learning_rate": 3.9133333333333334e-05,
      "loss": 1.7199,
      "step": 6520
    },
    {
      "epoch": 0.654,
      "grad_norm": 59293.421875,
      "learning_rate": 3.91e-05,
      "loss": 1.8558,
      "step": 6540
    },
    {
      "epoch": 0.656,
      "grad_norm": 27499.56640625,
      "learning_rate": 3.906666666666667e-05,
      "loss": 1.7094,
      "step": 6560
    },
    {
      "epoch": 0.658,
      "grad_norm": 30060.76953125,
      "learning_rate": 3.903333333333333e-05,
      "loss": 1.7696,
      "step": 6580
    },
    {
      "epoch": 0.66,
      "grad_norm": 91110.984375,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.5845,
      "step": 6600
    },
    {
      "epoch": 0.662,
      "grad_norm": 19371.89453125,
      "learning_rate": 3.896666666666667e-05,
      "loss": 1.7582,
      "step": 6620
    },
    {
      "epoch": 0.664,
      "grad_norm": 7257.3671875,
      "learning_rate": 3.8933333333333336e-05,
      "loss": 1.4723,
      "step": 6640
    },
    {
      "epoch": 0.666,
      "grad_norm": 13120.4404296875,
      "learning_rate": 3.8900000000000004e-05,
      "loss": 1.5602,
      "step": 6660
    },
    {
      "epoch": 0.668,
      "grad_norm": 27291.9453125,
      "learning_rate": 3.8866666666666665e-05,
      "loss": 1.5413,
      "step": 6680
    },
    {
      "epoch": 0.67,
      "grad_norm": 6813.24755859375,
      "learning_rate": 3.883333333333333e-05,
      "loss": 1.5667,
      "step": 6700
    },
    {
      "epoch": 0.672,
      "grad_norm": 254.15444946289062,
      "learning_rate": 3.88e-05,
      "loss": 1.5964,
      "step": 6720
    },
    {
      "epoch": 0.674,
      "grad_norm": 5968.04638671875,
      "learning_rate": 3.876666666666667e-05,
      "loss": 1.6142,
      "step": 6740
    },
    {
      "epoch": 0.676,
      "grad_norm": 4137.482421875,
      "learning_rate": 3.873333333333333e-05,
      "loss": 1.4241,
      "step": 6760
    },
    {
      "epoch": 0.678,
      "grad_norm": 8030.95263671875,
      "learning_rate": 3.8700000000000006e-05,
      "loss": 1.7621,
      "step": 6780
    },
    {
      "epoch": 0.68,
      "grad_norm": 1940.2772216796875,
      "learning_rate": 3.866666666666667e-05,
      "loss": 1.5255,
      "step": 6800
    },
    {
      "epoch": 0.682,
      "grad_norm": 4682.59716796875,
      "learning_rate": 3.8633333333333335e-05,
      "loss": 1.4523,
      "step": 6820
    },
    {
      "epoch": 0.684,
      "grad_norm": 833707.4375,
      "learning_rate": 3.86e-05,
      "loss": 2.1263,
      "step": 6840
    },
    {
      "epoch": 0.686,
      "grad_norm": 715721.4375,
      "learning_rate": 3.8566666666666664e-05,
      "loss": 4.5684,
      "step": 6860
    },
    {
      "epoch": 0.688,
      "grad_norm": 462477.65625,
      "learning_rate": 3.853333333333334e-05,
      "loss": 5.1995,
      "step": 6880
    },
    {
      "epoch": 0.69,
      "grad_norm": 184142.609375,
      "learning_rate": 3.85e-05,
      "loss": 5.0463,
      "step": 6900
    },
    {
      "epoch": 0.692,
      "grad_norm": 87439.7265625,
      "learning_rate": 3.846666666666667e-05,
      "loss": 4.7459,
      "step": 6920
    },
    {
      "epoch": 0.694,
      "grad_norm": 36735.20703125,
      "learning_rate": 3.843333333333334e-05,
      "loss": 4.6418,
      "step": 6940
    },
    {
      "epoch": 0.696,
      "grad_norm": 98072.640625,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 4.4648,
      "step": 6960
    },
    {
      "epoch": 0.698,
      "grad_norm": 39727.28515625,
      "learning_rate": 3.8366666666666666e-05,
      "loss": 4.3506,
      "step": 6980
    },
    {
      "epoch": 0.7,
      "grad_norm": 166353.78125,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 4.1883,
      "step": 7000
    },
    {
      "epoch": 0.702,
      "grad_norm": 85836.6328125,
      "learning_rate": 3.83e-05,
      "loss": 4.123,
      "step": 7020
    },
    {
      "epoch": 0.704,
      "grad_norm": 381526.09375,
      "learning_rate": 3.8266666666666664e-05,
      "loss": 4.3938,
      "step": 7040
    },
    {
      "epoch": 0.706,
      "grad_norm": 208092.09375,
      "learning_rate": 3.823333333333334e-05,
      "loss": 4.4604,
      "step": 7060
    },
    {
      "epoch": 0.708,
      "grad_norm": 290512.875,
      "learning_rate": 3.82e-05,
      "loss": 4.5109,
      "step": 7080
    },
    {
      "epoch": 0.71,
      "grad_norm": 53161.5703125,
      "learning_rate": 3.816666666666667e-05,
      "loss": 4.7688,
      "step": 7100
    },
    {
      "epoch": 0.712,
      "grad_norm": 33053.4453125,
      "learning_rate": 3.8133333333333336e-05,
      "loss": 5.1093,
      "step": 7120
    },
    {
      "epoch": 0.714,
      "grad_norm": 294050.6875,
      "learning_rate": 3.8100000000000005e-05,
      "loss": 5.7219,
      "step": 7140
    },
    {
      "epoch": 0.716,
      "grad_norm": 417945.53125,
      "learning_rate": 3.8066666666666666e-05,
      "loss": 6.1193,
      "step": 7160
    },
    {
      "epoch": 0.718,
      "grad_norm": 316637.21875,
      "learning_rate": 3.803333333333334e-05,
      "loss": 6.4913,
      "step": 7180
    },
    {
      "epoch": 0.72,
      "grad_norm": 10591698.0,
      "learning_rate": 3.8e-05,
      "loss": 7.2637,
      "step": 7200
    },
    {
      "epoch": 0.722,
      "grad_norm": 1084830.5,
      "learning_rate": 3.796666666666667e-05,
      "loss": 6.9482,
      "step": 7220
    },
    {
      "epoch": 0.724,
      "grad_norm": 7734005.0,
      "learning_rate": 3.793333333333334e-05,
      "loss": 6.9907,
      "step": 7240
    },
    {
      "epoch": 0.726,
      "grad_norm": 433973.53125,
      "learning_rate": 3.79e-05,
      "loss": 7.0019,
      "step": 7260
    },
    {
      "epoch": 0.728,
      "grad_norm": 1461679.25,
      "learning_rate": 3.786666666666667e-05,
      "loss": 6.984,
      "step": 7280
    },
    {
      "epoch": 0.73,
      "grad_norm": 1069990.5,
      "learning_rate": 3.7833333333333336e-05,
      "loss": 7.1001,
      "step": 7300
    },
    {
      "epoch": 0.732,
      "grad_norm": 2250795.0,
      "learning_rate": 3.7800000000000004e-05,
      "loss": 7.3885,
      "step": 7320
    },
    {
      "epoch": 0.734,
      "grad_norm": 842484.4375,
      "learning_rate": 3.7766666666666665e-05,
      "loss": 7.0701,
      "step": 7340
    },
    {
      "epoch": 0.736,
      "grad_norm": 161616.71875,
      "learning_rate": 3.773333333333334e-05,
      "loss": 6.9075,
      "step": 7360
    },
    {
      "epoch": 0.738,
      "grad_norm": 12185293.0,
      "learning_rate": 3.77e-05,
      "loss": 7.0169,
      "step": 7380
    },
    {
      "epoch": 0.74,
      "grad_norm": 1787457.125,
      "learning_rate": 3.766666666666667e-05,
      "loss": 6.8612,
      "step": 7400
    },
    {
      "epoch": 0.742,
      "grad_norm": 5817590.5,
      "learning_rate": 3.763333333333334e-05,
      "loss": 6.986,
      "step": 7420
    },
    {
      "epoch": 0.744,
      "grad_norm": 211309.5625,
      "learning_rate": 3.76e-05,
      "loss": 7.1628,
      "step": 7440
    },
    {
      "epoch": 0.746,
      "grad_norm": 129437.0,
      "learning_rate": 3.756666666666667e-05,
      "loss": 7.3786,
      "step": 7460
    },
    {
      "epoch": 0.748,
      "grad_norm": 44765184.0,
      "learning_rate": 3.7533333333333335e-05,
      "loss": 7.3823,
      "step": 7480
    },
    {
      "epoch": 0.75,
      "grad_norm": 7589487.0,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 7.1068,
      "step": 7500
    },
    {
      "epoch": 0.752,
      "grad_norm": 6921927.5,
      "learning_rate": 3.7466666666666665e-05,
      "loss": 7.1119,
      "step": 7520
    },
    {
      "epoch": 0.754,
      "grad_norm": 915683.6875,
      "learning_rate": 3.743333333333334e-05,
      "loss": 7.0016,
      "step": 7540
    },
    {
      "epoch": 0.756,
      "grad_norm": 6701966.5,
      "learning_rate": 3.74e-05,
      "loss": 6.9742,
      "step": 7560
    },
    {
      "epoch": 0.758,
      "grad_norm": 135810464.0,
      "learning_rate": 3.736666666666667e-05,
      "loss": 7.0027,
      "step": 7580
    },
    {
      "epoch": 0.76,
      "grad_norm": 48536076.0,
      "learning_rate": 3.733333333333334e-05,
      "loss": 7.0112,
      "step": 7600
    },
    {
      "epoch": 0.762,
      "grad_norm": 37693040.0,
      "learning_rate": 3.73e-05,
      "loss": 6.8772,
      "step": 7620
    },
    {
      "epoch": 0.764,
      "grad_norm": 19874626.0,
      "learning_rate": 3.726666666666667e-05,
      "loss": 6.928,
      "step": 7640
    },
    {
      "epoch": 0.766,
      "grad_norm": 13090121.0,
      "learning_rate": 3.7233333333333335e-05,
      "loss": 6.8635,
      "step": 7660
    },
    {
      "epoch": 0.768,
      "grad_norm": 47824964.0,
      "learning_rate": 3.72e-05,
      "loss": 7.0366,
      "step": 7680
    },
    {
      "epoch": 0.77,
      "grad_norm": 2848523.5,
      "learning_rate": 3.7166666666666664e-05,
      "loss": 7.1572,
      "step": 7700
    },
    {
      "epoch": 0.772,
      "grad_norm": 4003040.25,
      "learning_rate": 3.713333333333334e-05,
      "loss": 7.0158,
      "step": 7720
    },
    {
      "epoch": 0.774,
      "grad_norm": 589708.6875,
      "learning_rate": 3.71e-05,
      "loss": 7.0632,
      "step": 7740
    },
    {
      "epoch": 0.776,
      "grad_norm": 4242216.5,
      "learning_rate": 3.706666666666667e-05,
      "loss": 7.0558,
      "step": 7760
    },
    {
      "epoch": 0.778,
      "grad_norm": 1386776.75,
      "learning_rate": 3.703333333333334e-05,
      "loss": 6.9735,
      "step": 7780
    },
    {
      "epoch": 0.78,
      "grad_norm": 5876428.0,
      "learning_rate": 3.7e-05,
      "loss": 7.0356,
      "step": 7800
    },
    {
      "epoch": 0.782,
      "grad_norm": 11706286.0,
      "learning_rate": 3.6966666666666666e-05,
      "loss": 7.0155,
      "step": 7820
    },
    {
      "epoch": 0.784,
      "grad_norm": 26283408.0,
      "learning_rate": 3.6933333333333334e-05,
      "loss": 7.0049,
      "step": 7840
    },
    {
      "epoch": 0.786,
      "grad_norm": 16601164.0,
      "learning_rate": 3.69e-05,
      "loss": 6.9641,
      "step": 7860
    },
    {
      "epoch": 0.788,
      "grad_norm": 12284185.0,
      "learning_rate": 3.6866666666666664e-05,
      "loss": 6.9963,
      "step": 7880
    },
    {
      "epoch": 0.79,
      "grad_norm": 26819958.0,
      "learning_rate": 3.683333333333334e-05,
      "loss": 6.953,
      "step": 7900
    },
    {
      "epoch": 0.792,
      "grad_norm": 6924195.5,
      "learning_rate": 3.68e-05,
      "loss": 6.9775,
      "step": 7920
    },
    {
      "epoch": 0.794,
      "grad_norm": 8274286.0,
      "learning_rate": 3.676666666666667e-05,
      "loss": 6.9426,
      "step": 7940
    },
    {
      "epoch": 0.796,
      "grad_norm": 1749997.75,
      "learning_rate": 3.6733333333333336e-05,
      "loss": 6.8858,
      "step": 7960
    },
    {
      "epoch": 0.798,
      "grad_norm": 973578.375,
      "learning_rate": 3.6700000000000004e-05,
      "loss": 6.9212,
      "step": 7980
    },
    {
      "epoch": 0.8,
      "grad_norm": 10813249.0,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 6.9481,
      "step": 8000
    },
    {
      "epoch": 0.802,
      "grad_norm": 2355670.5,
      "learning_rate": 3.6633333333333334e-05,
      "loss": 6.8388,
      "step": 8020
    },
    {
      "epoch": 0.804,
      "grad_norm": 25707782.0,
      "learning_rate": 3.66e-05,
      "loss": 6.7354,
      "step": 8040
    },
    {
      "epoch": 0.806,
      "grad_norm": 3493281.0,
      "learning_rate": 3.656666666666666e-05,
      "loss": 6.5707,
      "step": 8060
    },
    {
      "epoch": 0.808,
      "grad_norm": 5389555.0,
      "learning_rate": 3.653333333333334e-05,
      "loss": 6.6634,
      "step": 8080
    },
    {
      "epoch": 0.81,
      "grad_norm": 8451055.0,
      "learning_rate": 3.65e-05,
      "loss": 6.5201,
      "step": 8100
    },
    {
      "epoch": 0.812,
      "grad_norm": 9792747.0,
      "learning_rate": 3.646666666666667e-05,
      "loss": 6.6376,
      "step": 8120
    },
    {
      "epoch": 0.814,
      "grad_norm": 10683582.0,
      "learning_rate": 3.6433333333333336e-05,
      "loss": 6.4305,
      "step": 8140
    },
    {
      "epoch": 0.816,
      "grad_norm": 1458782.125,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 6.6291,
      "step": 8160
    },
    {
      "epoch": 0.818,
      "grad_norm": 3186082.5,
      "learning_rate": 3.636666666666667e-05,
      "loss": 6.4324,
      "step": 8180
    },
    {
      "epoch": 0.82,
      "grad_norm": 7842809.0,
      "learning_rate": 3.633333333333333e-05,
      "loss": 6.4505,
      "step": 8200
    },
    {
      "epoch": 0.822,
      "grad_norm": 1414424.375,
      "learning_rate": 3.63e-05,
      "loss": 6.3283,
      "step": 8220
    },
    {
      "epoch": 0.824,
      "grad_norm": 364759.125,
      "learning_rate": 3.626666666666667e-05,
      "loss": 6.4044,
      "step": 8240
    },
    {
      "epoch": 0.826,
      "grad_norm": 7341863.0,
      "learning_rate": 3.623333333333334e-05,
      "loss": 6.3129,
      "step": 8260
    },
    {
      "epoch": 0.828,
      "grad_norm": 7578053.5,
      "learning_rate": 3.62e-05,
      "loss": 6.2279,
      "step": 8280
    },
    {
      "epoch": 0.83,
      "grad_norm": 8920973.0,
      "learning_rate": 3.6166666666666674e-05,
      "loss": 6.1267,
      "step": 8300
    },
    {
      "epoch": 0.832,
      "grad_norm": 266937.875,
      "learning_rate": 3.6133333333333335e-05,
      "loss": 6.1874,
      "step": 8320
    },
    {
      "epoch": 0.834,
      "grad_norm": 14998812.0,
      "learning_rate": 3.61e-05,
      "loss": 6.2024,
      "step": 8340
    },
    {
      "epoch": 0.836,
      "grad_norm": 269930.75,
      "learning_rate": 3.606666666666667e-05,
      "loss": 6.2169,
      "step": 8360
    },
    {
      "epoch": 0.838,
      "grad_norm": 353892.5,
      "learning_rate": 3.603333333333333e-05,
      "loss": 6.0674,
      "step": 8380
    },
    {
      "epoch": 0.84,
      "grad_norm": 1158407.5,
      "learning_rate": 3.6e-05,
      "loss": 6.0968,
      "step": 8400
    },
    {
      "epoch": 0.842,
      "grad_norm": 15536576.0,
      "learning_rate": 3.596666666666667e-05,
      "loss": 6.1008,
      "step": 8420
    },
    {
      "epoch": 0.844,
      "grad_norm": 24368516.0,
      "learning_rate": 3.593333333333334e-05,
      "loss": 6.2214,
      "step": 8440
    },
    {
      "epoch": 0.846,
      "grad_norm": 8643979.0,
      "learning_rate": 3.59e-05,
      "loss": 6.153,
      "step": 8460
    },
    {
      "epoch": 0.848,
      "grad_norm": 8615971.0,
      "learning_rate": 3.586666666666667e-05,
      "loss": 6.3918,
      "step": 8480
    },
    {
      "epoch": 0.85,
      "grad_norm": 295366.125,
      "learning_rate": 3.5833333333333335e-05,
      "loss": 6.1644,
      "step": 8500
    },
    {
      "epoch": 0.852,
      "grad_norm": 2435378.0,
      "learning_rate": 3.58e-05,
      "loss": 6.17,
      "step": 8520
    },
    {
      "epoch": 0.854,
      "grad_norm": 7576226.5,
      "learning_rate": 3.576666666666667e-05,
      "loss": 5.9854,
      "step": 8540
    },
    {
      "epoch": 0.856,
      "grad_norm": 20144426.0,
      "learning_rate": 3.573333333333333e-05,
      "loss": 5.7403,
      "step": 8560
    },
    {
      "epoch": 0.858,
      "grad_norm": 1693252.375,
      "learning_rate": 3.57e-05,
      "loss": 5.7254,
      "step": 8580
    },
    {
      "epoch": 0.86,
      "grad_norm": 21027.509765625,
      "learning_rate": 3.566666666666667e-05,
      "loss": 5.8158,
      "step": 8600
    },
    {
      "epoch": 0.862,
      "grad_norm": 2822311.75,
      "learning_rate": 3.563333333333334e-05,
      "loss": 5.7912,
      "step": 8620
    },
    {
      "epoch": 0.864,
      "grad_norm": 4631086.5,
      "learning_rate": 3.56e-05,
      "loss": 6.2006,
      "step": 8640
    },
    {
      "epoch": 0.866,
      "grad_norm": 1021642.5625,
      "learning_rate": 3.556666666666667e-05,
      "loss": 6.5858,
      "step": 8660
    },
    {
      "epoch": 0.868,
      "grad_norm": 18410268.0,
      "learning_rate": 3.5533333333333334e-05,
      "loss": 6.6881,
      "step": 8680
    },
    {
      "epoch": 0.87,
      "grad_norm": 2844102.5,
      "learning_rate": 3.55e-05,
      "loss": 6.5035,
      "step": 8700
    },
    {
      "epoch": 0.872,
      "grad_norm": 3151644.25,
      "learning_rate": 3.546666666666667e-05,
      "loss": 6.3835,
      "step": 8720
    },
    {
      "epoch": 0.874,
      "grad_norm": 370282.375,
      "learning_rate": 3.543333333333333e-05,
      "loss": 6.3426,
      "step": 8740
    },
    {
      "epoch": 0.876,
      "grad_norm": 15449944.0,
      "learning_rate": 3.54e-05,
      "loss": 6.2722,
      "step": 8760
    },
    {
      "epoch": 0.878,
      "grad_norm": 146169.609375,
      "learning_rate": 3.536666666666667e-05,
      "loss": 6.2552,
      "step": 8780
    },
    {
      "epoch": 0.88,
      "grad_norm": 465351.84375,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 6.2161,
      "step": 8800
    },
    {
      "epoch": 0.882,
      "grad_norm": 1629665.625,
      "learning_rate": 3.53e-05,
      "loss": 6.0363,
      "step": 8820
    },
    {
      "epoch": 0.884,
      "grad_norm": 208539.0,
      "learning_rate": 3.526666666666667e-05,
      "loss": 6.0082,
      "step": 8840
    },
    {
      "epoch": 0.886,
      "grad_norm": 332690.53125,
      "learning_rate": 3.5233333333333334e-05,
      "loss": 5.8543,
      "step": 8860
    },
    {
      "epoch": 0.888,
      "grad_norm": 2031440.75,
      "learning_rate": 3.52e-05,
      "loss": 5.8514,
      "step": 8880
    },
    {
      "epoch": 0.89,
      "grad_norm": 1757130.125,
      "learning_rate": 3.516666666666667e-05,
      "loss": 5.7021,
      "step": 8900
    },
    {
      "epoch": 0.892,
      "grad_norm": 463082.8125,
      "learning_rate": 3.513333333333334e-05,
      "loss": 5.6666,
      "step": 8920
    },
    {
      "epoch": 0.894,
      "grad_norm": 778410.625,
      "learning_rate": 3.51e-05,
      "loss": 5.6882,
      "step": 8940
    },
    {
      "epoch": 0.896,
      "grad_norm": 1156708.0,
      "learning_rate": 3.506666666666667e-05,
      "loss": 5.6014,
      "step": 8960
    },
    {
      "epoch": 0.898,
      "grad_norm": 327574.3125,
      "learning_rate": 3.5033333333333336e-05,
      "loss": 5.4846,
      "step": 8980
    },
    {
      "epoch": 0.9,
      "grad_norm": 3822907.75,
      "learning_rate": 3.5e-05,
      "loss": 5.5582,
      "step": 9000
    },
    {
      "epoch": 0.902,
      "grad_norm": 2433186.0,
      "learning_rate": 3.496666666666667e-05,
      "loss": 5.5254,
      "step": 9020
    },
    {
      "epoch": 0.904,
      "grad_norm": 3325553.5,
      "learning_rate": 3.493333333333333e-05,
      "loss": 5.4276,
      "step": 9040
    },
    {
      "epoch": 0.906,
      "grad_norm": 3219299.25,
      "learning_rate": 3.49e-05,
      "loss": 5.5523,
      "step": 9060
    },
    {
      "epoch": 0.908,
      "grad_norm": 3072453.0,
      "learning_rate": 3.486666666666667e-05,
      "loss": 5.5447,
      "step": 9080
    },
    {
      "epoch": 0.91,
      "grad_norm": 9922980.0,
      "learning_rate": 3.483333333333334e-05,
      "loss": 5.3915,
      "step": 9100
    },
    {
      "epoch": 0.912,
      "grad_norm": 2687212.5,
      "learning_rate": 3.48e-05,
      "loss": 5.3239,
      "step": 9120
    },
    {
      "epoch": 0.914,
      "grad_norm": 344288.875,
      "learning_rate": 3.476666666666667e-05,
      "loss": 5.433,
      "step": 9140
    },
    {
      "epoch": 0.916,
      "grad_norm": 274067.15625,
      "learning_rate": 3.4733333333333335e-05,
      "loss": 5.3115,
      "step": 9160
    },
    {
      "epoch": 0.918,
      "grad_norm": 3613402.5,
      "learning_rate": 3.4699999999999996e-05,
      "loss": 5.3704,
      "step": 9180
    },
    {
      "epoch": 0.92,
      "grad_norm": 142800.1875,
      "learning_rate": 3.466666666666667e-05,
      "loss": 5.2948,
      "step": 9200
    },
    {
      "epoch": 0.922,
      "grad_norm": 1716519.0,
      "learning_rate": 3.463333333333333e-05,
      "loss": 5.1893,
      "step": 9220
    },
    {
      "epoch": 0.924,
      "grad_norm": 1000237.5,
      "learning_rate": 3.46e-05,
      "loss": 5.3247,
      "step": 9240
    },
    {
      "epoch": 0.926,
      "grad_norm": 177038.6875,
      "learning_rate": 3.456666666666667e-05,
      "loss": 5.2343,
      "step": 9260
    },
    {
      "epoch": 0.928,
      "grad_norm": 1579022.125,
      "learning_rate": 3.453333333333334e-05,
      "loss": 5.2134,
      "step": 9280
    },
    {
      "epoch": 0.93,
      "grad_norm": 6716408.0,
      "learning_rate": 3.45e-05,
      "loss": 5.1944,
      "step": 9300
    },
    {
      "epoch": 0.932,
      "grad_norm": 1474410.0,
      "learning_rate": 3.4466666666666666e-05,
      "loss": 5.2415,
      "step": 9320
    },
    {
      "epoch": 0.934,
      "grad_norm": 184779.8125,
      "learning_rate": 3.4433333333333335e-05,
      "loss": 5.0436,
      "step": 9340
    },
    {
      "epoch": 0.936,
      "grad_norm": 145513.09375,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 5.288,
      "step": 9360
    },
    {
      "epoch": 0.938,
      "grad_norm": 88044.296875,
      "learning_rate": 3.436666666666667e-05,
      "loss": 5.3756,
      "step": 9380
    },
    {
      "epoch": 0.94,
      "grad_norm": 2334318.25,
      "learning_rate": 3.433333333333333e-05,
      "loss": 5.1556,
      "step": 9400
    },
    {
      "epoch": 0.942,
      "grad_norm": 4757008.0,
      "learning_rate": 3.430000000000001e-05,
      "loss": 5.0855,
      "step": 9420
    },
    {
      "epoch": 0.944,
      "grad_norm": 202232.6875,
      "learning_rate": 3.426666666666667e-05,
      "loss": 5.1958,
      "step": 9440
    },
    {
      "epoch": 0.946,
      "grad_norm": 938171.5,
      "learning_rate": 3.4233333333333336e-05,
      "loss": 5.0405,
      "step": 9460
    },
    {
      "epoch": 0.948,
      "grad_norm": 405884.9375,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 5.05,
      "step": 9480
    },
    {
      "epoch": 0.95,
      "grad_norm": 69307.21875,
      "learning_rate": 3.4166666666666666e-05,
      "loss": 5.0059,
      "step": 9500
    },
    {
      "epoch": 0.952,
      "grad_norm": 1143930.375,
      "learning_rate": 3.4133333333333334e-05,
      "loss": 5.1818,
      "step": 9520
    },
    {
      "epoch": 0.954,
      "grad_norm": 76863.4453125,
      "learning_rate": 3.41e-05,
      "loss": 4.9766,
      "step": 9540
    },
    {
      "epoch": 0.956,
      "grad_norm": 820180.75,
      "learning_rate": 3.406666666666667e-05,
      "loss": 4.9617,
      "step": 9560
    },
    {
      "epoch": 0.958,
      "grad_norm": 599425.5625,
      "learning_rate": 3.403333333333333e-05,
      "loss": 5.2061,
      "step": 9580
    },
    {
      "epoch": 0.96,
      "grad_norm": 794876.125,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 5.0942,
      "step": 9600
    },
    {
      "epoch": 0.962,
      "grad_norm": 6528913.0,
      "learning_rate": 3.396666666666667e-05,
      "loss": 4.8888,
      "step": 9620
    },
    {
      "epoch": 0.964,
      "grad_norm": 248680.609375,
      "learning_rate": 3.3933333333333336e-05,
      "loss": 4.9159,
      "step": 9640
    },
    {
      "epoch": 0.966,
      "grad_norm": 301998.3125,
      "learning_rate": 3.3900000000000004e-05,
      "loss": 4.8279,
      "step": 9660
    },
    {
      "epoch": 0.968,
      "grad_norm": 860192.0,
      "learning_rate": 3.3866666666666665e-05,
      "loss": 4.9315,
      "step": 9680
    },
    {
      "epoch": 0.97,
      "grad_norm": 108258.8203125,
      "learning_rate": 3.3833333333333334e-05,
      "loss": 4.7125,
      "step": 9700
    },
    {
      "epoch": 0.972,
      "grad_norm": 5658.24658203125,
      "learning_rate": 3.38e-05,
      "loss": 4.4863,
      "step": 9720
    },
    {
      "epoch": 0.974,
      "grad_norm": 7325.91455078125,
      "learning_rate": 3.376666666666667e-05,
      "loss": 3.3769,
      "step": 9740
    },
    {
      "epoch": 0.976,
      "grad_norm": 237.11788940429688,
      "learning_rate": 3.373333333333333e-05,
      "loss": 2.4076,
      "step": 9760
    },
    {
      "epoch": 0.978,
      "grad_norm": 911.4413452148438,
      "learning_rate": 3.3700000000000006e-05,
      "loss": 1.8621,
      "step": 9780
    },
    {
      "epoch": 0.98,
      "grad_norm": 2042.5006103515625,
      "learning_rate": 3.366666666666667e-05,
      "loss": 1.7622,
      "step": 9800
    },
    {
      "epoch": 0.982,
      "grad_norm": 15130.03515625,
      "learning_rate": 3.3633333333333335e-05,
      "loss": 2.038,
      "step": 9820
    },
    {
      "epoch": 0.984,
      "grad_norm": 35.12383270263672,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 3.446,
      "step": 9840
    },
    {
      "epoch": 0.986,
      "grad_norm": 9.588111877441406,
      "learning_rate": 3.356666666666667e-05,
      "loss": 1.7515,
      "step": 9860
    },
    {
      "epoch": 0.988,
      "grad_norm": 10.196602821350098,
      "learning_rate": 3.353333333333333e-05,
      "loss": 1.6873,
      "step": 9880
    },
    {
      "epoch": 0.99,
      "grad_norm": 50.044708251953125,
      "learning_rate": 3.35e-05,
      "loss": 1.4847,
      "step": 9900
    },
    {
      "epoch": 0.992,
      "grad_norm": 385.7933654785156,
      "learning_rate": 3.346666666666667e-05,
      "loss": 1.5431,
      "step": 9920
    },
    {
      "epoch": 0.994,
      "grad_norm": 178.92718505859375,
      "learning_rate": 3.343333333333333e-05,
      "loss": 1.5376,
      "step": 9940
    },
    {
      "epoch": 0.996,
      "grad_norm": 71.84709930419922,
      "learning_rate": 3.3400000000000005e-05,
      "loss": 1.4921,
      "step": 9960
    },
    {
      "epoch": 0.998,
      "grad_norm": 29.780685424804688,
      "learning_rate": 3.336666666666667e-05,
      "loss": 1.4285,
      "step": 9980
    },
    {
      "epoch": 1.0,
      "grad_norm": 9.724891662597656,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 1.321,
      "step": 10000
    },
    {
      "epoch": 1.002,
      "grad_norm": 181.32339477539062,
      "learning_rate": 3.33e-05,
      "loss": 1.2716,
      "step": 10020
    },
    {
      "epoch": 1.004,
      "grad_norm": 36.078311920166016,
      "learning_rate": 3.326666666666667e-05,
      "loss": 1.3361,
      "step": 10040
    },
    {
      "epoch": 1.006,
      "grad_norm": 21.47017478942871,
      "learning_rate": 3.323333333333333e-05,
      "loss": 1.3295,
      "step": 10060
    },
    {
      "epoch": 1.008,
      "grad_norm": 13.196696281433105,
      "learning_rate": 3.32e-05,
      "loss": 1.1939,
      "step": 10080
    },
    {
      "epoch": 1.01,
      "grad_norm": 65.83345794677734,
      "learning_rate": 3.316666666666667e-05,
      "loss": 1.188,
      "step": 10100
    },
    {
      "epoch": 1.012,
      "grad_norm": 15.822761535644531,
      "learning_rate": 3.313333333333333e-05,
      "loss": 1.2671,
      "step": 10120
    },
    {
      "epoch": 1.014,
      "grad_norm": 91.65248107910156,
      "learning_rate": 3.3100000000000005e-05,
      "loss": 1.203,
      "step": 10140
    },
    {
      "epoch": 1.016,
      "grad_norm": 347.2389221191406,
      "learning_rate": 3.3066666666666666e-05,
      "loss": 1.1974,
      "step": 10160
    },
    {
      "epoch": 1.018,
      "grad_norm": 83.05817413330078,
      "learning_rate": 3.3033333333333334e-05,
      "loss": 1.3191,
      "step": 10180
    },
    {
      "epoch": 1.02,
      "grad_norm": 21.028545379638672,
      "learning_rate": 3.3e-05,
      "loss": 1.161,
      "step": 10200
    },
    {
      "epoch": 1.022,
      "grad_norm": 630.3767700195312,
      "learning_rate": 3.296666666666667e-05,
      "loss": 1.208,
      "step": 10220
    },
    {
      "epoch": 1.024,
      "grad_norm": 450.01806640625,
      "learning_rate": 3.293333333333333e-05,
      "loss": 1.1586,
      "step": 10240
    },
    {
      "epoch": 1.026,
      "grad_norm": 477.416259765625,
      "learning_rate": 3.29e-05,
      "loss": 1.1327,
      "step": 10260
    },
    {
      "epoch": 1.028,
      "grad_norm": 1200.3778076171875,
      "learning_rate": 3.286666666666667e-05,
      "loss": 1.1517,
      "step": 10280
    },
    {
      "epoch": 1.03,
      "grad_norm": 2104.25537109375,
      "learning_rate": 3.283333333333333e-05,
      "loss": 1.2222,
      "step": 10300
    },
    {
      "epoch": 1.032,
      "grad_norm": 1469.0816650390625,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 1.1938,
      "step": 10320
    },
    {
      "epoch": 1.034,
      "grad_norm": 4370.9921875,
      "learning_rate": 3.2766666666666666e-05,
      "loss": 1.2268,
      "step": 10340
    },
    {
      "epoch": 1.036,
      "grad_norm": 2265.6630859375,
      "learning_rate": 3.2733333333333334e-05,
      "loss": 1.1798,
      "step": 10360
    },
    {
      "epoch": 1.038,
      "grad_norm": 6124.943359375,
      "learning_rate": 3.27e-05,
      "loss": 1.2455,
      "step": 10380
    },
    {
      "epoch": 1.04,
      "grad_norm": 4599.640625,
      "learning_rate": 3.266666666666667e-05,
      "loss": 1.5119,
      "step": 10400
    },
    {
      "epoch": 1.042,
      "grad_norm": 23489.337890625,
      "learning_rate": 3.263333333333333e-05,
      "loss": 1.6772,
      "step": 10420
    },
    {
      "epoch": 1.044,
      "grad_norm": 3960.75146484375,
      "learning_rate": 3.26e-05,
      "loss": 1.7718,
      "step": 10440
    },
    {
      "epoch": 1.046,
      "grad_norm": 2055.0048828125,
      "learning_rate": 3.256666666666667e-05,
      "loss": 2.5604,
      "step": 10460
    },
    {
      "epoch": 1.048,
      "grad_norm": 1770.9774169921875,
      "learning_rate": 3.253333333333333e-05,
      "loss": 2.3735,
      "step": 10480
    },
    {
      "epoch": 1.05,
      "grad_norm": 1517.843994140625,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 2.6301,
      "step": 10500
    },
    {
      "epoch": 1.052,
      "grad_norm": 1145.624267578125,
      "learning_rate": 3.2466666666666665e-05,
      "loss": 2.6733,
      "step": 10520
    },
    {
      "epoch": 1.054,
      "grad_norm": 1770.1629638671875,
      "learning_rate": 3.243333333333333e-05,
      "loss": 2.7568,
      "step": 10540
    },
    {
      "epoch": 1.056,
      "grad_norm": 3109.73974609375,
      "learning_rate": 3.24e-05,
      "loss": 3.6543,
      "step": 10560
    },
    {
      "epoch": 1.058,
      "grad_norm": 32589.849609375,
      "learning_rate": 3.236666666666667e-05,
      "loss": 3.4048,
      "step": 10580
    },
    {
      "epoch": 1.06,
      "grad_norm": 588.3323364257812,
      "learning_rate": 3.233333333333333e-05,
      "loss": 3.1233,
      "step": 10600
    },
    {
      "epoch": 1.062,
      "grad_norm": 5963.04736328125,
      "learning_rate": 3.2300000000000006e-05,
      "loss": 3.3243,
      "step": 10620
    },
    {
      "epoch": 1.064,
      "grad_norm": 1838.1356201171875,
      "learning_rate": 3.226666666666667e-05,
      "loss": 3.3038,
      "step": 10640
    },
    {
      "epoch": 1.066,
      "grad_norm": 24073.75,
      "learning_rate": 3.2233333333333335e-05,
      "loss": 3.3209,
      "step": 10660
    },
    {
      "epoch": 1.068,
      "grad_norm": 1624.074462890625,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 3.1314,
      "step": 10680
    },
    {
      "epoch": 1.07,
      "grad_norm": 2088.822265625,
      "learning_rate": 3.2166666666666665e-05,
      "loss": 3.1361,
      "step": 10700
    },
    {
      "epoch": 1.072,
      "grad_norm": 1829.293701171875,
      "learning_rate": 3.213333333333334e-05,
      "loss": 3.0571,
      "step": 10720
    },
    {
      "epoch": 1.074,
      "grad_norm": 12376.138671875,
      "learning_rate": 3.21e-05,
      "loss": 3.0178,
      "step": 10740
    },
    {
      "epoch": 1.076,
      "grad_norm": 1090.65087890625,
      "learning_rate": 3.206666666666667e-05,
      "loss": 3.0399,
      "step": 10760
    },
    {
      "epoch": 1.078,
      "grad_norm": 8547.5166015625,
      "learning_rate": 3.203333333333334e-05,
      "loss": 2.9878,
      "step": 10780
    },
    {
      "epoch": 1.08,
      "grad_norm": 24589.72265625,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 3.0442,
      "step": 10800
    },
    {
      "epoch": 1.082,
      "grad_norm": 10189.4326171875,
      "learning_rate": 3.196666666666667e-05,
      "loss": 3.0292,
      "step": 10820
    },
    {
      "epoch": 1.084,
      "grad_norm": 5835.8662109375,
      "learning_rate": 3.1933333333333335e-05,
      "loss": 3.1723,
      "step": 10840
    },
    {
      "epoch": 1.086,
      "grad_norm": 83328.2265625,
      "learning_rate": 3.19e-05,
      "loss": 3.0155,
      "step": 10860
    },
    {
      "epoch": 1.088,
      "grad_norm": 56429.921875,
      "learning_rate": 3.1866666666666664e-05,
      "loss": 3.026,
      "step": 10880
    },
    {
      "epoch": 1.09,
      "grad_norm": 780749.875,
      "learning_rate": 3.183333333333334e-05,
      "loss": 2.8966,
      "step": 10900
    },
    {
      "epoch": 1.092,
      "grad_norm": 58282.88671875,
      "learning_rate": 3.18e-05,
      "loss": 3.0015,
      "step": 10920
    },
    {
      "epoch": 1.094,
      "grad_norm": 89229.9453125,
      "learning_rate": 3.176666666666667e-05,
      "loss": 2.9659,
      "step": 10940
    },
    {
      "epoch": 1.096,
      "grad_norm": 450461.375,
      "learning_rate": 3.173333333333334e-05,
      "loss": 3.0133,
      "step": 10960
    },
    {
      "epoch": 1.098,
      "grad_norm": 3576103.5,
      "learning_rate": 3.1700000000000005e-05,
      "loss": 2.9601,
      "step": 10980
    },
    {
      "epoch": 1.1,
      "grad_norm": 929344.3125,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 3.01,
      "step": 11000
    },
    {
      "epoch": 1.102,
      "grad_norm": 351130.46875,
      "learning_rate": 3.1633333333333334e-05,
      "loss": 2.9534,
      "step": 11020
    },
    {
      "epoch": 1.104,
      "grad_norm": 461349.0625,
      "learning_rate": 3.16e-05,
      "loss": 2.9871,
      "step": 11040
    },
    {
      "epoch": 1.106,
      "grad_norm": 122031.9296875,
      "learning_rate": 3.1566666666666664e-05,
      "loss": 2.8992,
      "step": 11060
    },
    {
      "epoch": 1.108,
      "grad_norm": 588847.125,
      "learning_rate": 3.153333333333334e-05,
      "loss": 2.9531,
      "step": 11080
    },
    {
      "epoch": 1.11,
      "grad_norm": 1915607.125,
      "learning_rate": 3.15e-05,
      "loss": 2.8254,
      "step": 11100
    },
    {
      "epoch": 1.112,
      "grad_norm": 77734.5234375,
      "learning_rate": 3.146666666666667e-05,
      "loss": 2.844,
      "step": 11120
    },
    {
      "epoch": 1.114,
      "grad_norm": 168949.40625,
      "learning_rate": 3.1433333333333336e-05,
      "loss": 2.8244,
      "step": 11140
    },
    {
      "epoch": 1.116,
      "grad_norm": 38819.30859375,
      "learning_rate": 3.1400000000000004e-05,
      "loss": 2.8652,
      "step": 11160
    },
    {
      "epoch": 1.1179999999999999,
      "grad_norm": 1591904.375,
      "learning_rate": 3.1366666666666666e-05,
      "loss": 2.8981,
      "step": 11180
    },
    {
      "epoch": 1.12,
      "grad_norm": 230451.75,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 2.8748,
      "step": 11200
    },
    {
      "epoch": 1.1219999999999999,
      "grad_norm": 86986.828125,
      "learning_rate": 3.13e-05,
      "loss": 2.8418,
      "step": 11220
    },
    {
      "epoch": 1.124,
      "grad_norm": 3130059.0,
      "learning_rate": 3.126666666666666e-05,
      "loss": 2.8452,
      "step": 11240
    },
    {
      "epoch": 1.126,
      "grad_norm": 156289.015625,
      "learning_rate": 3.123333333333334e-05,
      "loss": 2.7907,
      "step": 11260
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 187304.3125,
      "learning_rate": 3.12e-05,
      "loss": 2.8626,
      "step": 11280
    },
    {
      "epoch": 1.13,
      "grad_norm": 313899.53125,
      "learning_rate": 3.116666666666667e-05,
      "loss": 2.7928,
      "step": 11300
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 315579.125,
      "learning_rate": 3.1133333333333336e-05,
      "loss": 2.769,
      "step": 11320
    },
    {
      "epoch": 1.134,
      "grad_norm": 2962139.25,
      "learning_rate": 3.1100000000000004e-05,
      "loss": 2.9598,
      "step": 11340
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 295486.375,
      "learning_rate": 3.1066666666666665e-05,
      "loss": 2.8771,
      "step": 11360
    },
    {
      "epoch": 1.138,
      "grad_norm": 185176.875,
      "learning_rate": 3.103333333333333e-05,
      "loss": 2.8665,
      "step": 11380
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 138763.078125,
      "learning_rate": 3.1e-05,
      "loss": 2.7162,
      "step": 11400
    },
    {
      "epoch": 1.142,
      "grad_norm": 48965.8984375,
      "learning_rate": 3.096666666666666e-05,
      "loss": 2.7833,
      "step": 11420
    },
    {
      "epoch": 1.144,
      "grad_norm": 360286.15625,
      "learning_rate": 3.093333333333334e-05,
      "loss": 2.8381,
      "step": 11440
    },
    {
      "epoch": 1.146,
      "grad_norm": 242861.34375,
      "learning_rate": 3.09e-05,
      "loss": 2.9206,
      "step": 11460
    },
    {
      "epoch": 1.148,
      "grad_norm": 172882.5,
      "learning_rate": 3.086666666666667e-05,
      "loss": 2.8276,
      "step": 11480
    },
    {
      "epoch": 1.15,
      "grad_norm": 53127.80859375,
      "learning_rate": 3.0833333333333335e-05,
      "loss": 2.826,
      "step": 11500
    },
    {
      "epoch": 1.152,
      "grad_norm": 30826.5625,
      "learning_rate": 3.08e-05,
      "loss": 2.786,
      "step": 11520
    },
    {
      "epoch": 1.154,
      "grad_norm": 1682.9078369140625,
      "learning_rate": 3.0766666666666665e-05,
      "loss": 2.9081,
      "step": 11540
    },
    {
      "epoch": 1.156,
      "grad_norm": 259425.71875,
      "learning_rate": 3.073333333333334e-05,
      "loss": 3.058,
      "step": 11560
    },
    {
      "epoch": 1.158,
      "grad_norm": 83043.8046875,
      "learning_rate": 3.07e-05,
      "loss": 3.0107,
      "step": 11580
    },
    {
      "epoch": 1.16,
      "grad_norm": 132830.578125,
      "learning_rate": 3.066666666666667e-05,
      "loss": 2.8094,
      "step": 11600
    },
    {
      "epoch": 1.162,
      "grad_norm": 550918.6875,
      "learning_rate": 3.063333333333334e-05,
      "loss": 3.0183,
      "step": 11620
    },
    {
      "epoch": 1.164,
      "grad_norm": 531781.0,
      "learning_rate": 3.06e-05,
      "loss": 3.5915,
      "step": 11640
    },
    {
      "epoch": 1.166,
      "grad_norm": 31165.3203125,
      "learning_rate": 3.0566666666666667e-05,
      "loss": 2.9255,
      "step": 11660
    },
    {
      "epoch": 1.168,
      "grad_norm": 1192.7523193359375,
      "learning_rate": 3.0533333333333335e-05,
      "loss": 2.745,
      "step": 11680
    },
    {
      "epoch": 1.17,
      "grad_norm": 1709.894287109375,
      "learning_rate": 3.05e-05,
      "loss": 3.0084,
      "step": 11700
    },
    {
      "epoch": 1.172,
      "grad_norm": 125218.5234375,
      "learning_rate": 3.0466666666666664e-05,
      "loss": 3.2397,
      "step": 11720
    },
    {
      "epoch": 1.174,
      "grad_norm": 35081.0,
      "learning_rate": 3.0433333333333336e-05,
      "loss": 3.2782,
      "step": 11740
    },
    {
      "epoch": 1.176,
      "grad_norm": 26123.0390625,
      "learning_rate": 3.04e-05,
      "loss": 3.4492,
      "step": 11760
    },
    {
      "epoch": 1.178,
      "grad_norm": 114275.390625,
      "learning_rate": 3.0366666666666665e-05,
      "loss": 3.1567,
      "step": 11780
    },
    {
      "epoch": 1.18,
      "grad_norm": 150757.984375,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 3.1219,
      "step": 11800
    },
    {
      "epoch": 1.182,
      "grad_norm": 148475.453125,
      "learning_rate": 3.03e-05,
      "loss": 3.0871,
      "step": 11820
    },
    {
      "epoch": 1.184,
      "grad_norm": 20302.2734375,
      "learning_rate": 3.0266666666666666e-05,
      "loss": 3.1407,
      "step": 11840
    },
    {
      "epoch": 1.186,
      "grad_norm": 908752.9375,
      "learning_rate": 3.0233333333333334e-05,
      "loss": 3.1752,
      "step": 11860
    },
    {
      "epoch": 1.188,
      "grad_norm": 2404694.5,
      "learning_rate": 3.02e-05,
      "loss": 3.1298,
      "step": 11880
    },
    {
      "epoch": 1.19,
      "grad_norm": 194723.640625,
      "learning_rate": 3.016666666666667e-05,
      "loss": 3.0654,
      "step": 11900
    },
    {
      "epoch": 1.192,
      "grad_norm": 361555.1875,
      "learning_rate": 3.0133333333333335e-05,
      "loss": 3.1675,
      "step": 11920
    },
    {
      "epoch": 1.194,
      "grad_norm": 12135.3818359375,
      "learning_rate": 3.01e-05,
      "loss": 2.9997,
      "step": 11940
    },
    {
      "epoch": 1.196,
      "grad_norm": 1412839.0,
      "learning_rate": 3.006666666666667e-05,
      "loss": 3.0708,
      "step": 11960
    },
    {
      "epoch": 1.198,
      "grad_norm": 829572.25,
      "learning_rate": 3.0033333333333336e-05,
      "loss": 3.082,
      "step": 11980
    },
    {
      "epoch": 1.2,
      "grad_norm": 396824.71875,
      "learning_rate": 3e-05,
      "loss": 3.1206,
      "step": 12000
    },
    {
      "epoch": 1.202,
      "grad_norm": 378959.9375,
      "learning_rate": 2.9966666666666672e-05,
      "loss": 3.0172,
      "step": 12020
    },
    {
      "epoch": 1.204,
      "grad_norm": 269262.65625,
      "learning_rate": 2.9933333333333337e-05,
      "loss": 3.146,
      "step": 12040
    },
    {
      "epoch": 1.206,
      "grad_norm": 2938052.25,
      "learning_rate": 2.9900000000000002e-05,
      "loss": 3.1483,
      "step": 12060
    },
    {
      "epoch": 1.208,
      "grad_norm": 5658438.5,
      "learning_rate": 2.986666666666667e-05,
      "loss": 3.0442,
      "step": 12080
    },
    {
      "epoch": 1.21,
      "grad_norm": 1883618.625,
      "learning_rate": 2.9833333333333335e-05,
      "loss": 3.2436,
      "step": 12100
    },
    {
      "epoch": 1.212,
      "grad_norm": 549188.5625,
      "learning_rate": 2.98e-05,
      "loss": 3.0956,
      "step": 12120
    },
    {
      "epoch": 1.214,
      "grad_norm": 702096.875,
      "learning_rate": 2.976666666666667e-05,
      "loss": 3.2964,
      "step": 12140
    },
    {
      "epoch": 1.216,
      "grad_norm": 879957.4375,
      "learning_rate": 2.9733333333333336e-05,
      "loss": 3.2778,
      "step": 12160
    },
    {
      "epoch": 1.218,
      "grad_norm": 2597416.0,
      "learning_rate": 2.97e-05,
      "loss": 3.0856,
      "step": 12180
    },
    {
      "epoch": 1.22,
      "grad_norm": 1743353.875,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 3.1693,
      "step": 12200
    },
    {
      "epoch": 1.222,
      "grad_norm": 518725.09375,
      "learning_rate": 2.9633333333333336e-05,
      "loss": 3.066,
      "step": 12220
    },
    {
      "epoch": 1.224,
      "grad_norm": 458113.59375,
      "learning_rate": 2.96e-05,
      "loss": 3.2114,
      "step": 12240
    },
    {
      "epoch": 1.226,
      "grad_norm": 122807.9296875,
      "learning_rate": 2.956666666666667e-05,
      "loss": 2.9362,
      "step": 12260
    },
    {
      "epoch": 1.228,
      "grad_norm": 1370015.375,
      "learning_rate": 2.9533333333333334e-05,
      "loss": 3.0512,
      "step": 12280
    },
    {
      "epoch": 1.23,
      "grad_norm": 887297.625,
      "learning_rate": 2.95e-05,
      "loss": 3.1269,
      "step": 12300
    },
    {
      "epoch": 1.232,
      "grad_norm": 367503.625,
      "learning_rate": 2.946666666666667e-05,
      "loss": 3.0584,
      "step": 12320
    },
    {
      "epoch": 1.234,
      "grad_norm": 15855.0576171875,
      "learning_rate": 2.9433333333333335e-05,
      "loss": 3.0372,
      "step": 12340
    },
    {
      "epoch": 1.236,
      "grad_norm": 425337.40625,
      "learning_rate": 2.94e-05,
      "loss": 3.0178,
      "step": 12360
    },
    {
      "epoch": 1.238,
      "grad_norm": 154568.984375,
      "learning_rate": 2.936666666666667e-05,
      "loss": 2.9689,
      "step": 12380
    },
    {
      "epoch": 1.24,
      "grad_norm": 158230.96875,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 3.0594,
      "step": 12400
    },
    {
      "epoch": 1.242,
      "grad_norm": 83873.6328125,
      "learning_rate": 2.93e-05,
      "loss": 2.9128,
      "step": 12420
    },
    {
      "epoch": 1.244,
      "grad_norm": 1114384.375,
      "learning_rate": 2.926666666666667e-05,
      "loss": 2.9426,
      "step": 12440
    },
    {
      "epoch": 1.246,
      "grad_norm": 286980.90625,
      "learning_rate": 2.9233333333333334e-05,
      "loss": 3.0175,
      "step": 12460
    },
    {
      "epoch": 1.248,
      "grad_norm": 18874.57421875,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 3.118,
      "step": 12480
    },
    {
      "epoch": 1.25,
      "grad_norm": 87492.03125,
      "learning_rate": 2.916666666666667e-05,
      "loss": 2.8591,
      "step": 12500
    },
    {
      "epoch": 1.252,
      "grad_norm": 6547.67529296875,
      "learning_rate": 2.9133333333333334e-05,
      "loss": 3.0494,
      "step": 12520
    },
    {
      "epoch": 1.254,
      "grad_norm": 310203.09375,
      "learning_rate": 2.91e-05,
      "loss": 3.0558,
      "step": 12540
    },
    {
      "epoch": 1.256,
      "grad_norm": 278009.25,
      "learning_rate": 2.906666666666667e-05,
      "loss": 3.0895,
      "step": 12560
    },
    {
      "epoch": 1.258,
      "grad_norm": 295092.375,
      "learning_rate": 2.9033333333333335e-05,
      "loss": 2.8594,
      "step": 12580
    },
    {
      "epoch": 1.26,
      "grad_norm": 159323.53125,
      "learning_rate": 2.9e-05,
      "loss": 2.8736,
      "step": 12600
    },
    {
      "epoch": 1.262,
      "grad_norm": 217555.765625,
      "learning_rate": 2.8966666666666668e-05,
      "loss": 3.0124,
      "step": 12620
    },
    {
      "epoch": 1.264,
      "grad_norm": 108546.8984375,
      "learning_rate": 2.8933333333333333e-05,
      "loss": 2.9,
      "step": 12640
    },
    {
      "epoch": 1.266,
      "grad_norm": 104203.4453125,
      "learning_rate": 2.8899999999999998e-05,
      "loss": 2.9656,
      "step": 12660
    },
    {
      "epoch": 1.268,
      "grad_norm": 108553.4453125,
      "learning_rate": 2.886666666666667e-05,
      "loss": 2.9094,
      "step": 12680
    },
    {
      "epoch": 1.27,
      "grad_norm": 107071.6484375,
      "learning_rate": 2.8833333333333334e-05,
      "loss": 2.8229,
      "step": 12700
    },
    {
      "epoch": 1.272,
      "grad_norm": 23649.08203125,
      "learning_rate": 2.88e-05,
      "loss": 2.7777,
      "step": 12720
    },
    {
      "epoch": 1.274,
      "grad_norm": 124856.0625,
      "learning_rate": 2.876666666666667e-05,
      "loss": 2.7563,
      "step": 12740
    },
    {
      "epoch": 1.276,
      "grad_norm": 297360.3125,
      "learning_rate": 2.8733333333333335e-05,
      "loss": 2.7981,
      "step": 12760
    },
    {
      "epoch": 1.278,
      "grad_norm": 44917.97265625,
      "learning_rate": 2.87e-05,
      "loss": 2.5848,
      "step": 12780
    },
    {
      "epoch": 1.28,
      "grad_norm": 8232.4462890625,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 2.9306,
      "step": 12800
    },
    {
      "epoch": 1.282,
      "grad_norm": 262776.5625,
      "learning_rate": 2.8633333333333336e-05,
      "loss": 2.8559,
      "step": 12820
    },
    {
      "epoch": 1.284,
      "grad_norm": 77319.21875,
      "learning_rate": 2.86e-05,
      "loss": 2.7903,
      "step": 12840
    },
    {
      "epoch": 1.286,
      "grad_norm": 168096.953125,
      "learning_rate": 2.856666666666667e-05,
      "loss": 2.8289,
      "step": 12860
    },
    {
      "epoch": 1.288,
      "grad_norm": 225651.65625,
      "learning_rate": 2.8533333333333333e-05,
      "loss": 2.844,
      "step": 12880
    },
    {
      "epoch": 1.29,
      "grad_norm": 78853.4296875,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 2.8178,
      "step": 12900
    },
    {
      "epoch": 1.292,
      "grad_norm": 183107.8125,
      "learning_rate": 2.846666666666667e-05,
      "loss": 3.2485,
      "step": 12920
    },
    {
      "epoch": 1.294,
      "grad_norm": 339557.5625,
      "learning_rate": 2.8433333333333334e-05,
      "loss": 3.0392,
      "step": 12940
    },
    {
      "epoch": 1.296,
      "grad_norm": 84949.1640625,
      "learning_rate": 2.84e-05,
      "loss": 2.9816,
      "step": 12960
    },
    {
      "epoch": 1.298,
      "grad_norm": 246014.65625,
      "learning_rate": 2.836666666666667e-05,
      "loss": 2.7645,
      "step": 12980
    },
    {
      "epoch": 1.3,
      "grad_norm": 351078.96875,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 2.8012,
      "step": 13000
    },
    {
      "epoch": 1.302,
      "grad_norm": 235075.34375,
      "learning_rate": 2.83e-05,
      "loss": 2.7931,
      "step": 13020
    },
    {
      "epoch": 1.304,
      "grad_norm": 61597.890625,
      "learning_rate": 2.8266666666666668e-05,
      "loss": 2.7285,
      "step": 13040
    },
    {
      "epoch": 1.306,
      "grad_norm": 26099.234375,
      "learning_rate": 2.8233333333333333e-05,
      "loss": 2.8309,
      "step": 13060
    },
    {
      "epoch": 1.308,
      "grad_norm": 164140.578125,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 2.6756,
      "step": 13080
    },
    {
      "epoch": 1.31,
      "grad_norm": 212663.640625,
      "learning_rate": 2.816666666666667e-05,
      "loss": 3.0718,
      "step": 13100
    },
    {
      "epoch": 1.312,
      "grad_norm": 159680.6875,
      "learning_rate": 2.8133333333333334e-05,
      "loss": 2.6634,
      "step": 13120
    },
    {
      "epoch": 1.314,
      "grad_norm": 239174.96875,
      "learning_rate": 2.8100000000000005e-05,
      "loss": 2.8372,
      "step": 13140
    },
    {
      "epoch": 1.316,
      "grad_norm": 26730.720703125,
      "learning_rate": 2.806666666666667e-05,
      "loss": 2.7959,
      "step": 13160
    },
    {
      "epoch": 1.318,
      "grad_norm": 135928.921875,
      "learning_rate": 2.8033333333333335e-05,
      "loss": 2.9997,
      "step": 13180
    },
    {
      "epoch": 1.32,
      "grad_norm": 9657.140625,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 2.8805,
      "step": 13200
    },
    {
      "epoch": 1.322,
      "grad_norm": 4468.14453125,
      "learning_rate": 2.7966666666666668e-05,
      "loss": 2.824,
      "step": 13220
    },
    {
      "epoch": 1.324,
      "grad_norm": 126020.78125,
      "learning_rate": 2.7933333333333332e-05,
      "loss": 2.614,
      "step": 13240
    },
    {
      "epoch": 1.326,
      "grad_norm": 924847.1875,
      "learning_rate": 2.7900000000000004e-05,
      "loss": 2.5546,
      "step": 13260
    },
    {
      "epoch": 1.328,
      "grad_norm": 72909.484375,
      "learning_rate": 2.786666666666667e-05,
      "loss": 2.6616,
      "step": 13280
    },
    {
      "epoch": 1.33,
      "grad_norm": 1720340.625,
      "learning_rate": 2.7833333333333333e-05,
      "loss": 2.6986,
      "step": 13300
    },
    {
      "epoch": 1.332,
      "grad_norm": 1549458.75,
      "learning_rate": 2.7800000000000005e-05,
      "loss": 2.6245,
      "step": 13320
    },
    {
      "epoch": 1.334,
      "grad_norm": 3925283.25,
      "learning_rate": 2.776666666666667e-05,
      "loss": 2.718,
      "step": 13340
    },
    {
      "epoch": 1.336,
      "grad_norm": 185785.3125,
      "learning_rate": 2.7733333333333334e-05,
      "loss": 2.5321,
      "step": 13360
    },
    {
      "epoch": 1.338,
      "grad_norm": 1231198.375,
      "learning_rate": 2.7700000000000002e-05,
      "loss": 2.7085,
      "step": 13380
    },
    {
      "epoch": 1.34,
      "grad_norm": 171855.0625,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 2.5947,
      "step": 13400
    },
    {
      "epoch": 1.342,
      "grad_norm": 739044.625,
      "learning_rate": 2.7633333333333332e-05,
      "loss": 2.4689,
      "step": 13420
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 2136914.75,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 2.6989,
      "step": 13440
    },
    {
      "epoch": 1.346,
      "grad_norm": 555828.625,
      "learning_rate": 2.7566666666666668e-05,
      "loss": 2.7143,
      "step": 13460
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 1335193.5,
      "learning_rate": 2.7533333333333333e-05,
      "loss": 2.5624,
      "step": 13480
    },
    {
      "epoch": 1.35,
      "grad_norm": 335746.40625,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 2.4994,
      "step": 13500
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 1235600.125,
      "learning_rate": 2.746666666666667e-05,
      "loss": 2.7193,
      "step": 13520
    },
    {
      "epoch": 1.354,
      "grad_norm": 1885658.625,
      "learning_rate": 2.7433333333333334e-05,
      "loss": 2.6403,
      "step": 13540
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 2444389.0,
      "learning_rate": 2.7400000000000002e-05,
      "loss": 2.6891,
      "step": 13560
    },
    {
      "epoch": 1.358,
      "grad_norm": 3160369.0,
      "learning_rate": 2.7366666666666667e-05,
      "loss": 2.6113,
      "step": 13580
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 3718755.25,
      "learning_rate": 2.733333333333333e-05,
      "loss": 2.5844,
      "step": 13600
    },
    {
      "epoch": 1.362,
      "grad_norm": 3358835.75,
      "learning_rate": 2.7300000000000003e-05,
      "loss": 2.7213,
      "step": 13620
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 1219543.75,
      "learning_rate": 2.7266666666666668e-05,
      "loss": 2.7549,
      "step": 13640
    },
    {
      "epoch": 1.366,
      "grad_norm": 2177365.75,
      "learning_rate": 2.7233333333333332e-05,
      "loss": 2.557,
      "step": 13660
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 498768.25,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 2.7889,
      "step": 13680
    },
    {
      "epoch": 1.37,
      "grad_norm": 2608224.5,
      "learning_rate": 2.716666666666667e-05,
      "loss": 2.6811,
      "step": 13700
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 6313540.0,
      "learning_rate": 2.7133333333333333e-05,
      "loss": 2.7313,
      "step": 13720
    },
    {
      "epoch": 1.374,
      "grad_norm": 320705.6875,
      "learning_rate": 2.7100000000000005e-05,
      "loss": 2.656,
      "step": 13740
    },
    {
      "epoch": 1.376,
      "grad_norm": 21735.8125,
      "learning_rate": 2.706666666666667e-05,
      "loss": 2.7143,
      "step": 13760
    },
    {
      "epoch": 1.3780000000000001,
      "grad_norm": 7445700.0,
      "learning_rate": 2.7033333333333334e-05,
      "loss": 2.8722,
      "step": 13780
    },
    {
      "epoch": 1.38,
      "grad_norm": 5087087.0,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 2.9218,
      "step": 13800
    },
    {
      "epoch": 1.3820000000000001,
      "grad_norm": 6505759.0,
      "learning_rate": 2.6966666666666667e-05,
      "loss": 2.5442,
      "step": 13820
    },
    {
      "epoch": 1.384,
      "grad_norm": 10400876.0,
      "learning_rate": 2.6933333333333332e-05,
      "loss": 2.7458,
      "step": 13840
    },
    {
      "epoch": 1.3860000000000001,
      "grad_norm": 2475050.75,
      "learning_rate": 2.6900000000000003e-05,
      "loss": 2.7171,
      "step": 13860
    },
    {
      "epoch": 1.388,
      "grad_norm": 26212176.0,
      "learning_rate": 2.6866666666666668e-05,
      "loss": 2.5416,
      "step": 13880
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 10590866.0,
      "learning_rate": 2.6833333333333333e-05,
      "loss": 2.7226,
      "step": 13900
    },
    {
      "epoch": 1.392,
      "grad_norm": 3832352.5,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 2.6811,
      "step": 13920
    },
    {
      "epoch": 1.3940000000000001,
      "grad_norm": 26097878.0,
      "learning_rate": 2.676666666666667e-05,
      "loss": 2.5777,
      "step": 13940
    },
    {
      "epoch": 1.396,
      "grad_norm": 5230437.0,
      "learning_rate": 2.6733333333333334e-05,
      "loss": 2.7855,
      "step": 13960
    },
    {
      "epoch": 1.3980000000000001,
      "grad_norm": 5323421.5,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 2.7022,
      "step": 13980
    },
    {
      "epoch": 1.4,
      "grad_norm": 2463488.5,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 2.745,
      "step": 14000
    },
    {
      "epoch": 1.4020000000000001,
      "grad_norm": 44448776.0,
      "learning_rate": 2.663333333333333e-05,
      "loss": 2.7765,
      "step": 14020
    },
    {
      "epoch": 1.404,
      "grad_norm": 5818936.5,
      "learning_rate": 2.6600000000000003e-05,
      "loss": 2.7248,
      "step": 14040
    },
    {
      "epoch": 1.4060000000000001,
      "grad_norm": 11899958.0,
      "learning_rate": 2.6566666666666668e-05,
      "loss": 2.87,
      "step": 14060
    },
    {
      "epoch": 1.408,
      "grad_norm": 13293641.0,
      "learning_rate": 2.6533333333333332e-05,
      "loss": 2.833,
      "step": 14080
    },
    {
      "epoch": 1.41,
      "grad_norm": 18490874.0,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 2.7302,
      "step": 14100
    },
    {
      "epoch": 1.412,
      "grad_norm": 12513694.0,
      "learning_rate": 2.646666666666667e-05,
      "loss": 2.9678,
      "step": 14120
    },
    {
      "epoch": 1.414,
      "grad_norm": 24832034.0,
      "learning_rate": 2.6433333333333333e-05,
      "loss": 2.8183,
      "step": 14140
    },
    {
      "epoch": 1.416,
      "grad_norm": 45577752.0,
      "learning_rate": 2.64e-05,
      "loss": 2.8186,
      "step": 14160
    },
    {
      "epoch": 1.418,
      "grad_norm": 14616083.0,
      "learning_rate": 2.6366666666666666e-05,
      "loss": 2.9685,
      "step": 14180
    },
    {
      "epoch": 1.42,
      "grad_norm": 4039725.75,
      "learning_rate": 2.633333333333333e-05,
      "loss": 2.9725,
      "step": 14200
    },
    {
      "epoch": 1.422,
      "grad_norm": 93629144.0,
      "learning_rate": 2.6300000000000002e-05,
      "loss": 2.9566,
      "step": 14220
    },
    {
      "epoch": 1.424,
      "grad_norm": 86967128.0,
      "learning_rate": 2.6266666666666667e-05,
      "loss": 3.0438,
      "step": 14240
    },
    {
      "epoch": 1.426,
      "grad_norm": 125366768.0,
      "learning_rate": 2.6233333333333332e-05,
      "loss": 3.1001,
      "step": 14260
    },
    {
      "epoch": 1.428,
      "grad_norm": 44672744.0,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 3.0559,
      "step": 14280
    },
    {
      "epoch": 1.43,
      "grad_norm": 16031542.0,
      "learning_rate": 2.6166666666666668e-05,
      "loss": 2.874,
      "step": 14300
    },
    {
      "epoch": 1.432,
      "grad_norm": 42003308.0,
      "learning_rate": 2.6133333333333333e-05,
      "loss": 2.7755,
      "step": 14320
    },
    {
      "epoch": 1.434,
      "grad_norm": 129368024.0,
      "learning_rate": 2.61e-05,
      "loss": 2.865,
      "step": 14340
    },
    {
      "epoch": 1.436,
      "grad_norm": 120928984.0,
      "learning_rate": 2.6066666666666666e-05,
      "loss": 2.8023,
      "step": 14360
    },
    {
      "epoch": 1.438,
      "grad_norm": 33421432.0,
      "learning_rate": 2.6033333333333337e-05,
      "loss": 2.8675,
      "step": 14380
    },
    {
      "epoch": 1.44,
      "grad_norm": 32158824.0,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 3.0366,
      "step": 14400
    },
    {
      "epoch": 1.442,
      "grad_norm": 85887856.0,
      "learning_rate": 2.5966666666666667e-05,
      "loss": 2.9076,
      "step": 14420
    },
    {
      "epoch": 1.444,
      "grad_norm": 228116224.0,
      "learning_rate": 2.5933333333333338e-05,
      "loss": 2.8542,
      "step": 14440
    },
    {
      "epoch": 1.446,
      "grad_norm": 41576744.0,
      "learning_rate": 2.5900000000000003e-05,
      "loss": 2.8554,
      "step": 14460
    },
    {
      "epoch": 1.448,
      "grad_norm": 68693688.0,
      "learning_rate": 2.5866666666666667e-05,
      "loss": 2.8101,
      "step": 14480
    },
    {
      "epoch": 1.45,
      "grad_norm": 32455824.0,
      "learning_rate": 2.5833333333333336e-05,
      "loss": 3.01,
      "step": 14500
    },
    {
      "epoch": 1.452,
      "grad_norm": 6972412.5,
      "learning_rate": 2.58e-05,
      "loss": 2.974,
      "step": 14520
    },
    {
      "epoch": 1.454,
      "grad_norm": 32728168.0,
      "learning_rate": 2.5766666666666665e-05,
      "loss": 2.9223,
      "step": 14540
    },
    {
      "epoch": 1.456,
      "grad_norm": 39721836.0,
      "learning_rate": 2.5733333333333337e-05,
      "loss": 2.8646,
      "step": 14560
    },
    {
      "epoch": 1.458,
      "grad_norm": 12006998.0,
      "learning_rate": 2.57e-05,
      "loss": 2.806,
      "step": 14580
    },
    {
      "epoch": 1.46,
      "grad_norm": 195958416.0,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 2.9336,
      "step": 14600
    },
    {
      "epoch": 1.462,
      "grad_norm": 94053336.0,
      "learning_rate": 2.5633333333333338e-05,
      "loss": 2.9417,
      "step": 14620
    },
    {
      "epoch": 1.464,
      "grad_norm": 57928496.0,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 2.8587,
      "step": 14640
    },
    {
      "epoch": 1.466,
      "grad_norm": 76051544.0,
      "learning_rate": 2.5566666666666667e-05,
      "loss": 2.8974,
      "step": 14660
    },
    {
      "epoch": 1.468,
      "grad_norm": 169458336.0,
      "learning_rate": 2.553333333333334e-05,
      "loss": 2.9797,
      "step": 14680
    },
    {
      "epoch": 1.47,
      "grad_norm": 4532240.0,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 2.9813,
      "step": 14700
    },
    {
      "epoch": 1.472,
      "grad_norm": 84941088.0,
      "learning_rate": 2.5466666666666668e-05,
      "loss": 2.892,
      "step": 14720
    },
    {
      "epoch": 1.474,
      "grad_norm": 79648736.0,
      "learning_rate": 2.5433333333333336e-05,
      "loss": 3.2167,
      "step": 14740
    },
    {
      "epoch": 1.476,
      "grad_norm": 63879664.0,
      "learning_rate": 2.54e-05,
      "loss": 3.1812,
      "step": 14760
    },
    {
      "epoch": 1.478,
      "grad_norm": 74156056.0,
      "learning_rate": 2.5366666666666665e-05,
      "loss": 3.0268,
      "step": 14780
    },
    {
      "epoch": 1.48,
      "grad_norm": 3946907.0,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 3.0721,
      "step": 14800
    },
    {
      "epoch": 1.482,
      "grad_norm": 148142592.0,
      "learning_rate": 2.5300000000000002e-05,
      "loss": 2.8951,
      "step": 14820
    },
    {
      "epoch": 1.484,
      "grad_norm": 28304226.0,
      "learning_rate": 2.5266666666666666e-05,
      "loss": 2.9238,
      "step": 14840
    },
    {
      "epoch": 1.486,
      "grad_norm": 29276290.0,
      "learning_rate": 2.5233333333333338e-05,
      "loss": 2.8349,
      "step": 14860
    },
    {
      "epoch": 1.488,
      "grad_norm": 22399756.0,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 2.9034,
      "step": 14880
    },
    {
      "epoch": 1.49,
      "grad_norm": 3963371.5,
      "learning_rate": 2.5166666666666667e-05,
      "loss": 2.8042,
      "step": 14900
    },
    {
      "epoch": 1.492,
      "grad_norm": 10982361.0,
      "learning_rate": 2.5133333333333336e-05,
      "loss": 2.8156,
      "step": 14920
    },
    {
      "epoch": 1.494,
      "grad_norm": 20346708.0,
      "learning_rate": 2.51e-05,
      "loss": 2.8601,
      "step": 14940
    },
    {
      "epoch": 1.496,
      "grad_norm": 16298269.0,
      "learning_rate": 2.5066666666666665e-05,
      "loss": 2.8516,
      "step": 14960
    },
    {
      "epoch": 1.498,
      "grad_norm": 2461510.75,
      "learning_rate": 2.5033333333333336e-05,
      "loss": 3.0487,
      "step": 14980
    },
    {
      "epoch": 1.5,
      "grad_norm": 1127178.875,
      "learning_rate": 2.5e-05,
      "loss": 3.3121,
      "step": 15000
    },
    {
      "epoch": 1.502,
      "grad_norm": 606665.3125,
      "learning_rate": 2.496666666666667e-05,
      "loss": 3.25,
      "step": 15020
    },
    {
      "epoch": 1.504,
      "grad_norm": 608814.6875,
      "learning_rate": 2.4933333333333334e-05,
      "loss": 3.074,
      "step": 15040
    },
    {
      "epoch": 1.506,
      "grad_norm": 3771485.75,
      "learning_rate": 2.4900000000000002e-05,
      "loss": 3.0977,
      "step": 15060
    },
    {
      "epoch": 1.508,
      "grad_norm": 25458924.0,
      "learning_rate": 2.486666666666667e-05,
      "loss": 3.1063,
      "step": 15080
    },
    {
      "epoch": 1.51,
      "grad_norm": 2573243.25,
      "learning_rate": 2.4833333333333335e-05,
      "loss": 3.174,
      "step": 15100
    },
    {
      "epoch": 1.512,
      "grad_norm": 3015994.25,
      "learning_rate": 2.48e-05,
      "loss": 3.2607,
      "step": 15120
    },
    {
      "epoch": 1.514,
      "grad_norm": 44269604.0,
      "learning_rate": 2.4766666666666668e-05,
      "loss": 3.2607,
      "step": 15140
    },
    {
      "epoch": 1.516,
      "grad_norm": 13099623.0,
      "learning_rate": 2.4733333333333333e-05,
      "loss": 3.3006,
      "step": 15160
    },
    {
      "epoch": 1.518,
      "grad_norm": 30262488.0,
      "learning_rate": 2.47e-05,
      "loss": 3.3591,
      "step": 15180
    },
    {
      "epoch": 1.52,
      "grad_norm": 21057810432.0,
      "learning_rate": 2.466666666666667e-05,
      "loss": 3.6149,
      "step": 15200
    },
    {
      "epoch": 1.522,
      "grad_norm": 312128000.0,
      "learning_rate": 2.4633333333333334e-05,
      "loss": 6.2799,
      "step": 15220
    },
    {
      "epoch": 1.524,
      "grad_norm": 71062616.0,
      "learning_rate": 2.46e-05,
      "loss": 7.4648,
      "step": 15240
    },
    {
      "epoch": 1.526,
      "grad_norm": 1462278272.0,
      "learning_rate": 2.456666666666667e-05,
      "loss": 6.5182,
      "step": 15260
    },
    {
      "epoch": 1.528,
      "grad_norm": 635497024.0,
      "learning_rate": 2.4533333333333334e-05,
      "loss": 6.7989,
      "step": 15280
    },
    {
      "epoch": 1.53,
      "grad_norm": 291237056.0,
      "learning_rate": 2.45e-05,
      "loss": 7.2129,
      "step": 15300
    },
    {
      "epoch": 1.532,
      "grad_norm": 8141736448.0,
      "learning_rate": 2.4466666666666667e-05,
      "loss": 7.7348,
      "step": 15320
    },
    {
      "epoch": 1.534,
      "grad_norm": 4316376064.0,
      "learning_rate": 2.4433333333333335e-05,
      "loss": 7.589,
      "step": 15340
    },
    {
      "epoch": 1.536,
      "grad_norm": 1113966.75,
      "learning_rate": 2.44e-05,
      "loss": 7.841,
      "step": 15360
    },
    {
      "epoch": 1.538,
      "grad_norm": 35837665280.0,
      "learning_rate": 2.4366666666666668e-05,
      "loss": 8.1789,
      "step": 15380
    },
    {
      "epoch": 1.54,
      "grad_norm": 7175515136.0,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 8.9611,
      "step": 15400
    },
    {
      "epoch": 1.542,
      "grad_norm": 11083471872.0,
      "learning_rate": 2.43e-05,
      "loss": 9.1566,
      "step": 15420
    },
    {
      "epoch": 1.544,
      "grad_norm": 4037112576.0,
      "learning_rate": 2.426666666666667e-05,
      "loss": 10.0425,
      "step": 15440
    },
    {
      "epoch": 1.546,
      "grad_norm": 67851040.0,
      "learning_rate": 2.4233333333333337e-05,
      "loss": 10.5541,
      "step": 15460
    },
    {
      "epoch": 1.548,
      "grad_norm": 26404155392.0,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 10.2371,
      "step": 15480
    },
    {
      "epoch": 1.55,
      "grad_norm": 1100190318592.0,
      "learning_rate": 2.4166666666666667e-05,
      "loss": 8.3229,
      "step": 15500
    },
    {
      "epoch": 1.552,
      "grad_norm": 15773585.0,
      "learning_rate": 2.4133333333333335e-05,
      "loss": 8.1156,
      "step": 15520
    },
    {
      "epoch": 1.554,
      "grad_norm": 2384915456.0,
      "learning_rate": 2.41e-05,
      "loss": 9.2958,
      "step": 15540
    },
    {
      "epoch": 1.556,
      "grad_norm": 1753872384.0,
      "learning_rate": 2.4066666666666668e-05,
      "loss": 7.798,
      "step": 15560
    },
    {
      "epoch": 1.558,
      "grad_norm": 718755.0625,
      "learning_rate": 2.4033333333333336e-05,
      "loss": 7.0343,
      "step": 15580
    },
    {
      "epoch": 1.56,
      "grad_norm": 200709.03125,
      "learning_rate": 2.4e-05,
      "loss": 8.6657,
      "step": 15600
    },
    {
      "epoch": 1.562,
      "grad_norm": 177324912.0,
      "learning_rate": 2.396666666666667e-05,
      "loss": 8.097,
      "step": 15620
    },
    {
      "epoch": 1.564,
      "grad_norm": 2051433216.0,
      "learning_rate": 2.3933333333333337e-05,
      "loss": 8.8225,
      "step": 15640
    },
    {
      "epoch": 1.5659999999999998,
      "grad_norm": 3918763.5,
      "learning_rate": 2.39e-05,
      "loss": 9.2854,
      "step": 15660
    },
    {
      "epoch": 1.568,
      "grad_norm": 5547615.0,
      "learning_rate": 2.3866666666666666e-05,
      "loss": 9.8951,
      "step": 15680
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 180502.40625,
      "learning_rate": 2.3833333333333334e-05,
      "loss": 7.6187,
      "step": 15700
    },
    {
      "epoch": 1.572,
      "grad_norm": 23735048192.0,
      "learning_rate": 2.38e-05,
      "loss": 8.8792,
      "step": 15720
    },
    {
      "epoch": 1.5739999999999998,
      "grad_norm": 312476032.0,
      "learning_rate": 2.3766666666666667e-05,
      "loss": 9.2046,
      "step": 15740
    },
    {
      "epoch": 1.576,
      "grad_norm": 51765520.0,
      "learning_rate": 2.3733333333333335e-05,
      "loss": 9.1057,
      "step": 15760
    },
    {
      "epoch": 1.5779999999999998,
      "grad_norm": 799777.75,
      "learning_rate": 2.37e-05,
      "loss": 8.9594,
      "step": 15780
    },
    {
      "epoch": 1.58,
      "grad_norm": 302714144.0,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 8.6382,
      "step": 15800
    },
    {
      "epoch": 1.5819999999999999,
      "grad_norm": 225361056.0,
      "learning_rate": 2.3633333333333336e-05,
      "loss": 8.3264,
      "step": 15820
    },
    {
      "epoch": 1.584,
      "grad_norm": 3382143.5,
      "learning_rate": 2.36e-05,
      "loss": 8.033,
      "step": 15840
    },
    {
      "epoch": 1.5859999999999999,
      "grad_norm": 24529268736.0,
      "learning_rate": 2.3566666666666666e-05,
      "loss": 7.6322,
      "step": 15860
    },
    {
      "epoch": 1.588,
      "grad_norm": 6475881984.0,
      "learning_rate": 2.3533333333333334e-05,
      "loss": 7.7581,
      "step": 15880
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 759062272.0,
      "learning_rate": 2.35e-05,
      "loss": 7.4839,
      "step": 15900
    },
    {
      "epoch": 1.592,
      "grad_norm": 7628055040.0,
      "learning_rate": 2.3466666666666667e-05,
      "loss": 7.4106,
      "step": 15920
    },
    {
      "epoch": 1.5939999999999999,
      "grad_norm": 9829865472.0,
      "learning_rate": 2.3433333333333335e-05,
      "loss": 7.6296,
      "step": 15940
    },
    {
      "epoch": 1.596,
      "grad_norm": 66203692.0,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 7.5683,
      "step": 15960
    },
    {
      "epoch": 1.5979999999999999,
      "grad_norm": 1874300032.0,
      "learning_rate": 2.3366666666666668e-05,
      "loss": 7.5817,
      "step": 15980
    },
    {
      "epoch": 1.6,
      "grad_norm": 51907456.0,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 7.2511,
      "step": 16000
    },
    {
      "epoch": 1.6019999999999999,
      "grad_norm": 755750720.0,
      "learning_rate": 2.3300000000000004e-05,
      "loss": 7.4536,
      "step": 16020
    },
    {
      "epoch": 1.604,
      "grad_norm": 1959303040.0,
      "learning_rate": 2.326666666666667e-05,
      "loss": 7.4577,
      "step": 16040
    },
    {
      "epoch": 1.6059999999999999,
      "grad_norm": 1026116096.0,
      "learning_rate": 2.3233333333333333e-05,
      "loss": 7.3006,
      "step": 16060
    },
    {
      "epoch": 1.608,
      "grad_norm": 2422273536.0,
      "learning_rate": 2.32e-05,
      "loss": 7.2754,
      "step": 16080
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 673253824.0,
      "learning_rate": 2.3166666666666666e-05,
      "loss": 7.224,
      "step": 16100
    },
    {
      "epoch": 1.612,
      "grad_norm": 12043130880.0,
      "learning_rate": 2.3133333333333334e-05,
      "loss": 7.3985,
      "step": 16120
    },
    {
      "epoch": 1.6139999999999999,
      "grad_norm": 98559664.0,
      "learning_rate": 2.3100000000000002e-05,
      "loss": 7.1125,
      "step": 16140
    },
    {
      "epoch": 1.616,
      "grad_norm": 2631997952.0,
      "learning_rate": 2.3066666666666667e-05,
      "loss": 7.0738,
      "step": 16160
    },
    {
      "epoch": 1.6179999999999999,
      "grad_norm": 390586880.0,
      "learning_rate": 2.3033333333333335e-05,
      "loss": 6.9484,
      "step": 16180
    },
    {
      "epoch": 1.62,
      "grad_norm": 454550560.0,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 7.2013,
      "step": 16200
    },
    {
      "epoch": 1.6219999999999999,
      "grad_norm": 864845952.0,
      "learning_rate": 2.2966666666666668e-05,
      "loss": 7.0198,
      "step": 16220
    },
    {
      "epoch": 1.624,
      "grad_norm": 830424704.0,
      "learning_rate": 2.2933333333333333e-05,
      "loss": 7.0009,
      "step": 16240
    },
    {
      "epoch": 1.626,
      "grad_norm": 648701888.0,
      "learning_rate": 2.29e-05,
      "loss": 6.9893,
      "step": 16260
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 676121600.0,
      "learning_rate": 2.2866666666666666e-05,
      "loss": 6.8278,
      "step": 16280
    },
    {
      "epoch": 1.63,
      "grad_norm": 8745519.0,
      "learning_rate": 2.2833333333333334e-05,
      "loss": 6.7801,
      "step": 16300
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 3294137.25,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 6.7934,
      "step": 16320
    },
    {
      "epoch": 1.634,
      "grad_norm": 50074096.0,
      "learning_rate": 2.2766666666666667e-05,
      "loss": 6.7422,
      "step": 16340
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 769532.0,
      "learning_rate": 2.2733333333333335e-05,
      "loss": 6.6075,
      "step": 16360
    },
    {
      "epoch": 1.638,
      "grad_norm": 12864.6826171875,
      "learning_rate": 2.2700000000000003e-05,
      "loss": 6.6554,
      "step": 16380
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 7549.9658203125,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 5.4861,
      "step": 16400
    },
    {
      "epoch": 1.642,
      "grad_norm": 1239.734619140625,
      "learning_rate": 2.2633333333333336e-05,
      "loss": 4.0411,
      "step": 16420
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 704590.125,
      "learning_rate": 2.26e-05,
      "loss": 4.5622,
      "step": 16440
    },
    {
      "epoch": 1.646,
      "grad_norm": 13890444.0,
      "learning_rate": 2.2566666666666665e-05,
      "loss": 5.472,
      "step": 16460
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 246.7532958984375,
      "learning_rate": 2.2533333333333333e-05,
      "loss": 4.64,
      "step": 16480
    },
    {
      "epoch": 1.65,
      "grad_norm": 21.68535614013672,
      "learning_rate": 2.25e-05,
      "loss": 2.5005,
      "step": 16500
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 32.53573989868164,
      "learning_rate": 2.2466666666666666e-05,
      "loss": 2.0036,
      "step": 16520
    },
    {
      "epoch": 1.654,
      "grad_norm": 32.591854095458984,
      "learning_rate": 2.2433333333333334e-05,
      "loss": 1.7162,
      "step": 16540
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 35.68950271606445,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 1.589,
      "step": 16560
    },
    {
      "epoch": 1.658,
      "grad_norm": 39.67201614379883,
      "learning_rate": 2.236666666666667e-05,
      "loss": 1.5152,
      "step": 16580
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 50.4696044921875,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 1.4513,
      "step": 16600
    },
    {
      "epoch": 1.662,
      "grad_norm": 40.520118713378906,
      "learning_rate": 2.23e-05,
      "loss": 1.4793,
      "step": 16620
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 29.123987197875977,
      "learning_rate": 2.2266666666666668e-05,
      "loss": 1.4157,
      "step": 16640
    },
    {
      "epoch": 1.666,
      "grad_norm": 17.10932731628418,
      "learning_rate": 2.2233333333333333e-05,
      "loss": 1.4405,
      "step": 16660
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 15.14992618560791,
      "learning_rate": 2.22e-05,
      "loss": 1.3614,
      "step": 16680
    },
    {
      "epoch": 1.67,
      "grad_norm": 11.425713539123535,
      "learning_rate": 2.216666666666667e-05,
      "loss": 1.3071,
      "step": 16700
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 18.654781341552734,
      "learning_rate": 2.2133333333333334e-05,
      "loss": 1.2635,
      "step": 16720
    },
    {
      "epoch": 1.674,
      "grad_norm": 11.91672134399414,
      "learning_rate": 2.2100000000000002e-05,
      "loss": 1.3047,
      "step": 16740
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 100.02921295166016,
      "learning_rate": 2.206666666666667e-05,
      "loss": 1.3677,
      "step": 16760
    },
    {
      "epoch": 1.678,
      "grad_norm": 18.30748748779297,
      "learning_rate": 2.2033333333333335e-05,
      "loss": 1.3263,
      "step": 16780
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 8.045022010803223,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.2371,
      "step": 16800
    },
    {
      "epoch": 1.682,
      "grad_norm": 16.382047653198242,
      "learning_rate": 2.1966666666666668e-05,
      "loss": 1.1846,
      "step": 16820
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 10.319565773010254,
      "learning_rate": 2.1933333333333332e-05,
      "loss": 1.2579,
      "step": 16840
    },
    {
      "epoch": 1.686,
      "grad_norm": 9.675960540771484,
      "learning_rate": 2.19e-05,
      "loss": 1.1639,
      "step": 16860
    },
    {
      "epoch": 1.688,
      "grad_norm": 14.503273010253906,
      "learning_rate": 2.186666666666667e-05,
      "loss": 1.2231,
      "step": 16880
    },
    {
      "epoch": 1.69,
      "grad_norm": 27.092832565307617,
      "learning_rate": 2.1833333333333333e-05,
      "loss": 1.218,
      "step": 16900
    },
    {
      "epoch": 1.692,
      "grad_norm": 131.1424560546875,
      "learning_rate": 2.18e-05,
      "loss": 1.2701,
      "step": 16920
    },
    {
      "epoch": 1.694,
      "grad_norm": 23.596921920776367,
      "learning_rate": 2.176666666666667e-05,
      "loss": 1.1743,
      "step": 16940
    },
    {
      "epoch": 1.696,
      "grad_norm": 234.94546508789062,
      "learning_rate": 2.1733333333333334e-05,
      "loss": 1.2181,
      "step": 16960
    },
    {
      "epoch": 1.698,
      "grad_norm": 86.4573745727539,
      "learning_rate": 2.1700000000000002e-05,
      "loss": 1.2063,
      "step": 16980
    },
    {
      "epoch": 1.7,
      "grad_norm": 59.94121170043945,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 1.2096,
      "step": 17000
    },
    {
      "epoch": 1.702,
      "grad_norm": 27.10272216796875,
      "learning_rate": 2.1633333333333332e-05,
      "loss": 1.2272,
      "step": 17020
    },
    {
      "epoch": 1.704,
      "grad_norm": 26.73221206665039,
      "learning_rate": 2.16e-05,
      "loss": 1.1147,
      "step": 17040
    },
    {
      "epoch": 1.706,
      "grad_norm": 278.8115234375,
      "learning_rate": 2.1566666666666668e-05,
      "loss": 1.1905,
      "step": 17060
    },
    {
      "epoch": 1.708,
      "grad_norm": 85.376708984375,
      "learning_rate": 2.1533333333333333e-05,
      "loss": 1.1605,
      "step": 17080
    },
    {
      "epoch": 1.71,
      "grad_norm": 280.0211486816406,
      "learning_rate": 2.15e-05,
      "loss": 1.2623,
      "step": 17100
    },
    {
      "epoch": 1.712,
      "grad_norm": 299.5413818359375,
      "learning_rate": 2.146666666666667e-05,
      "loss": 1.2664,
      "step": 17120
    },
    {
      "epoch": 1.714,
      "grad_norm": 27.937150955200195,
      "learning_rate": 2.1433333333333334e-05,
      "loss": 1.1382,
      "step": 17140
    },
    {
      "epoch": 1.716,
      "grad_norm": 330.593017578125,
      "learning_rate": 2.1400000000000002e-05,
      "loss": 1.2445,
      "step": 17160
    },
    {
      "epoch": 1.718,
      "grad_norm": 28.268115997314453,
      "learning_rate": 2.1366666666666667e-05,
      "loss": 1.219,
      "step": 17180
    },
    {
      "epoch": 1.72,
      "grad_norm": 46.53116226196289,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 1.2137,
      "step": 17200
    },
    {
      "epoch": 1.722,
      "grad_norm": 60.98815155029297,
      "learning_rate": 2.13e-05,
      "loss": 1.2618,
      "step": 17220
    },
    {
      "epoch": 1.724,
      "grad_norm": 32.86701965332031,
      "learning_rate": 2.1266666666666667e-05,
      "loss": 1.2434,
      "step": 17240
    },
    {
      "epoch": 1.726,
      "grad_norm": 171.56622314453125,
      "learning_rate": 2.1233333333333336e-05,
      "loss": 1.3116,
      "step": 17260
    },
    {
      "epoch": 1.728,
      "grad_norm": 890.1871948242188,
      "learning_rate": 2.12e-05,
      "loss": 1.2954,
      "step": 17280
    },
    {
      "epoch": 1.73,
      "grad_norm": 38.210941314697266,
      "learning_rate": 2.116666666666667e-05,
      "loss": 1.1641,
      "step": 17300
    },
    {
      "epoch": 1.732,
      "grad_norm": 889.267578125,
      "learning_rate": 2.1133333333333337e-05,
      "loss": 1.2102,
      "step": 17320
    },
    {
      "epoch": 1.734,
      "grad_norm": 32.74985885620117,
      "learning_rate": 2.11e-05,
      "loss": 1.1105,
      "step": 17340
    },
    {
      "epoch": 1.736,
      "grad_norm": 77.81307220458984,
      "learning_rate": 2.106666666666667e-05,
      "loss": 1.1548,
      "step": 17360
    },
    {
      "epoch": 1.738,
      "grad_norm": 46.38954162597656,
      "learning_rate": 2.1033333333333334e-05,
      "loss": 1.2535,
      "step": 17380
    },
    {
      "epoch": 1.74,
      "grad_norm": 109.35469818115234,
      "learning_rate": 2.1e-05,
      "loss": 1.2244,
      "step": 17400
    },
    {
      "epoch": 1.742,
      "grad_norm": 32.609127044677734,
      "learning_rate": 2.0966666666666667e-05,
      "loss": 1.3181,
      "step": 17420
    },
    {
      "epoch": 1.744,
      "grad_norm": 449.99432373046875,
      "learning_rate": 2.0933333333333335e-05,
      "loss": 1.2165,
      "step": 17440
    },
    {
      "epoch": 1.746,
      "grad_norm": 44.991764068603516,
      "learning_rate": 2.09e-05,
      "loss": 1.2366,
      "step": 17460
    },
    {
      "epoch": 1.748,
      "grad_norm": 122.0331802368164,
      "learning_rate": 2.0866666666666668e-05,
      "loss": 1.2473,
      "step": 17480
    },
    {
      "epoch": 1.75,
      "grad_norm": 489.9118957519531,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 1.2449,
      "step": 17500
    },
    {
      "epoch": 1.752,
      "grad_norm": 57.297630310058594,
      "learning_rate": 2.08e-05,
      "loss": 1.248,
      "step": 17520
    },
    {
      "epoch": 1.754,
      "grad_norm": 28.242345809936523,
      "learning_rate": 2.076666666666667e-05,
      "loss": 1.2799,
      "step": 17540
    },
    {
      "epoch": 1.756,
      "grad_norm": 281.0416259765625,
      "learning_rate": 2.0733333333333334e-05,
      "loss": 1.2813,
      "step": 17560
    },
    {
      "epoch": 1.758,
      "grad_norm": 145.66427612304688,
      "learning_rate": 2.07e-05,
      "loss": 1.2859,
      "step": 17580
    },
    {
      "epoch": 1.76,
      "grad_norm": 34.77787399291992,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 1.243,
      "step": 17600
    },
    {
      "epoch": 1.762,
      "grad_norm": 137.36203002929688,
      "learning_rate": 2.0633333333333335e-05,
      "loss": 1.2018,
      "step": 17620
    },
    {
      "epoch": 1.764,
      "grad_norm": 172.24856567382812,
      "learning_rate": 2.06e-05,
      "loss": 1.1136,
      "step": 17640
    },
    {
      "epoch": 1.766,
      "grad_norm": 664.022216796875,
      "learning_rate": 2.0566666666666667e-05,
      "loss": 1.2168,
      "step": 17660
    },
    {
      "epoch": 1.768,
      "grad_norm": 766.5511474609375,
      "learning_rate": 2.0533333333333336e-05,
      "loss": 1.4885,
      "step": 17680
    },
    {
      "epoch": 1.77,
      "grad_norm": 880.3402099609375,
      "learning_rate": 2.05e-05,
      "loss": 1.2748,
      "step": 17700
    },
    {
      "epoch": 1.772,
      "grad_norm": 70.72003173828125,
      "learning_rate": 2.046666666666667e-05,
      "loss": 1.5437,
      "step": 17720
    },
    {
      "epoch": 1.774,
      "grad_norm": 66.18737030029297,
      "learning_rate": 2.0433333333333336e-05,
      "loss": 1.3376,
      "step": 17740
    },
    {
      "epoch": 1.776,
      "grad_norm": 528.4737548828125,
      "learning_rate": 2.04e-05,
      "loss": 1.3571,
      "step": 17760
    },
    {
      "epoch": 1.778,
      "grad_norm": 122.64622497558594,
      "learning_rate": 2.0366666666666666e-05,
      "loss": 1.2766,
      "step": 17780
    },
    {
      "epoch": 1.78,
      "grad_norm": 906.8474731445312,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 1.2672,
      "step": 17800
    },
    {
      "epoch": 1.782,
      "grad_norm": 712.140869140625,
      "learning_rate": 2.0300000000000002e-05,
      "loss": 1.2548,
      "step": 17820
    },
    {
      "epoch": 1.784,
      "grad_norm": 841.242431640625,
      "learning_rate": 2.0266666666666667e-05,
      "loss": 1.2409,
      "step": 17840
    },
    {
      "epoch": 1.786,
      "grad_norm": 73.4737319946289,
      "learning_rate": 2.0233333333333335e-05,
      "loss": 1.3731,
      "step": 17860
    },
    {
      "epoch": 1.788,
      "grad_norm": 527.154296875,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 1.3131,
      "step": 17880
    },
    {
      "epoch": 1.79,
      "grad_norm": 118.30097961425781,
      "learning_rate": 2.0166666666666668e-05,
      "loss": 1.2407,
      "step": 17900
    },
    {
      "epoch": 1.792,
      "grad_norm": 216.41885375976562,
      "learning_rate": 2.0133333333333336e-05,
      "loss": 1.3267,
      "step": 17920
    },
    {
      "epoch": 1.794,
      "grad_norm": 345.0979309082031,
      "learning_rate": 2.01e-05,
      "loss": 1.1926,
      "step": 17940
    },
    {
      "epoch": 1.796,
      "grad_norm": 1222.9505615234375,
      "learning_rate": 2.0066666666666665e-05,
      "loss": 1.3121,
      "step": 17960
    },
    {
      "epoch": 1.798,
      "grad_norm": 88.97775268554688,
      "learning_rate": 2.0033333333333334e-05,
      "loss": 1.273,
      "step": 17980
    },
    {
      "epoch": 1.8,
      "grad_norm": 167.8631591796875,
      "learning_rate": 2e-05,
      "loss": 1.2738,
      "step": 18000
    },
    {
      "epoch": 1.802,
      "grad_norm": 288.953369140625,
      "learning_rate": 1.9966666666666666e-05,
      "loss": 1.4185,
      "step": 18020
    },
    {
      "epoch": 1.804,
      "grad_norm": 718.7320556640625,
      "learning_rate": 1.9933333333333334e-05,
      "loss": 1.2993,
      "step": 18040
    },
    {
      "epoch": 1.806,
      "grad_norm": 118.8982925415039,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 1.3688,
      "step": 18060
    },
    {
      "epoch": 1.808,
      "grad_norm": 1164.8082275390625,
      "learning_rate": 1.9866666666666667e-05,
      "loss": 1.3049,
      "step": 18080
    },
    {
      "epoch": 1.81,
      "grad_norm": 258.7065734863281,
      "learning_rate": 1.9833333333333335e-05,
      "loss": 1.218,
      "step": 18100
    },
    {
      "epoch": 1.812,
      "grad_norm": 65.17852020263672,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 1.2732,
      "step": 18120
    },
    {
      "epoch": 1.814,
      "grad_norm": 1555.1307373046875,
      "learning_rate": 1.9766666666666668e-05,
      "loss": 1.324,
      "step": 18140
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 695.0717163085938,
      "learning_rate": 1.9733333333333333e-05,
      "loss": 1.4252,
      "step": 18160
    },
    {
      "epoch": 1.818,
      "grad_norm": 1202.084228515625,
      "learning_rate": 1.97e-05,
      "loss": 1.3841,
      "step": 18180
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 59.71566390991211,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 1.2159,
      "step": 18200
    },
    {
      "epoch": 1.822,
      "grad_norm": 282.2738037109375,
      "learning_rate": 1.9633333333333334e-05,
      "loss": 1.3055,
      "step": 18220
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 4804.04150390625,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 1.3675,
      "step": 18240
    },
    {
      "epoch": 1.826,
      "grad_norm": 235.66162109375,
      "learning_rate": 1.9566666666666667e-05,
      "loss": 1.2701,
      "step": 18260
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 1712.9512939453125,
      "learning_rate": 1.9533333333333335e-05,
      "loss": 1.3175,
      "step": 18280
    },
    {
      "epoch": 1.83,
      "grad_norm": 778.7507934570312,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 1.3839,
      "step": 18300
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 1614.27880859375,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 1.3298,
      "step": 18320
    },
    {
      "epoch": 1.834,
      "grad_norm": 212.96420288085938,
      "learning_rate": 1.9433333333333332e-05,
      "loss": 1.659,
      "step": 18340
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 2625.16845703125,
      "learning_rate": 1.94e-05,
      "loss": 1.4081,
      "step": 18360
    },
    {
      "epoch": 1.838,
      "grad_norm": 189.0636444091797,
      "learning_rate": 1.9366666666666665e-05,
      "loss": 1.4678,
      "step": 18380
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 95.18768310546875,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 1.3987,
      "step": 18400
    },
    {
      "epoch": 1.842,
      "grad_norm": 5189.974609375,
      "learning_rate": 1.93e-05,
      "loss": 1.4012,
      "step": 18420
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 163.2099609375,
      "learning_rate": 1.926666666666667e-05,
      "loss": 1.4146,
      "step": 18440
    },
    {
      "epoch": 1.846,
      "grad_norm": 396.7876892089844,
      "learning_rate": 1.9233333333333334e-05,
      "loss": 1.365,
      "step": 18460
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 831.0633544921875,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 1.4849,
      "step": 18480
    },
    {
      "epoch": 1.85,
      "grad_norm": 88.59900665283203,
      "learning_rate": 1.9166666666666667e-05,
      "loss": 1.3516,
      "step": 18500
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 271.4938659667969,
      "learning_rate": 1.9133333333333332e-05,
      "loss": 1.4279,
      "step": 18520
    },
    {
      "epoch": 1.854,
      "grad_norm": 352.4703674316406,
      "learning_rate": 1.91e-05,
      "loss": 1.3453,
      "step": 18540
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 1927.8236083984375,
      "learning_rate": 1.9066666666666668e-05,
      "loss": 1.393,
      "step": 18560
    },
    {
      "epoch": 1.858,
      "grad_norm": 175.3081817626953,
      "learning_rate": 1.9033333333333333e-05,
      "loss": 1.3791,
      "step": 18580
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 314.0987243652344,
      "learning_rate": 1.9e-05,
      "loss": 1.4106,
      "step": 18600
    },
    {
      "epoch": 1.862,
      "grad_norm": 183.3300018310547,
      "learning_rate": 1.896666666666667e-05,
      "loss": 1.2916,
      "step": 18620
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 262.4710998535156,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 1.4541,
      "step": 18640
    },
    {
      "epoch": 1.866,
      "grad_norm": 855.8900756835938,
      "learning_rate": 1.8900000000000002e-05,
      "loss": 1.3855,
      "step": 18660
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 1042.357666015625,
      "learning_rate": 1.886666666666667e-05,
      "loss": 1.3552,
      "step": 18680
    },
    {
      "epoch": 1.87,
      "grad_norm": 214.373291015625,
      "learning_rate": 1.8833333333333335e-05,
      "loss": 1.3126,
      "step": 18700
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 1658.3858642578125,
      "learning_rate": 1.88e-05,
      "loss": 1.3806,
      "step": 18720
    },
    {
      "epoch": 1.874,
      "grad_norm": 135.43714904785156,
      "learning_rate": 1.8766666666666668e-05,
      "loss": 1.4251,
      "step": 18740
    },
    {
      "epoch": 1.876,
      "grad_norm": 437.58612060546875,
      "learning_rate": 1.8733333333333332e-05,
      "loss": 1.3976,
      "step": 18760
    },
    {
      "epoch": 1.8780000000000001,
      "grad_norm": 5948.453125,
      "learning_rate": 1.87e-05,
      "loss": 1.3794,
      "step": 18780
    },
    {
      "epoch": 1.88,
      "grad_norm": 550.6681518554688,
      "learning_rate": 1.866666666666667e-05,
      "loss": 1.4437,
      "step": 18800
    },
    {
      "epoch": 1.8820000000000001,
      "grad_norm": 231.3031005859375,
      "learning_rate": 1.8633333333333333e-05,
      "loss": 1.5166,
      "step": 18820
    },
    {
      "epoch": 1.884,
      "grad_norm": 198.7760467529297,
      "learning_rate": 1.86e-05,
      "loss": 1.4735,
      "step": 18840
    },
    {
      "epoch": 1.8860000000000001,
      "grad_norm": 1017.025146484375,
      "learning_rate": 1.856666666666667e-05,
      "loss": 1.4737,
      "step": 18860
    },
    {
      "epoch": 1.888,
      "grad_norm": 2568.46240234375,
      "learning_rate": 1.8533333333333334e-05,
      "loss": 1.501,
      "step": 18880
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 705.8021850585938,
      "learning_rate": 1.85e-05,
      "loss": 1.6127,
      "step": 18900
    },
    {
      "epoch": 1.892,
      "grad_norm": 698.5326538085938,
      "learning_rate": 1.8466666666666667e-05,
      "loss": 1.327,
      "step": 18920
    },
    {
      "epoch": 1.8940000000000001,
      "grad_norm": 658.7694702148438,
      "learning_rate": 1.8433333333333332e-05,
      "loss": 1.5535,
      "step": 18940
    },
    {
      "epoch": 1.896,
      "grad_norm": 6704.95654296875,
      "learning_rate": 1.84e-05,
      "loss": 1.5456,
      "step": 18960
    },
    {
      "epoch": 1.8980000000000001,
      "grad_norm": 245.90989685058594,
      "learning_rate": 1.8366666666666668e-05,
      "loss": 1.3272,
      "step": 18980
    },
    {
      "epoch": 1.9,
      "grad_norm": 1230.77880859375,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 1.3924,
      "step": 19000
    },
    {
      "epoch": 1.9020000000000001,
      "grad_norm": 6506.25732421875,
      "learning_rate": 1.83e-05,
      "loss": 1.4092,
      "step": 19020
    },
    {
      "epoch": 1.904,
      "grad_norm": 207.99642944335938,
      "learning_rate": 1.826666666666667e-05,
      "loss": 1.4342,
      "step": 19040
    },
    {
      "epoch": 1.9060000000000001,
      "grad_norm": 269.0279235839844,
      "learning_rate": 1.8233333333333334e-05,
      "loss": 1.3679,
      "step": 19060
    },
    {
      "epoch": 1.908,
      "grad_norm": 11797.009765625,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 1.4925,
      "step": 19080
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 741.2421875,
      "learning_rate": 1.8166666666666667e-05,
      "loss": 1.4329,
      "step": 19100
    },
    {
      "epoch": 1.912,
      "grad_norm": 1787.5933837890625,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 1.462,
      "step": 19120
    },
    {
      "epoch": 1.9140000000000001,
      "grad_norm": 850.8580322265625,
      "learning_rate": 1.81e-05,
      "loss": 1.4406,
      "step": 19140
    },
    {
      "epoch": 1.916,
      "grad_norm": 386.5813293457031,
      "learning_rate": 1.8066666666666668e-05,
      "loss": 1.5344,
      "step": 19160
    },
    {
      "epoch": 1.9180000000000001,
      "grad_norm": 88.53961181640625,
      "learning_rate": 1.8033333333333336e-05,
      "loss": 1.3606,
      "step": 19180
    },
    {
      "epoch": 1.92,
      "grad_norm": 4267.02392578125,
      "learning_rate": 1.8e-05,
      "loss": 1.4108,
      "step": 19200
    },
    {
      "epoch": 1.9220000000000002,
      "grad_norm": 127.90373229980469,
      "learning_rate": 1.796666666666667e-05,
      "loss": 1.4349,
      "step": 19220
    },
    {
      "epoch": 1.924,
      "grad_norm": 592.335693359375,
      "learning_rate": 1.7933333333333337e-05,
      "loss": 1.4333,
      "step": 19240
    },
    {
      "epoch": 1.9260000000000002,
      "grad_norm": 858.7900390625,
      "learning_rate": 1.79e-05,
      "loss": 1.4859,
      "step": 19260
    },
    {
      "epoch": 1.928,
      "grad_norm": 493.2312316894531,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 1.5305,
      "step": 19280
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 18756.107421875,
      "learning_rate": 1.7833333333333334e-05,
      "loss": 1.5202,
      "step": 19300
    },
    {
      "epoch": 1.932,
      "grad_norm": 363.2593994140625,
      "learning_rate": 1.78e-05,
      "loss": 1.4643,
      "step": 19320
    },
    {
      "epoch": 1.9340000000000002,
      "grad_norm": 852.4552001953125,
      "learning_rate": 1.7766666666666667e-05,
      "loss": 1.6043,
      "step": 19340
    },
    {
      "epoch": 1.936,
      "grad_norm": 619.285400390625,
      "learning_rate": 1.7733333333333335e-05,
      "loss": 1.4534,
      "step": 19360
    },
    {
      "epoch": 1.938,
      "grad_norm": 2181.243896484375,
      "learning_rate": 1.77e-05,
      "loss": 1.3898,
      "step": 19380
    },
    {
      "epoch": 1.94,
      "grad_norm": 251.0579376220703,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 1.4729,
      "step": 19400
    },
    {
      "epoch": 1.942,
      "grad_norm": 4962.5126953125,
      "learning_rate": 1.7633333333333336e-05,
      "loss": 1.445,
      "step": 19420
    },
    {
      "epoch": 1.944,
      "grad_norm": 2942.367431640625,
      "learning_rate": 1.76e-05,
      "loss": 1.3792,
      "step": 19440
    },
    {
      "epoch": 1.946,
      "grad_norm": 528.2632446289062,
      "learning_rate": 1.756666666666667e-05,
      "loss": 1.4808,
      "step": 19460
    },
    {
      "epoch": 1.948,
      "grad_norm": 415.82843017578125,
      "learning_rate": 1.7533333333333334e-05,
      "loss": 1.4207,
      "step": 19480
    },
    {
      "epoch": 1.95,
      "grad_norm": 23135.65625,
      "learning_rate": 1.75e-05,
      "loss": 1.5056,
      "step": 19500
    },
    {
      "epoch": 1.952,
      "grad_norm": 142.1939697265625,
      "learning_rate": 1.7466666666666667e-05,
      "loss": 1.4598,
      "step": 19520
    },
    {
      "epoch": 1.954,
      "grad_norm": 373.7459411621094,
      "learning_rate": 1.7433333333333335e-05,
      "loss": 1.453,
      "step": 19540
    },
    {
      "epoch": 1.956,
      "grad_norm": 310.1610107421875,
      "learning_rate": 1.74e-05,
      "loss": 1.4659,
      "step": 19560
    },
    {
      "epoch": 1.958,
      "grad_norm": 206.68878173828125,
      "learning_rate": 1.7366666666666668e-05,
      "loss": 1.4336,
      "step": 19580
    },
    {
      "epoch": 1.96,
      "grad_norm": 498.8172912597656,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 1.3504,
      "step": 19600
    },
    {
      "epoch": 1.962,
      "grad_norm": 241.4607696533203,
      "learning_rate": 1.73e-05,
      "loss": 1.4878,
      "step": 19620
    },
    {
      "epoch": 1.964,
      "grad_norm": 650.5484008789062,
      "learning_rate": 1.726666666666667e-05,
      "loss": 1.2973,
      "step": 19640
    },
    {
      "epoch": 1.966,
      "grad_norm": 2476.230224609375,
      "learning_rate": 1.7233333333333333e-05,
      "loss": 1.501,
      "step": 19660
    },
    {
      "epoch": 1.968,
      "grad_norm": 319.1070556640625,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 1.5567,
      "step": 19680
    },
    {
      "epoch": 1.97,
      "grad_norm": 191.2964630126953,
      "learning_rate": 1.7166666666666666e-05,
      "loss": 1.5567,
      "step": 19700
    },
    {
      "epoch": 1.972,
      "grad_norm": 459.5495300292969,
      "learning_rate": 1.7133333333333334e-05,
      "loss": 1.4246,
      "step": 19720
    },
    {
      "epoch": 1.974,
      "grad_norm": 108.12608337402344,
      "learning_rate": 1.7100000000000002e-05,
      "loss": 1.396,
      "step": 19740
    },
    {
      "epoch": 1.976,
      "grad_norm": 698.1387939453125,
      "learning_rate": 1.7066666666666667e-05,
      "loss": 1.4575,
      "step": 19760
    },
    {
      "epoch": 1.978,
      "grad_norm": 531.2373046875,
      "learning_rate": 1.7033333333333335e-05,
      "loss": 1.4765,
      "step": 19780
    },
    {
      "epoch": 1.98,
      "grad_norm": 208.4431915283203,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 1.4919,
      "step": 19800
    },
    {
      "epoch": 1.982,
      "grad_norm": 2560.400146484375,
      "learning_rate": 1.6966666666666668e-05,
      "loss": 1.4655,
      "step": 19820
    },
    {
      "epoch": 1.984,
      "grad_norm": 632.1144409179688,
      "learning_rate": 1.6933333333333333e-05,
      "loss": 1.3834,
      "step": 19840
    },
    {
      "epoch": 1.986,
      "grad_norm": 841.896728515625,
      "learning_rate": 1.69e-05,
      "loss": 1.5582,
      "step": 19860
    },
    {
      "epoch": 1.988,
      "grad_norm": 623.3063354492188,
      "learning_rate": 1.6866666666666666e-05,
      "loss": 1.573,
      "step": 19880
    },
    {
      "epoch": 1.99,
      "grad_norm": 494.42193603515625,
      "learning_rate": 1.6833333333333334e-05,
      "loss": 1.3855,
      "step": 19900
    },
    {
      "epoch": 1.992,
      "grad_norm": 367.8184814453125,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 1.4665,
      "step": 19920
    },
    {
      "epoch": 1.994,
      "grad_norm": 517.778076171875,
      "learning_rate": 1.6766666666666667e-05,
      "loss": 1.538,
      "step": 19940
    },
    {
      "epoch": 1.996,
      "grad_norm": 997.5643920898438,
      "learning_rate": 1.6733333333333335e-05,
      "loss": 1.608,
      "step": 19960
    },
    {
      "epoch": 1.998,
      "grad_norm": 2638.472412109375,
      "learning_rate": 1.6700000000000003e-05,
      "loss": 1.3514,
      "step": 19980
    },
    {
      "epoch": 2.0,
      "grad_norm": 595.8427734375,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 1.434,
      "step": 20000
    },
    {
      "epoch": 2.002,
      "grad_norm": 9813.73046875,
      "learning_rate": 1.6633333333333336e-05,
      "loss": 1.398,
      "step": 20020
    },
    {
      "epoch": 2.004,
      "grad_norm": 2318.6611328125,
      "learning_rate": 1.66e-05,
      "loss": 1.3558,
      "step": 20040
    },
    {
      "epoch": 2.006,
      "grad_norm": 902.0985107421875,
      "learning_rate": 1.6566666666666665e-05,
      "loss": 1.5007,
      "step": 20060
    },
    {
      "epoch": 2.008,
      "grad_norm": 902.9249267578125,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 1.3943,
      "step": 20080
    },
    {
      "epoch": 2.01,
      "grad_norm": 621.1014404296875,
      "learning_rate": 1.65e-05,
      "loss": 1.3802,
      "step": 20100
    },
    {
      "epoch": 2.012,
      "grad_norm": 120.91958618164062,
      "learning_rate": 1.6466666666666666e-05,
      "loss": 1.4285,
      "step": 20120
    },
    {
      "epoch": 2.014,
      "grad_norm": 11398.703125,
      "learning_rate": 1.6433333333333334e-05,
      "loss": 1.4556,
      "step": 20140
    },
    {
      "epoch": 2.016,
      "grad_norm": 621.6302490234375,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 1.4457,
      "step": 20160
    },
    {
      "epoch": 2.018,
      "grad_norm": 10868.44921875,
      "learning_rate": 1.6366666666666667e-05,
      "loss": 1.4748,
      "step": 20180
    },
    {
      "epoch": 2.02,
      "grad_norm": 807.2017822265625,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 1.4329,
      "step": 20200
    },
    {
      "epoch": 2.022,
      "grad_norm": 1038.1724853515625,
      "learning_rate": 1.63e-05,
      "loss": 1.4192,
      "step": 20220
    },
    {
      "epoch": 2.024,
      "grad_norm": 86.3229751586914,
      "learning_rate": 1.6266666666666665e-05,
      "loss": 1.4726,
      "step": 20240
    },
    {
      "epoch": 2.026,
      "grad_norm": 8059.830078125,
      "learning_rate": 1.6233333333333333e-05,
      "loss": 1.4555,
      "step": 20260
    },
    {
      "epoch": 2.028,
      "grad_norm": 983.4299926757812,
      "learning_rate": 1.62e-05,
      "loss": 1.4638,
      "step": 20280
    },
    {
      "epoch": 2.03,
      "grad_norm": 541.8125610351562,
      "learning_rate": 1.6166666666666665e-05,
      "loss": 1.4255,
      "step": 20300
    },
    {
      "epoch": 2.032,
      "grad_norm": 533.2998657226562,
      "learning_rate": 1.6133333333333334e-05,
      "loss": 1.5244,
      "step": 20320
    },
    {
      "epoch": 2.034,
      "grad_norm": 266.362060546875,
      "learning_rate": 1.6100000000000002e-05,
      "loss": 1.4513,
      "step": 20340
    },
    {
      "epoch": 2.036,
      "grad_norm": 421.88079833984375,
      "learning_rate": 1.606666666666667e-05,
      "loss": 1.3894,
      "step": 20360
    },
    {
      "epoch": 2.038,
      "grad_norm": 532.5721435546875,
      "learning_rate": 1.6033333333333335e-05,
      "loss": 1.4275,
      "step": 20380
    },
    {
      "epoch": 2.04,
      "grad_norm": 699.323486328125,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.4082,
      "step": 20400
    },
    {
      "epoch": 2.042,
      "grad_norm": 2574.60009765625,
      "learning_rate": 1.5966666666666667e-05,
      "loss": 1.3493,
      "step": 20420
    },
    {
      "epoch": 2.044,
      "grad_norm": 13986.2109375,
      "learning_rate": 1.5933333333333332e-05,
      "loss": 1.5074,
      "step": 20440
    },
    {
      "epoch": 2.046,
      "grad_norm": 4497.66650390625,
      "learning_rate": 1.59e-05,
      "loss": 1.4976,
      "step": 20460
    },
    {
      "epoch": 2.048,
      "grad_norm": 825.44921875,
      "learning_rate": 1.586666666666667e-05,
      "loss": 1.3243,
      "step": 20480
    },
    {
      "epoch": 2.05,
      "grad_norm": 487.18988037109375,
      "learning_rate": 1.5833333333333333e-05,
      "loss": 1.4333,
      "step": 20500
    },
    {
      "epoch": 2.052,
      "grad_norm": 806.6051635742188,
      "learning_rate": 1.58e-05,
      "loss": 1.4086,
      "step": 20520
    },
    {
      "epoch": 2.054,
      "grad_norm": 764.0698852539062,
      "learning_rate": 1.576666666666667e-05,
      "loss": 1.4694,
      "step": 20540
    },
    {
      "epoch": 2.056,
      "grad_norm": 1387.5137939453125,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 1.5462,
      "step": 20560
    },
    {
      "epoch": 2.058,
      "grad_norm": 733.199951171875,
      "learning_rate": 1.5700000000000002e-05,
      "loss": 1.5082,
      "step": 20580
    },
    {
      "epoch": 2.06,
      "grad_norm": 11997.3798828125,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 1.491,
      "step": 20600
    },
    {
      "epoch": 2.062,
      "grad_norm": 5662.76904296875,
      "learning_rate": 1.563333333333333e-05,
      "loss": 1.4941,
      "step": 20620
    },
    {
      "epoch": 2.064,
      "grad_norm": 333.06243896484375,
      "learning_rate": 1.56e-05,
      "loss": 1.4638,
      "step": 20640
    },
    {
      "epoch": 2.066,
      "grad_norm": 2726.095703125,
      "learning_rate": 1.5566666666666668e-05,
      "loss": 1.6699,
      "step": 20660
    },
    {
      "epoch": 2.068,
      "grad_norm": 89.22781372070312,
      "learning_rate": 1.5533333333333333e-05,
      "loss": 1.5673,
      "step": 20680
    },
    {
      "epoch": 2.07,
      "grad_norm": 1623.8294677734375,
      "learning_rate": 1.55e-05,
      "loss": 1.7777,
      "step": 20700
    },
    {
      "epoch": 2.072,
      "grad_norm": 1084.75146484375,
      "learning_rate": 1.546666666666667e-05,
      "loss": 2.4537,
      "step": 20720
    },
    {
      "epoch": 2.074,
      "grad_norm": 2037.9283447265625,
      "learning_rate": 1.5433333333333334e-05,
      "loss": 1.735,
      "step": 20740
    },
    {
      "epoch": 2.076,
      "grad_norm": 58345.76953125,
      "learning_rate": 1.54e-05,
      "loss": 3.1725,
      "step": 20760
    },
    {
      "epoch": 2.078,
      "grad_norm": 23612.60546875,
      "learning_rate": 1.536666666666667e-05,
      "loss": 3.8864,
      "step": 20780
    },
    {
      "epoch": 2.08,
      "grad_norm": 33196.13671875,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 3.8176,
      "step": 20800
    },
    {
      "epoch": 2.082,
      "grad_norm": 23439.548828125,
      "learning_rate": 1.53e-05,
      "loss": 5.1063,
      "step": 20820
    },
    {
      "epoch": 2.084,
      "grad_norm": 22081.8671875,
      "learning_rate": 1.5266666666666667e-05,
      "loss": 4.8388,
      "step": 20840
    },
    {
      "epoch": 2.086,
      "grad_norm": 83865.921875,
      "learning_rate": 1.5233333333333332e-05,
      "loss": 5.4472,
      "step": 20860
    },
    {
      "epoch": 2.088,
      "grad_norm": 43523.94921875,
      "learning_rate": 1.52e-05,
      "loss": 5.0112,
      "step": 20880
    },
    {
      "epoch": 2.09,
      "grad_norm": 15485.6103515625,
      "learning_rate": 1.5166666666666668e-05,
      "loss": 4.471,
      "step": 20900
    },
    {
      "epoch": 2.092,
      "grad_norm": 442891.84375,
      "learning_rate": 1.5133333333333333e-05,
      "loss": 4.9025,
      "step": 20920
    },
    {
      "epoch": 2.094,
      "grad_norm": 74835.7890625,
      "learning_rate": 1.51e-05,
      "loss": 4.8829,
      "step": 20940
    },
    {
      "epoch": 2.096,
      "grad_norm": 1379333.875,
      "learning_rate": 1.5066666666666668e-05,
      "loss": 4.0751,
      "step": 20960
    },
    {
      "epoch": 2.098,
      "grad_norm": 138258.734375,
      "learning_rate": 1.5033333333333336e-05,
      "loss": 3.7345,
      "step": 20980
    },
    {
      "epoch": 2.1,
      "grad_norm": 58168.4765625,
      "learning_rate": 1.5e-05,
      "loss": 3.5399,
      "step": 21000
    },
    {
      "epoch": 2.102,
      "grad_norm": 303432.71875,
      "learning_rate": 1.4966666666666668e-05,
      "loss": 3.4308,
      "step": 21020
    },
    {
      "epoch": 2.104,
      "grad_norm": 156283.546875,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 3.5805,
      "step": 21040
    },
    {
      "epoch": 2.106,
      "grad_norm": 373567.71875,
      "learning_rate": 1.49e-05,
      "loss": 3.6302,
      "step": 21060
    },
    {
      "epoch": 2.108,
      "grad_norm": 140224.859375,
      "learning_rate": 1.4866666666666668e-05,
      "loss": 3.0085,
      "step": 21080
    },
    {
      "epoch": 2.11,
      "grad_norm": 13540.515625,
      "learning_rate": 1.4833333333333336e-05,
      "loss": 2.9093,
      "step": 21100
    },
    {
      "epoch": 2.112,
      "grad_norm": 1238059.125,
      "learning_rate": 1.48e-05,
      "loss": 3.0848,
      "step": 21120
    },
    {
      "epoch": 2.114,
      "grad_norm": 280115.15625,
      "learning_rate": 1.4766666666666667e-05,
      "loss": 3.2441,
      "step": 21140
    },
    {
      "epoch": 2.116,
      "grad_norm": 91352.6640625,
      "learning_rate": 1.4733333333333335e-05,
      "loss": 3.1874,
      "step": 21160
    },
    {
      "epoch": 2.118,
      "grad_norm": 5125.53466796875,
      "learning_rate": 1.47e-05,
      "loss": 2.8795,
      "step": 21180
    },
    {
      "epoch": 2.12,
      "grad_norm": 47587.75,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 3.0735,
      "step": 21200
    },
    {
      "epoch": 2.122,
      "grad_norm": 124651.0,
      "learning_rate": 1.4633333333333334e-05,
      "loss": 2.8391,
      "step": 21220
    },
    {
      "epoch": 2.124,
      "grad_norm": 906.8658447265625,
      "learning_rate": 1.4599999999999999e-05,
      "loss": 2.5454,
      "step": 21240
    },
    {
      "epoch": 2.126,
      "grad_norm": 23222.810546875,
      "learning_rate": 1.4566666666666667e-05,
      "loss": 2.848,
      "step": 21260
    },
    {
      "epoch": 2.128,
      "grad_norm": 28865.375,
      "learning_rate": 1.4533333333333335e-05,
      "loss": 2.7507,
      "step": 21280
    },
    {
      "epoch": 2.13,
      "grad_norm": 111959.6484375,
      "learning_rate": 1.45e-05,
      "loss": 2.9901,
      "step": 21300
    },
    {
      "epoch": 2.132,
      "grad_norm": 33349.55078125,
      "learning_rate": 1.4466666666666667e-05,
      "loss": 2.7115,
      "step": 21320
    },
    {
      "epoch": 2.134,
      "grad_norm": 122648.671875,
      "learning_rate": 1.4433333333333335e-05,
      "loss": 2.7868,
      "step": 21340
    },
    {
      "epoch": 2.136,
      "grad_norm": 25654.654296875,
      "learning_rate": 1.44e-05,
      "loss": 2.5407,
      "step": 21360
    },
    {
      "epoch": 2.138,
      "grad_norm": 155796.125,
      "learning_rate": 1.4366666666666667e-05,
      "loss": 2.6688,
      "step": 21380
    },
    {
      "epoch": 2.14,
      "grad_norm": 301111.71875,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 2.6958,
      "step": 21400
    },
    {
      "epoch": 2.142,
      "grad_norm": 221476.15625,
      "learning_rate": 1.43e-05,
      "loss": 2.78,
      "step": 21420
    },
    {
      "epoch": 2.144,
      "grad_norm": 164673.078125,
      "learning_rate": 1.4266666666666667e-05,
      "loss": 2.2907,
      "step": 21440
    },
    {
      "epoch": 2.146,
      "grad_norm": 13251.30859375,
      "learning_rate": 1.4233333333333335e-05,
      "loss": 2.5396,
      "step": 21460
    },
    {
      "epoch": 2.148,
      "grad_norm": 29030.353515625,
      "learning_rate": 1.42e-05,
      "loss": 2.3567,
      "step": 21480
    },
    {
      "epoch": 2.15,
      "grad_norm": 2200.994384765625,
      "learning_rate": 1.4166666666666668e-05,
      "loss": 2.5696,
      "step": 21500
    },
    {
      "epoch": 2.152,
      "grad_norm": 77107.859375,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 2.7247,
      "step": 21520
    },
    {
      "epoch": 2.154,
      "grad_norm": 20813.875,
      "learning_rate": 1.4099999999999999e-05,
      "loss": 2.5523,
      "step": 21540
    },
    {
      "epoch": 2.156,
      "grad_norm": 14422.1494140625,
      "learning_rate": 1.4066666666666667e-05,
      "loss": 2.5111,
      "step": 21560
    },
    {
      "epoch": 2.158,
      "grad_norm": 31591.384765625,
      "learning_rate": 1.4033333333333335e-05,
      "loss": 2.7549,
      "step": 21580
    },
    {
      "epoch": 2.16,
      "grad_norm": 34619.40625,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 2.2608,
      "step": 21600
    },
    {
      "epoch": 2.162,
      "grad_norm": 55079.953125,
      "learning_rate": 1.3966666666666666e-05,
      "loss": 2.1267,
      "step": 21620
    },
    {
      "epoch": 2.164,
      "grad_norm": 20425.119140625,
      "learning_rate": 1.3933333333333334e-05,
      "loss": 2.3742,
      "step": 21640
    },
    {
      "epoch": 2.166,
      "grad_norm": 66301.625,
      "learning_rate": 1.3900000000000002e-05,
      "loss": 2.1782,
      "step": 21660
    },
    {
      "epoch": 2.168,
      "grad_norm": 102591.015625,
      "learning_rate": 1.3866666666666667e-05,
      "loss": 2.4031,
      "step": 21680
    },
    {
      "epoch": 2.17,
      "grad_norm": 87262.71875,
      "learning_rate": 1.3833333333333334e-05,
      "loss": 2.2357,
      "step": 21700
    },
    {
      "epoch": 2.172,
      "grad_norm": 187167.046875,
      "learning_rate": 1.3800000000000002e-05,
      "loss": 2.0467,
      "step": 21720
    },
    {
      "epoch": 2.174,
      "grad_norm": 173192.296875,
      "learning_rate": 1.3766666666666666e-05,
      "loss": 2.0073,
      "step": 21740
    },
    {
      "epoch": 2.176,
      "grad_norm": 37738.63671875,
      "learning_rate": 1.3733333333333335e-05,
      "loss": 1.9754,
      "step": 21760
    },
    {
      "epoch": 2.178,
      "grad_norm": 61630.92578125,
      "learning_rate": 1.3700000000000001e-05,
      "loss": 2.0478,
      "step": 21780
    },
    {
      "epoch": 2.18,
      "grad_norm": 2452.236083984375,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 2.0773,
      "step": 21800
    },
    {
      "epoch": 2.182,
      "grad_norm": 974.2271118164062,
      "learning_rate": 1.3633333333333334e-05,
      "loss": 1.9009,
      "step": 21820
    },
    {
      "epoch": 2.184,
      "grad_norm": 37736.296875,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 2.2816,
      "step": 21840
    },
    {
      "epoch": 2.186,
      "grad_norm": 8403.93359375,
      "learning_rate": 1.3566666666666667e-05,
      "loss": 1.8384,
      "step": 21860
    },
    {
      "epoch": 2.188,
      "grad_norm": 59080.4296875,
      "learning_rate": 1.3533333333333335e-05,
      "loss": 2.335,
      "step": 21880
    },
    {
      "epoch": 2.19,
      "grad_norm": 12083.056640625,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 2.123,
      "step": 21900
    },
    {
      "epoch": 2.192,
      "grad_norm": 119762.6953125,
      "learning_rate": 1.3466666666666666e-05,
      "loss": 2.0739,
      "step": 21920
    },
    {
      "epoch": 2.194,
      "grad_norm": 4493.4013671875,
      "learning_rate": 1.3433333333333334e-05,
      "loss": 1.897,
      "step": 21940
    },
    {
      "epoch": 2.196,
      "grad_norm": 5531.79931640625,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 1.9741,
      "step": 21960
    },
    {
      "epoch": 2.198,
      "grad_norm": 19817.322265625,
      "learning_rate": 1.3366666666666667e-05,
      "loss": 2.1106,
      "step": 21980
    },
    {
      "epoch": 2.2,
      "grad_norm": 3250.21435546875,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 1.9131,
      "step": 22000
    },
    {
      "epoch": 2.202,
      "grad_norm": 21372.177734375,
      "learning_rate": 1.3300000000000001e-05,
      "loss": 1.9549,
      "step": 22020
    },
    {
      "epoch": 2.204,
      "grad_norm": 118899.796875,
      "learning_rate": 1.3266666666666666e-05,
      "loss": 2.0226,
      "step": 22040
    },
    {
      "epoch": 2.206,
      "grad_norm": 4646.30810546875,
      "learning_rate": 1.3233333333333334e-05,
      "loss": 1.935,
      "step": 22060
    },
    {
      "epoch": 2.208,
      "grad_norm": 13342.0986328125,
      "learning_rate": 1.32e-05,
      "loss": 1.7847,
      "step": 22080
    },
    {
      "epoch": 2.21,
      "grad_norm": 14102.310546875,
      "learning_rate": 1.3166666666666665e-05,
      "loss": 1.9681,
      "step": 22100
    },
    {
      "epoch": 2.212,
      "grad_norm": 5462.90576171875,
      "learning_rate": 1.3133333333333334e-05,
      "loss": 1.8671,
      "step": 22120
    },
    {
      "epoch": 2.214,
      "grad_norm": 89445.5703125,
      "learning_rate": 1.3100000000000002e-05,
      "loss": 1.863,
      "step": 22140
    },
    {
      "epoch": 2.216,
      "grad_norm": 144716.171875,
      "learning_rate": 1.3066666666666666e-05,
      "loss": 1.9824,
      "step": 22160
    },
    {
      "epoch": 2.218,
      "grad_norm": 31395.3359375,
      "learning_rate": 1.3033333333333333e-05,
      "loss": 1.768,
      "step": 22180
    },
    {
      "epoch": 2.22,
      "grad_norm": 6298.03369140625,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.6787,
      "step": 22200
    },
    {
      "epoch": 2.222,
      "grad_norm": 137703.171875,
      "learning_rate": 1.2966666666666669e-05,
      "loss": 1.6426,
      "step": 22220
    },
    {
      "epoch": 2.224,
      "grad_norm": 13986.44921875,
      "learning_rate": 1.2933333333333334e-05,
      "loss": 1.7667,
      "step": 22240
    },
    {
      "epoch": 2.226,
      "grad_norm": 59925.23046875,
      "learning_rate": 1.29e-05,
      "loss": 1.7839,
      "step": 22260
    },
    {
      "epoch": 2.228,
      "grad_norm": 10852.02734375,
      "learning_rate": 1.2866666666666668e-05,
      "loss": 1.7178,
      "step": 22280
    },
    {
      "epoch": 2.23,
      "grad_norm": 12271.58984375,
      "learning_rate": 1.2833333333333333e-05,
      "loss": 1.8097,
      "step": 22300
    },
    {
      "epoch": 2.232,
      "grad_norm": 2614.316162109375,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 1.7037,
      "step": 22320
    },
    {
      "epoch": 2.234,
      "grad_norm": 189.3728790283203,
      "learning_rate": 1.276666666666667e-05,
      "loss": 1.8473,
      "step": 22340
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 125121.4375,
      "learning_rate": 1.2733333333333334e-05,
      "loss": 1.8717,
      "step": 22360
    },
    {
      "epoch": 2.238,
      "grad_norm": 12344.7685546875,
      "learning_rate": 1.27e-05,
      "loss": 1.7261,
      "step": 22380
    },
    {
      "epoch": 2.24,
      "grad_norm": 5825.4609375,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 1.7332,
      "step": 22400
    },
    {
      "epoch": 2.242,
      "grad_norm": 444985.59375,
      "learning_rate": 1.2633333333333333e-05,
      "loss": 1.6066,
      "step": 22420
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 2406.4951171875,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 1.6905,
      "step": 22440
    },
    {
      "epoch": 2.246,
      "grad_norm": 1397.1851806640625,
      "learning_rate": 1.2566666666666668e-05,
      "loss": 1.6937,
      "step": 22460
    },
    {
      "epoch": 2.248,
      "grad_norm": 11773.853515625,
      "learning_rate": 1.2533333333333332e-05,
      "loss": 1.7141,
      "step": 22480
    },
    {
      "epoch": 2.25,
      "grad_norm": 66881.46875,
      "learning_rate": 1.25e-05,
      "loss": 1.7044,
      "step": 22500
    },
    {
      "epoch": 2.252,
      "grad_norm": 4711.4765625,
      "learning_rate": 1.2466666666666667e-05,
      "loss": 1.5677,
      "step": 22520
    },
    {
      "epoch": 2.254,
      "grad_norm": 11339.681640625,
      "learning_rate": 1.2433333333333335e-05,
      "loss": 1.6581,
      "step": 22540
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 17353.150390625,
      "learning_rate": 1.24e-05,
      "loss": 1.6292,
      "step": 22560
    },
    {
      "epoch": 2.258,
      "grad_norm": 84783.609375,
      "learning_rate": 1.2366666666666666e-05,
      "loss": 1.6134,
      "step": 22580
    },
    {
      "epoch": 2.26,
      "grad_norm": 27611.955078125,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 1.5613,
      "step": 22600
    },
    {
      "epoch": 2.262,
      "grad_norm": 8226.701171875,
      "learning_rate": 1.23e-05,
      "loss": 1.6585,
      "step": 22620
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 6988.1806640625,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 1.607,
      "step": 22640
    },
    {
      "epoch": 2.266,
      "grad_norm": 45802.1640625,
      "learning_rate": 1.2233333333333334e-05,
      "loss": 1.6115,
      "step": 22660
    },
    {
      "epoch": 2.268,
      "grad_norm": 7862.396484375,
      "learning_rate": 1.22e-05,
      "loss": 1.6467,
      "step": 22680
    },
    {
      "epoch": 2.27,
      "grad_norm": 5165.14306640625,
      "learning_rate": 1.2166666666666668e-05,
      "loss": 1.5117,
      "step": 22700
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 12181.4150390625,
      "learning_rate": 1.2133333333333335e-05,
      "loss": 1.5949,
      "step": 22720
    },
    {
      "epoch": 2.274,
      "grad_norm": 4611.97900390625,
      "learning_rate": 1.2100000000000001e-05,
      "loss": 1.5704,
      "step": 22740
    },
    {
      "epoch": 2.276,
      "grad_norm": 1684.5799560546875,
      "learning_rate": 1.2066666666666667e-05,
      "loss": 1.4499,
      "step": 22760
    },
    {
      "epoch": 2.278,
      "grad_norm": 5080.82763671875,
      "learning_rate": 1.2033333333333334e-05,
      "loss": 1.4872,
      "step": 22780
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 4121.0537109375,
      "learning_rate": 1.2e-05,
      "loss": 1.4723,
      "step": 22800
    },
    {
      "epoch": 2.282,
      "grad_norm": 16282.927734375,
      "learning_rate": 1.1966666666666668e-05,
      "loss": 1.595,
      "step": 22820
    },
    {
      "epoch": 2.284,
      "grad_norm": 5547.9326171875,
      "learning_rate": 1.1933333333333333e-05,
      "loss": 1.6103,
      "step": 22840
    },
    {
      "epoch": 2.286,
      "grad_norm": 26573.130859375,
      "learning_rate": 1.19e-05,
      "loss": 1.5427,
      "step": 22860
    },
    {
      "epoch": 2.288,
      "grad_norm": 2353.488525390625,
      "learning_rate": 1.1866666666666668e-05,
      "loss": 1.3787,
      "step": 22880
    },
    {
      "epoch": 2.29,
      "grad_norm": 3528.873291015625,
      "learning_rate": 1.1833333333333334e-05,
      "loss": 1.4347,
      "step": 22900
    },
    {
      "epoch": 2.292,
      "grad_norm": 2682.18408203125,
      "learning_rate": 1.18e-05,
      "loss": 1.5037,
      "step": 22920
    },
    {
      "epoch": 2.294,
      "grad_norm": 2415.387451171875,
      "learning_rate": 1.1766666666666667e-05,
      "loss": 1.3894,
      "step": 22940
    },
    {
      "epoch": 2.296,
      "grad_norm": 422625.03125,
      "learning_rate": 1.1733333333333333e-05,
      "loss": 1.4047,
      "step": 22960
    },
    {
      "epoch": 2.298,
      "grad_norm": 2100.211669921875,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 1.4454,
      "step": 22980
    },
    {
      "epoch": 2.3,
      "grad_norm": 3323.014404296875,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 1.3862,
      "step": 23000
    },
    {
      "epoch": 2.302,
      "grad_norm": 5170.283203125,
      "learning_rate": 1.1633333333333334e-05,
      "loss": 1.3787,
      "step": 23020
    },
    {
      "epoch": 2.304,
      "grad_norm": 940314.25,
      "learning_rate": 1.16e-05,
      "loss": 1.4505,
      "step": 23040
    },
    {
      "epoch": 2.306,
      "grad_norm": 21021.3828125,
      "learning_rate": 1.1566666666666667e-05,
      "loss": 1.3426,
      "step": 23060
    },
    {
      "epoch": 2.308,
      "grad_norm": 3219.49853515625,
      "learning_rate": 1.1533333333333334e-05,
      "loss": 1.4875,
      "step": 23080
    },
    {
      "epoch": 2.31,
      "grad_norm": 23889.927734375,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 1.3666,
      "step": 23100
    },
    {
      "epoch": 2.312,
      "grad_norm": 2454.898681640625,
      "learning_rate": 1.1466666666666666e-05,
      "loss": 1.4118,
      "step": 23120
    },
    {
      "epoch": 2.314,
      "grad_norm": 19312.509765625,
      "learning_rate": 1.1433333333333333e-05,
      "loss": 1.3148,
      "step": 23140
    },
    {
      "epoch": 2.316,
      "grad_norm": 2689.38232421875,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 1.3848,
      "step": 23160
    },
    {
      "epoch": 2.318,
      "grad_norm": 2871.2119140625,
      "learning_rate": 1.1366666666666667e-05,
      "loss": 1.3074,
      "step": 23180
    },
    {
      "epoch": 2.32,
      "grad_norm": 3304.813232421875,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 1.3444,
      "step": 23200
    },
    {
      "epoch": 2.322,
      "grad_norm": 2855.016357421875,
      "learning_rate": 1.13e-05,
      "loss": 1.4387,
      "step": 23220
    },
    {
      "epoch": 2.324,
      "grad_norm": 6921042.5,
      "learning_rate": 1.1266666666666667e-05,
      "loss": 1.3096,
      "step": 23240
    },
    {
      "epoch": 2.326,
      "grad_norm": 762.0936889648438,
      "learning_rate": 1.1233333333333333e-05,
      "loss": 1.3395,
      "step": 23260
    },
    {
      "epoch": 2.328,
      "grad_norm": 1498208.125,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 1.3389,
      "step": 23280
    },
    {
      "epoch": 2.33,
      "grad_norm": 5852.5498046875,
      "learning_rate": 1.1166666666666668e-05,
      "loss": 1.3201,
      "step": 23300
    },
    {
      "epoch": 2.332,
      "grad_norm": 7950.283203125,
      "learning_rate": 1.1133333333333334e-05,
      "loss": 1.4038,
      "step": 23320
    },
    {
      "epoch": 2.334,
      "grad_norm": 1736.927001953125,
      "learning_rate": 1.11e-05,
      "loss": 1.3351,
      "step": 23340
    },
    {
      "epoch": 2.336,
      "grad_norm": 2779.999755859375,
      "learning_rate": 1.1066666666666667e-05,
      "loss": 1.3345,
      "step": 23360
    },
    {
      "epoch": 2.338,
      "grad_norm": 1269.1756591796875,
      "learning_rate": 1.1033333333333335e-05,
      "loss": 1.2899,
      "step": 23380
    },
    {
      "epoch": 2.34,
      "grad_norm": 3272.07568359375,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.2815,
      "step": 23400
    },
    {
      "epoch": 2.342,
      "grad_norm": 1077.37060546875,
      "learning_rate": 1.0966666666666666e-05,
      "loss": 1.288,
      "step": 23420
    },
    {
      "epoch": 2.344,
      "grad_norm": 859.9088745117188,
      "learning_rate": 1.0933333333333334e-05,
      "loss": 1.3193,
      "step": 23440
    },
    {
      "epoch": 2.346,
      "grad_norm": 2457.1552734375,
      "learning_rate": 1.09e-05,
      "loss": 1.2847,
      "step": 23460
    },
    {
      "epoch": 2.348,
      "grad_norm": 3587.368896484375,
      "learning_rate": 1.0866666666666667e-05,
      "loss": 1.3088,
      "step": 23480
    },
    {
      "epoch": 2.35,
      "grad_norm": 5192.86083984375,
      "learning_rate": 1.0833333333333334e-05,
      "loss": 1.2154,
      "step": 23500
    },
    {
      "epoch": 2.352,
      "grad_norm": 1934.273681640625,
      "learning_rate": 1.08e-05,
      "loss": 1.2394,
      "step": 23520
    },
    {
      "epoch": 2.354,
      "grad_norm": 7067.1474609375,
      "learning_rate": 1.0766666666666666e-05,
      "loss": 1.3542,
      "step": 23540
    },
    {
      "epoch": 2.356,
      "grad_norm": 2560.611328125,
      "learning_rate": 1.0733333333333334e-05,
      "loss": 1.2683,
      "step": 23560
    },
    {
      "epoch": 2.358,
      "grad_norm": 10371.9111328125,
      "learning_rate": 1.0700000000000001e-05,
      "loss": 1.2461,
      "step": 23580
    },
    {
      "epoch": 2.36,
      "grad_norm": 5075.6455078125,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 1.3335,
      "step": 23600
    },
    {
      "epoch": 2.362,
      "grad_norm": 1197.43603515625,
      "learning_rate": 1.0633333333333334e-05,
      "loss": 1.2305,
      "step": 23620
    },
    {
      "epoch": 2.364,
      "grad_norm": 4405.71923828125,
      "learning_rate": 1.06e-05,
      "loss": 1.2601,
      "step": 23640
    },
    {
      "epoch": 2.366,
      "grad_norm": 1493.4573974609375,
      "learning_rate": 1.0566666666666668e-05,
      "loss": 1.2501,
      "step": 23660
    },
    {
      "epoch": 2.368,
      "grad_norm": 1672.96875,
      "learning_rate": 1.0533333333333335e-05,
      "loss": 1.2916,
      "step": 23680
    },
    {
      "epoch": 2.37,
      "grad_norm": 1669.0899658203125,
      "learning_rate": 1.05e-05,
      "loss": 1.2299,
      "step": 23700
    },
    {
      "epoch": 2.372,
      "grad_norm": 1946.2762451171875,
      "learning_rate": 1.0466666666666668e-05,
      "loss": 1.1077,
      "step": 23720
    },
    {
      "epoch": 2.374,
      "grad_norm": 1224.379150390625,
      "learning_rate": 1.0433333333333334e-05,
      "loss": 1.2362,
      "step": 23740
    },
    {
      "epoch": 2.376,
      "grad_norm": 1848.1754150390625,
      "learning_rate": 1.04e-05,
      "loss": 1.2622,
      "step": 23760
    },
    {
      "epoch": 2.378,
      "grad_norm": 888.7305908203125,
      "learning_rate": 1.0366666666666667e-05,
      "loss": 1.3171,
      "step": 23780
    },
    {
      "epoch": 2.38,
      "grad_norm": 9284.0986328125,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 1.2455,
      "step": 23800
    },
    {
      "epoch": 2.382,
      "grad_norm": 1076.863037109375,
      "learning_rate": 1.03e-05,
      "loss": 1.3279,
      "step": 23820
    },
    {
      "epoch": 2.384,
      "grad_norm": 1456.5831298828125,
      "learning_rate": 1.0266666666666668e-05,
      "loss": 1.2287,
      "step": 23840
    },
    {
      "epoch": 2.386,
      "grad_norm": 996.1130981445312,
      "learning_rate": 1.0233333333333334e-05,
      "loss": 1.2185,
      "step": 23860
    },
    {
      "epoch": 2.388,
      "grad_norm": 1050.3011474609375,
      "learning_rate": 1.02e-05,
      "loss": 1.1661,
      "step": 23880
    },
    {
      "epoch": 2.39,
      "grad_norm": 488.2942199707031,
      "learning_rate": 1.0166666666666667e-05,
      "loss": 1.1776,
      "step": 23900
    },
    {
      "epoch": 2.392,
      "grad_norm": 1470.0009765625,
      "learning_rate": 1.0133333333333333e-05,
      "loss": 1.2533,
      "step": 23920
    },
    {
      "epoch": 2.394,
      "grad_norm": 1419.144775390625,
      "learning_rate": 1.0100000000000002e-05,
      "loss": 1.2271,
      "step": 23940
    },
    {
      "epoch": 2.396,
      "grad_norm": 1530.9345703125,
      "learning_rate": 1.0066666666666668e-05,
      "loss": 1.3014,
      "step": 23960
    },
    {
      "epoch": 2.398,
      "grad_norm": 1044.189208984375,
      "learning_rate": 1.0033333333333333e-05,
      "loss": 1.204,
      "step": 23980
    },
    {
      "epoch": 2.4,
      "grad_norm": 434.6991882324219,
      "learning_rate": 1e-05,
      "loss": 1.2548,
      "step": 24000
    },
    {
      "epoch": 2.402,
      "grad_norm": 1700.3443603515625,
      "learning_rate": 9.966666666666667e-06,
      "loss": 1.1924,
      "step": 24020
    },
    {
      "epoch": 2.404,
      "grad_norm": 3341.357177734375,
      "learning_rate": 9.933333333333334e-06,
      "loss": 1.2822,
      "step": 24040
    },
    {
      "epoch": 2.406,
      "grad_norm": 890.135009765625,
      "learning_rate": 9.900000000000002e-06,
      "loss": 1.2821,
      "step": 24060
    },
    {
      "epoch": 2.408,
      "grad_norm": 1549.1610107421875,
      "learning_rate": 9.866666666666667e-06,
      "loss": 1.1793,
      "step": 24080
    },
    {
      "epoch": 2.41,
      "grad_norm": 905.4209594726562,
      "learning_rate": 9.833333333333333e-06,
      "loss": 1.2234,
      "step": 24100
    },
    {
      "epoch": 2.412,
      "grad_norm": 1832.18798828125,
      "learning_rate": 9.800000000000001e-06,
      "loss": 1.1691,
      "step": 24120
    },
    {
      "epoch": 2.414,
      "grad_norm": 936.2359008789062,
      "learning_rate": 9.766666666666667e-06,
      "loss": 1.142,
      "step": 24140
    },
    {
      "epoch": 2.416,
      "grad_norm": 2651.02001953125,
      "learning_rate": 9.733333333333334e-06,
      "loss": 1.1365,
      "step": 24160
    },
    {
      "epoch": 2.418,
      "grad_norm": 3613.363037109375,
      "learning_rate": 9.7e-06,
      "loss": 1.2515,
      "step": 24180
    },
    {
      "epoch": 2.42,
      "grad_norm": 1042.13916015625,
      "learning_rate": 9.666666666666667e-06,
      "loss": 1.1972,
      "step": 24200
    },
    {
      "epoch": 2.422,
      "grad_norm": 519.5924072265625,
      "learning_rate": 9.633333333333335e-06,
      "loss": 1.2323,
      "step": 24220
    },
    {
      "epoch": 2.424,
      "grad_norm": 550.30224609375,
      "learning_rate": 9.600000000000001e-06,
      "loss": 1.2447,
      "step": 24240
    },
    {
      "epoch": 2.426,
      "grad_norm": 1589.9420166015625,
      "learning_rate": 9.566666666666666e-06,
      "loss": 1.1458,
      "step": 24260
    },
    {
      "epoch": 2.428,
      "grad_norm": 3516.6630859375,
      "learning_rate": 9.533333333333334e-06,
      "loss": 1.2186,
      "step": 24280
    },
    {
      "epoch": 2.43,
      "grad_norm": 966.2009887695312,
      "learning_rate": 9.5e-06,
      "loss": 1.1552,
      "step": 24300
    },
    {
      "epoch": 2.432,
      "grad_norm": 817.7381591796875,
      "learning_rate": 9.466666666666667e-06,
      "loss": 1.2773,
      "step": 24320
    },
    {
      "epoch": 2.434,
      "grad_norm": 2313.82275390625,
      "learning_rate": 9.433333333333335e-06,
      "loss": 1.2692,
      "step": 24340
    },
    {
      "epoch": 2.436,
      "grad_norm": 505.7396545410156,
      "learning_rate": 9.4e-06,
      "loss": 1.2305,
      "step": 24360
    },
    {
      "epoch": 2.438,
      "grad_norm": 1630.0860595703125,
      "learning_rate": 9.366666666666666e-06,
      "loss": 1.2743,
      "step": 24380
    },
    {
      "epoch": 2.44,
      "grad_norm": 3248.76904296875,
      "learning_rate": 9.333333333333334e-06,
      "loss": 1.256,
      "step": 24400
    },
    {
      "epoch": 2.442,
      "grad_norm": 533.4279174804688,
      "learning_rate": 9.3e-06,
      "loss": 1.1238,
      "step": 24420
    },
    {
      "epoch": 2.444,
      "grad_norm": 1537.059326171875,
      "learning_rate": 9.266666666666667e-06,
      "loss": 1.2119,
      "step": 24440
    },
    {
      "epoch": 2.446,
      "grad_norm": 2027.8311767578125,
      "learning_rate": 9.233333333333334e-06,
      "loss": 1.1575,
      "step": 24460
    },
    {
      "epoch": 2.448,
      "grad_norm": 3025.70361328125,
      "learning_rate": 9.2e-06,
      "loss": 1.1488,
      "step": 24480
    },
    {
      "epoch": 2.45,
      "grad_norm": 1634.86376953125,
      "learning_rate": 9.166666666666666e-06,
      "loss": 1.0907,
      "step": 24500
    },
    {
      "epoch": 2.452,
      "grad_norm": 1549.1219482421875,
      "learning_rate": 9.133333333333335e-06,
      "loss": 1.2646,
      "step": 24520
    },
    {
      "epoch": 2.454,
      "grad_norm": 629.5067749023438,
      "learning_rate": 9.100000000000001e-06,
      "loss": 1.1393,
      "step": 24540
    },
    {
      "epoch": 2.456,
      "grad_norm": 2778.14208984375,
      "learning_rate": 9.066666666666667e-06,
      "loss": 1.1975,
      "step": 24560
    },
    {
      "epoch": 2.458,
      "grad_norm": 1327.270751953125,
      "learning_rate": 9.033333333333334e-06,
      "loss": 1.1873,
      "step": 24580
    },
    {
      "epoch": 2.46,
      "grad_norm": 2156.656494140625,
      "learning_rate": 9e-06,
      "loss": 1.1534,
      "step": 24600
    },
    {
      "epoch": 2.462,
      "grad_norm": 917.2973022460938,
      "learning_rate": 8.966666666666668e-06,
      "loss": 1.078,
      "step": 24620
    },
    {
      "epoch": 2.464,
      "grad_norm": 798.0820922851562,
      "learning_rate": 8.933333333333333e-06,
      "loss": 1.1088,
      "step": 24640
    },
    {
      "epoch": 2.466,
      "grad_norm": 1161.6920166015625,
      "learning_rate": 8.9e-06,
      "loss": 1.1213,
      "step": 24660
    },
    {
      "epoch": 2.468,
      "grad_norm": 286.25750732421875,
      "learning_rate": 8.866666666666668e-06,
      "loss": 1.2596,
      "step": 24680
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 947.1106567382812,
      "learning_rate": 8.833333333333334e-06,
      "loss": 1.2053,
      "step": 24700
    },
    {
      "epoch": 2.472,
      "grad_norm": 1132.353515625,
      "learning_rate": 8.8e-06,
      "loss": 1.0932,
      "step": 24720
    },
    {
      "epoch": 2.474,
      "grad_norm": 1110.742431640625,
      "learning_rate": 8.766666666666667e-06,
      "loss": 1.2043,
      "step": 24740
    },
    {
      "epoch": 2.476,
      "grad_norm": 3417.398681640625,
      "learning_rate": 8.733333333333333e-06,
      "loss": 1.2049,
      "step": 24760
    },
    {
      "epoch": 2.4779999999999998,
      "grad_norm": 3056.623046875,
      "learning_rate": 8.7e-06,
      "loss": 1.1401,
      "step": 24780
    },
    {
      "epoch": 2.48,
      "grad_norm": 501.2319030761719,
      "learning_rate": 8.666666666666668e-06,
      "loss": 1.1416,
      "step": 24800
    },
    {
      "epoch": 2.482,
      "grad_norm": 2085.692138671875,
      "learning_rate": 8.633333333333334e-06,
      "loss": 1.1766,
      "step": 24820
    },
    {
      "epoch": 2.484,
      "grad_norm": 584.5850830078125,
      "learning_rate": 8.599999999999999e-06,
      "loss": 1.234,
      "step": 24840
    },
    {
      "epoch": 2.4859999999999998,
      "grad_norm": 727.2009887695312,
      "learning_rate": 8.566666666666667e-06,
      "loss": 1.1905,
      "step": 24860
    },
    {
      "epoch": 2.488,
      "grad_norm": 4168.93408203125,
      "learning_rate": 8.533333333333334e-06,
      "loss": 1.1277,
      "step": 24880
    },
    {
      "epoch": 2.49,
      "grad_norm": 499.1331481933594,
      "learning_rate": 8.500000000000002e-06,
      "loss": 1.1268,
      "step": 24900
    },
    {
      "epoch": 2.492,
      "grad_norm": 745.141845703125,
      "learning_rate": 8.466666666666666e-06,
      "loss": 1.184,
      "step": 24920
    },
    {
      "epoch": 2.4939999999999998,
      "grad_norm": 2249.562744140625,
      "learning_rate": 8.433333333333333e-06,
      "loss": 1.1439,
      "step": 24940
    },
    {
      "epoch": 2.496,
      "grad_norm": 804.7815551757812,
      "learning_rate": 8.400000000000001e-06,
      "loss": 1.2182,
      "step": 24960
    },
    {
      "epoch": 2.498,
      "grad_norm": 1050.8773193359375,
      "learning_rate": 8.366666666666667e-06,
      "loss": 1.0832,
      "step": 24980
    },
    {
      "epoch": 2.5,
      "grad_norm": 2321.2666015625,
      "learning_rate": 8.333333333333334e-06,
      "loss": 1.1475,
      "step": 25000
    },
    {
      "epoch": 2.502,
      "grad_norm": 333.4263610839844,
      "learning_rate": 8.3e-06,
      "loss": 1.175,
      "step": 25020
    },
    {
      "epoch": 2.504,
      "grad_norm": 699.892578125,
      "learning_rate": 8.266666666666667e-06,
      "loss": 1.2421,
      "step": 25040
    },
    {
      "epoch": 2.5060000000000002,
      "grad_norm": 1497.76123046875,
      "learning_rate": 8.233333333333333e-06,
      "loss": 1.1434,
      "step": 25060
    },
    {
      "epoch": 2.508,
      "grad_norm": 961.0537109375,
      "learning_rate": 8.200000000000001e-06,
      "loss": 1.0911,
      "step": 25080
    },
    {
      "epoch": 2.51,
      "grad_norm": 1675.6275634765625,
      "learning_rate": 8.166666666666668e-06,
      "loss": 1.1336,
      "step": 25100
    },
    {
      "epoch": 2.512,
      "grad_norm": 1425.5303955078125,
      "learning_rate": 8.133333333333332e-06,
      "loss": 1.1238,
      "step": 25120
    },
    {
      "epoch": 2.5140000000000002,
      "grad_norm": 903.9735717773438,
      "learning_rate": 8.1e-06,
      "loss": 1.18,
      "step": 25140
    },
    {
      "epoch": 2.516,
      "grad_norm": 484.7840270996094,
      "learning_rate": 8.066666666666667e-06,
      "loss": 1.1598,
      "step": 25160
    },
    {
      "epoch": 2.518,
      "grad_norm": 1002.13037109375,
      "learning_rate": 8.033333333333335e-06,
      "loss": 1.1425,
      "step": 25180
    },
    {
      "epoch": 2.52,
      "grad_norm": 679.6351928710938,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.1719,
      "step": 25200
    },
    {
      "epoch": 2.5220000000000002,
      "grad_norm": 703.18701171875,
      "learning_rate": 7.966666666666666e-06,
      "loss": 1.2226,
      "step": 25220
    },
    {
      "epoch": 2.524,
      "grad_norm": 8416.5673828125,
      "learning_rate": 7.933333333333334e-06,
      "loss": 1.1446,
      "step": 25240
    },
    {
      "epoch": 2.526,
      "grad_norm": 2986.31640625,
      "learning_rate": 7.9e-06,
      "loss": 1.1034,
      "step": 25260
    },
    {
      "epoch": 2.528,
      "grad_norm": 2676.37841796875,
      "learning_rate": 7.866666666666667e-06,
      "loss": 1.1369,
      "step": 25280
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 2681.953369140625,
      "learning_rate": 7.833333333333333e-06,
      "loss": 1.1792,
      "step": 25300
    },
    {
      "epoch": 2.532,
      "grad_norm": 1267.3751220703125,
      "learning_rate": 7.8e-06,
      "loss": 1.1601,
      "step": 25320
    },
    {
      "epoch": 2.534,
      "grad_norm": 1327.2386474609375,
      "learning_rate": 7.766666666666666e-06,
      "loss": 1.0661,
      "step": 25340
    },
    {
      "epoch": 2.536,
      "grad_norm": 2298.9541015625,
      "learning_rate": 7.733333333333334e-06,
      "loss": 1.1041,
      "step": 25360
    },
    {
      "epoch": 2.5380000000000003,
      "grad_norm": 540.3416748046875,
      "learning_rate": 7.7e-06,
      "loss": 1.1696,
      "step": 25380
    },
    {
      "epoch": 2.54,
      "grad_norm": 1410.569091796875,
      "learning_rate": 7.666666666666667e-06,
      "loss": 1.2525,
      "step": 25400
    },
    {
      "epoch": 2.542,
      "grad_norm": 705.41650390625,
      "learning_rate": 7.633333333333334e-06,
      "loss": 1.1508,
      "step": 25420
    },
    {
      "epoch": 2.544,
      "grad_norm": 1038.3968505859375,
      "learning_rate": 7.6e-06,
      "loss": 1.1619,
      "step": 25440
    },
    {
      "epoch": 2.5460000000000003,
      "grad_norm": 1220.8419189453125,
      "learning_rate": 7.5666666666666665e-06,
      "loss": 1.1734,
      "step": 25460
    },
    {
      "epoch": 2.548,
      "grad_norm": 1284.935791015625,
      "learning_rate": 7.533333333333334e-06,
      "loss": 1.2167,
      "step": 25480
    },
    {
      "epoch": 2.55,
      "grad_norm": 901.4940185546875,
      "learning_rate": 7.5e-06,
      "loss": 1.2386,
      "step": 25500
    },
    {
      "epoch": 2.552,
      "grad_norm": 1021.4574584960938,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 1.1305,
      "step": 25520
    },
    {
      "epoch": 2.5540000000000003,
      "grad_norm": 754.4595336914062,
      "learning_rate": 7.433333333333334e-06,
      "loss": 1.1485,
      "step": 25540
    },
    {
      "epoch": 2.556,
      "grad_norm": 2381.431640625,
      "learning_rate": 7.4e-06,
      "loss": 1.1918,
      "step": 25560
    },
    {
      "epoch": 2.558,
      "grad_norm": 519.1929321289062,
      "learning_rate": 7.3666666666666676e-06,
      "loss": 1.0749,
      "step": 25580
    },
    {
      "epoch": 2.56,
      "grad_norm": 2218.57373046875,
      "learning_rate": 7.333333333333334e-06,
      "loss": 1.1304,
      "step": 25600
    },
    {
      "epoch": 2.5620000000000003,
      "grad_norm": 1130.3065185546875,
      "learning_rate": 7.2999999999999996e-06,
      "loss": 1.1503,
      "step": 25620
    },
    {
      "epoch": 2.564,
      "grad_norm": 804.9378051757812,
      "learning_rate": 7.266666666666668e-06,
      "loss": 1.112,
      "step": 25640
    },
    {
      "epoch": 2.566,
      "grad_norm": 2492.673828125,
      "learning_rate": 7.233333333333333e-06,
      "loss": 1.1343,
      "step": 25660
    },
    {
      "epoch": 2.568,
      "grad_norm": 1474.8441162109375,
      "learning_rate": 7.2e-06,
      "loss": 1.1224,
      "step": 25680
    },
    {
      "epoch": 2.57,
      "grad_norm": 1111.6163330078125,
      "learning_rate": 7.166666666666667e-06,
      "loss": 1.015,
      "step": 25700
    },
    {
      "epoch": 2.572,
      "grad_norm": 2429.624267578125,
      "learning_rate": 7.133333333333333e-06,
      "loss": 1.2168,
      "step": 25720
    },
    {
      "epoch": 2.574,
      "grad_norm": 523.108154296875,
      "learning_rate": 7.1e-06,
      "loss": 1.1083,
      "step": 25740
    },
    {
      "epoch": 2.576,
      "grad_norm": 308.9642028808594,
      "learning_rate": 7.066666666666667e-06,
      "loss": 1.1584,
      "step": 25760
    },
    {
      "epoch": 2.578,
      "grad_norm": 928.828857421875,
      "learning_rate": 7.0333333333333335e-06,
      "loss": 1.178,
      "step": 25780
    },
    {
      "epoch": 2.58,
      "grad_norm": 1729.1363525390625,
      "learning_rate": 7.000000000000001e-06,
      "loss": 1.1168,
      "step": 25800
    },
    {
      "epoch": 2.582,
      "grad_norm": 816.48779296875,
      "learning_rate": 6.966666666666667e-06,
      "loss": 1.1607,
      "step": 25820
    },
    {
      "epoch": 2.584,
      "grad_norm": 1522.0205078125,
      "learning_rate": 6.933333333333334e-06,
      "loss": 1.1501,
      "step": 25840
    },
    {
      "epoch": 2.586,
      "grad_norm": 1334.1796875,
      "learning_rate": 6.900000000000001e-06,
      "loss": 1.1836,
      "step": 25860
    },
    {
      "epoch": 2.588,
      "grad_norm": 601.0672607421875,
      "learning_rate": 6.866666666666667e-06,
      "loss": 1.1273,
      "step": 25880
    },
    {
      "epoch": 2.59,
      "grad_norm": 1105.354736328125,
      "learning_rate": 6.833333333333333e-06,
      "loss": 1.1528,
      "step": 25900
    },
    {
      "epoch": 2.592,
      "grad_norm": 385.7096862792969,
      "learning_rate": 6.800000000000001e-06,
      "loss": 1.1442,
      "step": 25920
    },
    {
      "epoch": 2.594,
      "grad_norm": 809.31494140625,
      "learning_rate": 6.766666666666667e-06,
      "loss": 1.1305,
      "step": 25940
    },
    {
      "epoch": 2.596,
      "grad_norm": 1102.5927734375,
      "learning_rate": 6.733333333333333e-06,
      "loss": 1.2481,
      "step": 25960
    },
    {
      "epoch": 2.598,
      "grad_norm": 509.9324645996094,
      "learning_rate": 6.700000000000001e-06,
      "loss": 1.211,
      "step": 25980
    },
    {
      "epoch": 2.6,
      "grad_norm": 650.2620849609375,
      "learning_rate": 6.666666666666667e-06,
      "loss": 1.192,
      "step": 26000
    },
    {
      "epoch": 2.602,
      "grad_norm": 1712.4136962890625,
      "learning_rate": 6.633333333333333e-06,
      "loss": 1.1506,
      "step": 26020
    },
    {
      "epoch": 2.604,
      "grad_norm": 390.35302734375,
      "learning_rate": 6.6e-06,
      "loss": 1.165,
      "step": 26040
    },
    {
      "epoch": 2.606,
      "grad_norm": 1347.922119140625,
      "learning_rate": 6.566666666666667e-06,
      "loss": 1.1597,
      "step": 26060
    },
    {
      "epoch": 2.608,
      "grad_norm": 3069.322509765625,
      "learning_rate": 6.533333333333333e-06,
      "loss": 1.1173,
      "step": 26080
    },
    {
      "epoch": 2.61,
      "grad_norm": 727.1051635742188,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 1.1307,
      "step": 26100
    },
    {
      "epoch": 2.612,
      "grad_norm": 513.666748046875,
      "learning_rate": 6.466666666666667e-06,
      "loss": 1.2352,
      "step": 26120
    },
    {
      "epoch": 2.614,
      "grad_norm": 1687.2518310546875,
      "learning_rate": 6.433333333333334e-06,
      "loss": 1.0633,
      "step": 26140
    },
    {
      "epoch": 2.616,
      "grad_norm": 1544.3497314453125,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 1.1451,
      "step": 26160
    },
    {
      "epoch": 2.618,
      "grad_norm": 603.3240356445312,
      "learning_rate": 6.366666666666667e-06,
      "loss": 1.164,
      "step": 26180
    },
    {
      "epoch": 2.62,
      "grad_norm": 2433.118896484375,
      "learning_rate": 6.333333333333334e-06,
      "loss": 1.2173,
      "step": 26200
    },
    {
      "epoch": 2.622,
      "grad_norm": 806.2594604492188,
      "learning_rate": 6.300000000000001e-06,
      "loss": 1.1975,
      "step": 26220
    },
    {
      "epoch": 2.624,
      "grad_norm": 988.8622436523438,
      "learning_rate": 6.266666666666666e-06,
      "loss": 1.1078,
      "step": 26240
    },
    {
      "epoch": 2.626,
      "grad_norm": 532.99609375,
      "learning_rate": 6.2333333333333335e-06,
      "loss": 1.1393,
      "step": 26260
    },
    {
      "epoch": 2.628,
      "grad_norm": 1352.6983642578125,
      "learning_rate": 6.2e-06,
      "loss": 1.1235,
      "step": 26280
    },
    {
      "epoch": 2.63,
      "grad_norm": 652.5396728515625,
      "learning_rate": 6.166666666666667e-06,
      "loss": 1.1229,
      "step": 26300
    },
    {
      "epoch": 2.632,
      "grad_norm": 1119.1422119140625,
      "learning_rate": 6.133333333333334e-06,
      "loss": 1.0926,
      "step": 26320
    },
    {
      "epoch": 2.634,
      "grad_norm": 1042.480712890625,
      "learning_rate": 6.1e-06,
      "loss": 1.1744,
      "step": 26340
    },
    {
      "epoch": 2.636,
      "grad_norm": 1270.384765625,
      "learning_rate": 6.066666666666667e-06,
      "loss": 1.0827,
      "step": 26360
    },
    {
      "epoch": 2.638,
      "grad_norm": 4480.81591796875,
      "learning_rate": 6.033333333333334e-06,
      "loss": 1.1226,
      "step": 26380
    },
    {
      "epoch": 2.64,
      "grad_norm": 18086926.0,
      "learning_rate": 6e-06,
      "loss": 1.1448,
      "step": 26400
    },
    {
      "epoch": 2.642,
      "grad_norm": 729.82177734375,
      "learning_rate": 5.9666666666666666e-06,
      "loss": 1.1249,
      "step": 26420
    },
    {
      "epoch": 2.644,
      "grad_norm": 482.6578369140625,
      "learning_rate": 5.933333333333334e-06,
      "loss": 1.1464,
      "step": 26440
    },
    {
      "epoch": 2.646,
      "grad_norm": 2745.6943359375,
      "learning_rate": 5.9e-06,
      "loss": 1.1255,
      "step": 26460
    },
    {
      "epoch": 2.648,
      "grad_norm": 1199.656982421875,
      "learning_rate": 5.866666666666667e-06,
      "loss": 1.1617,
      "step": 26480
    },
    {
      "epoch": 2.65,
      "grad_norm": 205.3389129638672,
      "learning_rate": 5.833333333333334e-06,
      "loss": 1.1399,
      "step": 26500
    },
    {
      "epoch": 2.652,
      "grad_norm": 53.54338073730469,
      "learning_rate": 5.8e-06,
      "loss": 1.1987,
      "step": 26520
    },
    {
      "epoch": 2.654,
      "grad_norm": 326.6540222167969,
      "learning_rate": 5.766666666666667e-06,
      "loss": 1.2366,
      "step": 26540
    },
    {
      "epoch": 2.656,
      "grad_norm": 1772.7943115234375,
      "learning_rate": 5.733333333333333e-06,
      "loss": 1.0698,
      "step": 26560
    },
    {
      "epoch": 2.658,
      "grad_norm": 696.6214599609375,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 1.1048,
      "step": 26580
    },
    {
      "epoch": 2.66,
      "grad_norm": 1071.0950927734375,
      "learning_rate": 5.666666666666667e-06,
      "loss": 1.1195,
      "step": 26600
    },
    {
      "epoch": 2.662,
      "grad_norm": 474.5443420410156,
      "learning_rate": 5.633333333333333e-06,
      "loss": 1.2199,
      "step": 26620
    },
    {
      "epoch": 2.664,
      "grad_norm": 1887.746337890625,
      "learning_rate": 5.600000000000001e-06,
      "loss": 1.1254,
      "step": 26640
    },
    {
      "epoch": 2.666,
      "grad_norm": 438.7510681152344,
      "learning_rate": 5.566666666666667e-06,
      "loss": 1.1812,
      "step": 26660
    },
    {
      "epoch": 2.668,
      "grad_norm": 1095.39404296875,
      "learning_rate": 5.5333333333333334e-06,
      "loss": 1.1788,
      "step": 26680
    },
    {
      "epoch": 2.67,
      "grad_norm": 192.66629028320312,
      "learning_rate": 5.500000000000001e-06,
      "loss": 1.1827,
      "step": 26700
    },
    {
      "epoch": 2.672,
      "grad_norm": 1825.5419921875,
      "learning_rate": 5.466666666666667e-06,
      "loss": 1.1371,
      "step": 26720
    },
    {
      "epoch": 2.674,
      "grad_norm": 264.949462890625,
      "learning_rate": 5.4333333333333335e-06,
      "loss": 1.1885,
      "step": 26740
    },
    {
      "epoch": 2.676,
      "grad_norm": 1224.7806396484375,
      "learning_rate": 5.4e-06,
      "loss": 1.1673,
      "step": 26760
    },
    {
      "epoch": 2.678,
      "grad_norm": 635.1727294921875,
      "learning_rate": 5.366666666666667e-06,
      "loss": 1.2231,
      "step": 26780
    },
    {
      "epoch": 2.68,
      "grad_norm": 1223.1900634765625,
      "learning_rate": 5.333333333333334e-06,
      "loss": 1.0684,
      "step": 26800
    },
    {
      "epoch": 2.682,
      "grad_norm": 3650.568603515625,
      "learning_rate": 5.3e-06,
      "loss": 1.0713,
      "step": 26820
    },
    {
      "epoch": 2.684,
      "grad_norm": 435.6389465332031,
      "learning_rate": 5.266666666666667e-06,
      "loss": 1.0945,
      "step": 26840
    },
    {
      "epoch": 2.686,
      "grad_norm": 1412.4954833984375,
      "learning_rate": 5.233333333333334e-06,
      "loss": 1.1453,
      "step": 26860
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 662.2605590820312,
      "learning_rate": 5.2e-06,
      "loss": 1.1292,
      "step": 26880
    },
    {
      "epoch": 2.69,
      "grad_norm": 566.28564453125,
      "learning_rate": 5.166666666666667e-06,
      "loss": 1.2796,
      "step": 26900
    },
    {
      "epoch": 2.692,
      "grad_norm": 282.8934020996094,
      "learning_rate": 5.133333333333334e-06,
      "loss": 1.1255,
      "step": 26920
    },
    {
      "epoch": 2.694,
      "grad_norm": 1408.3662109375,
      "learning_rate": 5.1e-06,
      "loss": 1.1475,
      "step": 26940
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 981.4431762695312,
      "learning_rate": 5.066666666666667e-06,
      "loss": 1.2098,
      "step": 26960
    },
    {
      "epoch": 2.698,
      "grad_norm": 1213.145751953125,
      "learning_rate": 5.033333333333334e-06,
      "loss": 1.2061,
      "step": 26980
    },
    {
      "epoch": 2.7,
      "grad_norm": 1222.633056640625,
      "learning_rate": 5e-06,
      "loss": 1.1679,
      "step": 27000
    },
    {
      "epoch": 2.702,
      "grad_norm": 348.3948059082031,
      "learning_rate": 4.966666666666667e-06,
      "loss": 1.2074,
      "step": 27020
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 284.0867004394531,
      "learning_rate": 4.933333333333333e-06,
      "loss": 1.1556,
      "step": 27040
    },
    {
      "epoch": 2.706,
      "grad_norm": 668.2919311523438,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 1.178,
      "step": 27060
    },
    {
      "epoch": 2.708,
      "grad_norm": 374.5926818847656,
      "learning_rate": 4.866666666666667e-06,
      "loss": 1.1826,
      "step": 27080
    },
    {
      "epoch": 2.71,
      "grad_norm": 2750.524658203125,
      "learning_rate": 4.833333333333333e-06,
      "loss": 1.1676,
      "step": 27100
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 486.2842102050781,
      "learning_rate": 4.800000000000001e-06,
      "loss": 1.1413,
      "step": 27120
    },
    {
      "epoch": 2.714,
      "grad_norm": 405.6146240234375,
      "learning_rate": 4.766666666666667e-06,
      "loss": 1.1616,
      "step": 27140
    },
    {
      "epoch": 2.716,
      "grad_norm": 513.7738037109375,
      "learning_rate": 4.7333333333333335e-06,
      "loss": 1.1925,
      "step": 27160
    },
    {
      "epoch": 2.718,
      "grad_norm": 523.7448120117188,
      "learning_rate": 4.7e-06,
      "loss": 1.2147,
      "step": 27180
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 2051.760986328125,
      "learning_rate": 4.666666666666667e-06,
      "loss": 1.1053,
      "step": 27200
    },
    {
      "epoch": 2.722,
      "grad_norm": 805.367919921875,
      "learning_rate": 4.633333333333334e-06,
      "loss": 1.1613,
      "step": 27220
    },
    {
      "epoch": 2.724,
      "grad_norm": 2554.360595703125,
      "learning_rate": 4.6e-06,
      "loss": 1.1134,
      "step": 27240
    },
    {
      "epoch": 2.726,
      "grad_norm": 515.102294921875,
      "learning_rate": 4.566666666666667e-06,
      "loss": 1.0947,
      "step": 27260
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 1510.4578857421875,
      "learning_rate": 4.533333333333334e-06,
      "loss": 1.0967,
      "step": 27280
    },
    {
      "epoch": 2.73,
      "grad_norm": 238.00584411621094,
      "learning_rate": 4.5e-06,
      "loss": 1.2466,
      "step": 27300
    },
    {
      "epoch": 2.732,
      "grad_norm": 922.4171142578125,
      "learning_rate": 4.4666666666666665e-06,
      "loss": 1.1483,
      "step": 27320
    },
    {
      "epoch": 2.734,
      "grad_norm": 1202.1807861328125,
      "learning_rate": 4.433333333333334e-06,
      "loss": 1.1545,
      "step": 27340
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 596.5418701171875,
      "learning_rate": 4.4e-06,
      "loss": 1.1305,
      "step": 27360
    },
    {
      "epoch": 2.738,
      "grad_norm": 297.2582092285156,
      "learning_rate": 4.366666666666667e-06,
      "loss": 1.133,
      "step": 27380
    },
    {
      "epoch": 2.74,
      "grad_norm": 280.2720642089844,
      "learning_rate": 4.333333333333334e-06,
      "loss": 1.1737,
      "step": 27400
    },
    {
      "epoch": 2.742,
      "grad_norm": 243.25509643554688,
      "learning_rate": 4.2999999999999995e-06,
      "loss": 1.119,
      "step": 27420
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 118.2464370727539,
      "learning_rate": 4.266666666666667e-06,
      "loss": 1.1735,
      "step": 27440
    },
    {
      "epoch": 2.746,
      "grad_norm": 310.41864013671875,
      "learning_rate": 4.233333333333333e-06,
      "loss": 1.043,
      "step": 27460
    },
    {
      "epoch": 2.748,
      "grad_norm": 691.6290893554688,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 1.1875,
      "step": 27480
    },
    {
      "epoch": 2.75,
      "grad_norm": 635.705810546875,
      "learning_rate": 4.166666666666667e-06,
      "loss": 1.1203,
      "step": 27500
    },
    {
      "epoch": 2.752,
      "grad_norm": 768.6624145507812,
      "learning_rate": 4.133333333333333e-06,
      "loss": 1.1839,
      "step": 27520
    },
    {
      "epoch": 2.754,
      "grad_norm": 486.2532958984375,
      "learning_rate": 4.1000000000000006e-06,
      "loss": 1.1187,
      "step": 27540
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 1683.538330078125,
      "learning_rate": 4.066666666666666e-06,
      "loss": 1.1201,
      "step": 27560
    },
    {
      "epoch": 2.758,
      "grad_norm": 944.666015625,
      "learning_rate": 4.033333333333333e-06,
      "loss": 1.2079,
      "step": 27580
    },
    {
      "epoch": 2.76,
      "grad_norm": 666.9877319335938,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.1515,
      "step": 27600
    },
    {
      "epoch": 2.762,
      "grad_norm": 847.9832153320312,
      "learning_rate": 3.966666666666667e-06,
      "loss": 1.2159,
      "step": 27620
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 821.6002807617188,
      "learning_rate": 3.9333333333333335e-06,
      "loss": 1.2071,
      "step": 27640
    },
    {
      "epoch": 2.766,
      "grad_norm": 1164.1444091796875,
      "learning_rate": 3.9e-06,
      "loss": 1.1588,
      "step": 27660
    },
    {
      "epoch": 2.768,
      "grad_norm": 2043.4156494140625,
      "learning_rate": 3.866666666666667e-06,
      "loss": 1.0882,
      "step": 27680
    },
    {
      "epoch": 2.77,
      "grad_norm": 1404.30517578125,
      "learning_rate": 3.833333333333334e-06,
      "loss": 1.1534,
      "step": 27700
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 1103.95068359375,
      "learning_rate": 3.8e-06,
      "loss": 1.15,
      "step": 27720
    },
    {
      "epoch": 2.774,
      "grad_norm": 2319.060546875,
      "learning_rate": 3.766666666666667e-06,
      "loss": 1.232,
      "step": 27740
    },
    {
      "epoch": 2.776,
      "grad_norm": 3576.00537109375,
      "learning_rate": 3.7333333333333337e-06,
      "loss": 1.1231,
      "step": 27760
    },
    {
      "epoch": 2.778,
      "grad_norm": 41.25361251831055,
      "learning_rate": 3.7e-06,
      "loss": 1.1669,
      "step": 27780
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 132.64279174804688,
      "learning_rate": 3.666666666666667e-06,
      "loss": 1.2409,
      "step": 27800
    },
    {
      "epoch": 2.782,
      "grad_norm": 460.7060546875,
      "learning_rate": 3.633333333333334e-06,
      "loss": 1.1141,
      "step": 27820
    },
    {
      "epoch": 2.784,
      "grad_norm": 346.6821594238281,
      "learning_rate": 3.6e-06,
      "loss": 1.1378,
      "step": 27840
    },
    {
      "epoch": 2.786,
      "grad_norm": 1279.1085205078125,
      "learning_rate": 3.5666666666666667e-06,
      "loss": 1.1929,
      "step": 27860
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 1717.6561279296875,
      "learning_rate": 3.5333333333333335e-06,
      "loss": 1.1427,
      "step": 27880
    },
    {
      "epoch": 2.79,
      "grad_norm": 126.75027465820312,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 1.1718,
      "step": 27900
    },
    {
      "epoch": 2.792,
      "grad_norm": 819.6079711914062,
      "learning_rate": 3.466666666666667e-06,
      "loss": 1.1762,
      "step": 27920
    },
    {
      "epoch": 2.794,
      "grad_norm": 833.9706420898438,
      "learning_rate": 3.4333333333333336e-06,
      "loss": 1.0446,
      "step": 27940
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 1223.464599609375,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 1.1041,
      "step": 27960
    },
    {
      "epoch": 2.798,
      "grad_norm": 1721.025146484375,
      "learning_rate": 3.3666666666666665e-06,
      "loss": 1.2517,
      "step": 27980
    },
    {
      "epoch": 2.8,
      "grad_norm": 394.15838623046875,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 1.0876,
      "step": 28000
    },
    {
      "epoch": 2.802,
      "grad_norm": 2221.77685546875,
      "learning_rate": 3.3e-06,
      "loss": 1.1978,
      "step": 28020
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 385.18505859375,
      "learning_rate": 3.2666666666666666e-06,
      "loss": 1.1549,
      "step": 28040
    },
    {
      "epoch": 2.806,
      "grad_norm": 1490.73291015625,
      "learning_rate": 3.2333333333333334e-06,
      "loss": 1.1554,
      "step": 28060
    },
    {
      "epoch": 2.808,
      "grad_norm": 2048.425537109375,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 1.1171,
      "step": 28080
    },
    {
      "epoch": 2.81,
      "grad_norm": 253.7383270263672,
      "learning_rate": 3.166666666666667e-06,
      "loss": 1.1359,
      "step": 28100
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 2673.585693359375,
      "learning_rate": 3.133333333333333e-06,
      "loss": 1.1139,
      "step": 28120
    },
    {
      "epoch": 2.814,
      "grad_norm": 864.5245971679688,
      "learning_rate": 3.1e-06,
      "loss": 1.1052,
      "step": 28140
    },
    {
      "epoch": 2.816,
      "grad_norm": 193.7861785888672,
      "learning_rate": 3.066666666666667e-06,
      "loss": 1.157,
      "step": 28160
    },
    {
      "epoch": 2.818,
      "grad_norm": 2237.257080078125,
      "learning_rate": 3.0333333333333337e-06,
      "loss": 1.1352,
      "step": 28180
    },
    {
      "epoch": 2.82,
      "grad_norm": 652.4060668945312,
      "learning_rate": 3e-06,
      "loss": 1.1497,
      "step": 28200
    },
    {
      "epoch": 2.822,
      "grad_norm": 881.3284912109375,
      "learning_rate": 2.966666666666667e-06,
      "loss": 1.2301,
      "step": 28220
    },
    {
      "epoch": 2.824,
      "grad_norm": 696.4691772460938,
      "learning_rate": 2.9333333333333333e-06,
      "loss": 1.1213,
      "step": 28240
    },
    {
      "epoch": 2.826,
      "grad_norm": 1493.8609619140625,
      "learning_rate": 2.9e-06,
      "loss": 1.1378,
      "step": 28260
    },
    {
      "epoch": 2.828,
      "grad_norm": 394.5662841796875,
      "learning_rate": 2.8666666666666666e-06,
      "loss": 1.1972,
      "step": 28280
    },
    {
      "epoch": 2.83,
      "grad_norm": 928.1893920898438,
      "learning_rate": 2.8333333333333335e-06,
      "loss": 1.0966,
      "step": 28300
    },
    {
      "epoch": 2.832,
      "grad_norm": 1225.5152587890625,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 1.1106,
      "step": 28320
    },
    {
      "epoch": 2.834,
      "grad_norm": 459.8016357421875,
      "learning_rate": 2.7666666666666667e-06,
      "loss": 1.1021,
      "step": 28340
    },
    {
      "epoch": 2.836,
      "grad_norm": 3011.572021484375,
      "learning_rate": 2.7333333333333336e-06,
      "loss": 1.2077,
      "step": 28360
    },
    {
      "epoch": 2.838,
      "grad_norm": 494.5107421875,
      "learning_rate": 2.7e-06,
      "loss": 1.2175,
      "step": 28380
    },
    {
      "epoch": 2.84,
      "grad_norm": 2433.201171875,
      "learning_rate": 2.666666666666667e-06,
      "loss": 1.1261,
      "step": 28400
    },
    {
      "epoch": 2.842,
      "grad_norm": 1150.734130859375,
      "learning_rate": 2.6333333333333337e-06,
      "loss": 1.0786,
      "step": 28420
    },
    {
      "epoch": 2.844,
      "grad_norm": 237.69015502929688,
      "learning_rate": 2.6e-06,
      "loss": 1.239,
      "step": 28440
    },
    {
      "epoch": 2.846,
      "grad_norm": 609.1223754882812,
      "learning_rate": 2.566666666666667e-06,
      "loss": 1.2707,
      "step": 28460
    },
    {
      "epoch": 2.848,
      "grad_norm": 2636.20361328125,
      "learning_rate": 2.5333333333333334e-06,
      "loss": 1.1014,
      "step": 28480
    },
    {
      "epoch": 2.85,
      "grad_norm": 198.6291046142578,
      "learning_rate": 2.5e-06,
      "loss": 1.1078,
      "step": 28500
    },
    {
      "epoch": 2.852,
      "grad_norm": 1283.5977783203125,
      "learning_rate": 2.4666666666666666e-06,
      "loss": 1.0757,
      "step": 28520
    },
    {
      "epoch": 2.854,
      "grad_norm": 328.0797119140625,
      "learning_rate": 2.4333333333333335e-06,
      "loss": 1.1635,
      "step": 28540
    },
    {
      "epoch": 2.856,
      "grad_norm": 479.72216796875,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 1.1466,
      "step": 28560
    },
    {
      "epoch": 2.858,
      "grad_norm": 1600.9127197265625,
      "learning_rate": 2.3666666666666667e-06,
      "loss": 1.1341,
      "step": 28580
    },
    {
      "epoch": 2.86,
      "grad_norm": 2086.45849609375,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 1.1774,
      "step": 28600
    },
    {
      "epoch": 2.862,
      "grad_norm": 2375.671875,
      "learning_rate": 2.3e-06,
      "loss": 1.1291,
      "step": 28620
    },
    {
      "epoch": 2.864,
      "grad_norm": 495.9068908691406,
      "learning_rate": 2.266666666666667e-06,
      "loss": 1.1622,
      "step": 28640
    },
    {
      "epoch": 2.866,
      "grad_norm": 1256.2520751953125,
      "learning_rate": 2.2333333333333333e-06,
      "loss": 1.1377,
      "step": 28660
    },
    {
      "epoch": 2.868,
      "grad_norm": 3255.149169921875,
      "learning_rate": 2.2e-06,
      "loss": 1.14,
      "step": 28680
    },
    {
      "epoch": 2.87,
      "grad_norm": 362.88031005859375,
      "learning_rate": 2.166666666666667e-06,
      "loss": 1.2216,
      "step": 28700
    },
    {
      "epoch": 2.872,
      "grad_norm": 420.52410888671875,
      "learning_rate": 2.1333333333333334e-06,
      "loss": 1.2153,
      "step": 28720
    },
    {
      "epoch": 2.874,
      "grad_norm": 689.0511474609375,
      "learning_rate": 2.1000000000000002e-06,
      "loss": 1.1929,
      "step": 28740
    },
    {
      "epoch": 2.876,
      "grad_norm": 572.6959228515625,
      "learning_rate": 2.0666666666666666e-06,
      "loss": 1.1,
      "step": 28760
    },
    {
      "epoch": 2.878,
      "grad_norm": 552.08251953125,
      "learning_rate": 2.033333333333333e-06,
      "loss": 1.1513,
      "step": 28780
    },
    {
      "epoch": 2.88,
      "grad_norm": 640.4539184570312,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.1186,
      "step": 28800
    },
    {
      "epoch": 2.882,
      "grad_norm": 1407.6556396484375,
      "learning_rate": 1.9666666666666668e-06,
      "loss": 1.1649,
      "step": 28820
    },
    {
      "epoch": 2.884,
      "grad_norm": 603.9071044921875,
      "learning_rate": 1.9333333333333336e-06,
      "loss": 1.2079,
      "step": 28840
    },
    {
      "epoch": 2.886,
      "grad_norm": 1042.5311279296875,
      "learning_rate": 1.9e-06,
      "loss": 1.1029,
      "step": 28860
    },
    {
      "epoch": 2.888,
      "grad_norm": 1162.208984375,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 1.122,
      "step": 28880
    },
    {
      "epoch": 2.89,
      "grad_norm": 344.19976806640625,
      "learning_rate": 1.8333333333333335e-06,
      "loss": 1.2172,
      "step": 28900
    },
    {
      "epoch": 2.892,
      "grad_norm": 505.5280456542969,
      "learning_rate": 1.8e-06,
      "loss": 1.0691,
      "step": 28920
    },
    {
      "epoch": 2.894,
      "grad_norm": 1102.36376953125,
      "learning_rate": 1.7666666666666668e-06,
      "loss": 1.1492,
      "step": 28940
    },
    {
      "epoch": 2.896,
      "grad_norm": 1594.858154296875,
      "learning_rate": 1.7333333333333334e-06,
      "loss": 1.1062,
      "step": 28960
    },
    {
      "epoch": 2.898,
      "grad_norm": 457.0406188964844,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 1.1712,
      "step": 28980
    },
    {
      "epoch": 2.9,
      "grad_norm": 4112.75048828125,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 1.1613,
      "step": 29000
    },
    {
      "epoch": 2.902,
      "grad_norm": 424.05206298828125,
      "learning_rate": 1.6333333333333333e-06,
      "loss": 1.1128,
      "step": 29020
    },
    {
      "epoch": 2.904,
      "grad_norm": 1754.124755859375,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 1.1008,
      "step": 29040
    },
    {
      "epoch": 2.906,
      "grad_norm": 482.0342712402344,
      "learning_rate": 1.5666666666666666e-06,
      "loss": 1.1236,
      "step": 29060
    },
    {
      "epoch": 2.908,
      "grad_norm": 522.3374633789062,
      "learning_rate": 1.5333333333333334e-06,
      "loss": 1.1243,
      "step": 29080
    },
    {
      "epoch": 2.91,
      "grad_norm": 1424.936767578125,
      "learning_rate": 1.5e-06,
      "loss": 1.1763,
      "step": 29100
    },
    {
      "epoch": 2.912,
      "grad_norm": 263.1778564453125,
      "learning_rate": 1.4666666666666667e-06,
      "loss": 1.2002,
      "step": 29120
    },
    {
      "epoch": 2.914,
      "grad_norm": 2350.2216796875,
      "learning_rate": 1.4333333333333333e-06,
      "loss": 1.1596,
      "step": 29140
    },
    {
      "epoch": 2.916,
      "grad_norm": 99.24890899658203,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 1.1596,
      "step": 29160
    },
    {
      "epoch": 2.918,
      "grad_norm": 682.656005859375,
      "learning_rate": 1.3666666666666668e-06,
      "loss": 1.1604,
      "step": 29180
    },
    {
      "epoch": 2.92,
      "grad_norm": 294.6539001464844,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 1.212,
      "step": 29200
    },
    {
      "epoch": 2.922,
      "grad_norm": 1175.583251953125,
      "learning_rate": 1.3e-06,
      "loss": 1.0545,
      "step": 29220
    },
    {
      "epoch": 2.924,
      "grad_norm": 87439.6796875,
      "learning_rate": 1.2666666666666667e-06,
      "loss": 1.2054,
      "step": 29240
    },
    {
      "epoch": 2.926,
      "grad_norm": 2363.88720703125,
      "learning_rate": 1.2333333333333333e-06,
      "loss": 1.2021,
      "step": 29260
    },
    {
      "epoch": 2.928,
      "grad_norm": 245.2016143798828,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 1.1028,
      "step": 29280
    },
    {
      "epoch": 2.93,
      "grad_norm": 835.929931640625,
      "learning_rate": 1.1666666666666668e-06,
      "loss": 1.1612,
      "step": 29300
    },
    {
      "epoch": 2.932,
      "grad_norm": 425.85174560546875,
      "learning_rate": 1.1333333333333334e-06,
      "loss": 1.1539,
      "step": 29320
    },
    {
      "epoch": 2.934,
      "grad_norm": 1196.2191162109375,
      "learning_rate": 1.1e-06,
      "loss": 1.1396,
      "step": 29340
    },
    {
      "epoch": 2.936,
      "grad_norm": 659.5652465820312,
      "learning_rate": 1.0666666666666667e-06,
      "loss": 1.1127,
      "step": 29360
    },
    {
      "epoch": 2.9379999999999997,
      "grad_norm": 991.3369140625,
      "learning_rate": 1.0333333333333333e-06,
      "loss": 1.1754,
      "step": 29380
    },
    {
      "epoch": 2.94,
      "grad_norm": 850.6887817382812,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 1.056,
      "step": 29400
    },
    {
      "epoch": 2.942,
      "grad_norm": 396.2066955566406,
      "learning_rate": 9.666666666666668e-07,
      "loss": 1.1588,
      "step": 29420
    },
    {
      "epoch": 2.944,
      "grad_norm": 564.3185424804688,
      "learning_rate": 9.333333333333334e-07,
      "loss": 1.1464,
      "step": 29440
    },
    {
      "epoch": 2.9459999999999997,
      "grad_norm": 746.0046997070312,
      "learning_rate": 9e-07,
      "loss": 1.1324,
      "step": 29460
    },
    {
      "epoch": 2.948,
      "grad_norm": 1294.8160400390625,
      "learning_rate": 8.666666666666667e-07,
      "loss": 1.1849,
      "step": 29480
    },
    {
      "epoch": 2.95,
      "grad_norm": 200.301025390625,
      "learning_rate": 8.333333333333333e-07,
      "loss": 1.1319,
      "step": 29500
    },
    {
      "epoch": 2.952,
      "grad_norm": 967.2866821289062,
      "learning_rate": 8.000000000000001e-07,
      "loss": 1.1937,
      "step": 29520
    },
    {
      "epoch": 2.9539999999999997,
      "grad_norm": 3222.185791015625,
      "learning_rate": 7.666666666666667e-07,
      "loss": 1.1038,
      "step": 29540
    },
    {
      "epoch": 2.956,
      "grad_norm": 300.9315490722656,
      "learning_rate": 7.333333333333333e-07,
      "loss": 1.2163,
      "step": 29560
    },
    {
      "epoch": 2.958,
      "grad_norm": 1044.31787109375,
      "learning_rate": 7.000000000000001e-07,
      "loss": 1.2119,
      "step": 29580
    },
    {
      "epoch": 2.96,
      "grad_norm": 254.96006774902344,
      "learning_rate": 6.666666666666667e-07,
      "loss": 1.2239,
      "step": 29600
    },
    {
      "epoch": 2.9619999999999997,
      "grad_norm": 1148.574951171875,
      "learning_rate": 6.333333333333333e-07,
      "loss": 1.0471,
      "step": 29620
    },
    {
      "epoch": 2.964,
      "grad_norm": 2273.381103515625,
      "learning_rate": 6.000000000000001e-07,
      "loss": 1.1422,
      "step": 29640
    },
    {
      "epoch": 2.966,
      "grad_norm": 372.23687744140625,
      "learning_rate": 5.666666666666667e-07,
      "loss": 1.1592,
      "step": 29660
    },
    {
      "epoch": 2.968,
      "grad_norm": 405.4087219238281,
      "learning_rate": 5.333333333333333e-07,
      "loss": 1.2457,
      "step": 29680
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 892.0088500976562,
      "learning_rate": 5.000000000000001e-07,
      "loss": 1.1231,
      "step": 29700
    },
    {
      "epoch": 2.972,
      "grad_norm": 526.0433349609375,
      "learning_rate": 4.666666666666667e-07,
      "loss": 1.1986,
      "step": 29720
    },
    {
      "epoch": 2.974,
      "grad_norm": 1007.865478515625,
      "learning_rate": 4.3333333333333335e-07,
      "loss": 1.1594,
      "step": 29740
    },
    {
      "epoch": 2.976,
      "grad_norm": 926.5580444335938,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 1.1341,
      "step": 29760
    },
    {
      "epoch": 2.9779999999999998,
      "grad_norm": 2239.78125,
      "learning_rate": 3.6666666666666667e-07,
      "loss": 1.1653,
      "step": 29780
    },
    {
      "epoch": 2.98,
      "grad_norm": 1706.525634765625,
      "learning_rate": 3.3333333333333335e-07,
      "loss": 1.1473,
      "step": 29800
    },
    {
      "epoch": 2.982,
      "grad_norm": 1974.6739501953125,
      "learning_rate": 3.0000000000000004e-07,
      "loss": 1.1465,
      "step": 29820
    },
    {
      "epoch": 2.984,
      "grad_norm": 527.654541015625,
      "learning_rate": 2.6666666666666667e-07,
      "loss": 1.1425,
      "step": 29840
    },
    {
      "epoch": 2.9859999999999998,
      "grad_norm": 811.0064697265625,
      "learning_rate": 2.3333333333333336e-07,
      "loss": 1.1792,
      "step": 29860
    },
    {
      "epoch": 2.988,
      "grad_norm": 148.41273498535156,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 1.2008,
      "step": 29880
    },
    {
      "epoch": 2.99,
      "grad_norm": 1751.7294921875,
      "learning_rate": 1.6666666666666668e-07,
      "loss": 1.2192,
      "step": 29900
    },
    {
      "epoch": 2.992,
      "grad_norm": 2208.6669921875,
      "learning_rate": 1.3333333333333334e-07,
      "loss": 1.1487,
      "step": 29920
    },
    {
      "epoch": 2.9939999999999998,
      "grad_norm": 555.1998901367188,
      "learning_rate": 1.0000000000000001e-07,
      "loss": 1.1352,
      "step": 29940
    },
    {
      "epoch": 2.996,
      "grad_norm": 2683.5986328125,
      "learning_rate": 6.666666666666667e-08,
      "loss": 1.1438,
      "step": 29960
    },
    {
      "epoch": 2.998,
      "grad_norm": 427.49237060546875,
      "learning_rate": 3.3333333333333334e-08,
      "loss": 1.0811,
      "step": 29980
    },
    {
      "epoch": 3.0,
      "grad_norm": 304.3790588378906,
      "learning_rate": 0.0,
      "loss": 1.1115,
      "step": 30000
    }
  ],
  "logging_steps": 20,
  "max_steps": 30000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.561738338304e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
